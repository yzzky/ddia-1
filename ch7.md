<h1 id="toc_0">7. 事务</h1>

<h2 id="toc_1">7.1 事务的棘手概念</h2>

<h3 id="toc_2">7.1.1 ACID的含义</h3>

<h4 id="toc_3">原子性（Atomicity）</h4>

<h4 id="toc_4">一致性（Consistency）</h4>

<h4 id="toc_5">隔离性（Isolation）</h4>

<h4 id="toc_6">持久性（Durability）</h4>

<h4 id="toc_7">复制和持久性</h4>

<h3 id="toc_8">7.1.2 单对象和多对象操作</h3>

<h4 id="toc_9">单对象写入</h4>

<h4 id="toc_10">多对象事务的需求</h4>

<h4 id="toc_11">处理错误和中止</h4>

<h2 id="toc_12">7.2 弱隔离级别</h2>

<h3 id="toc_13">7.2.1 读已提交</h3>

<h4 id="toc_14">没有脏读</h4>

<h4 id="toc_15">没有脏写</h4>

<h4 id="toc_16">实现读已提交</h4>

<h3 id="toc_17">7.2.2 快照隔离和可重复读</h3>

<h4 id="toc_18">实现快照隔离</h4>

<h4 id="toc_19">观察一致性快照的可见性规则</h4>

<h4 id="toc_20">索引和快照隔离</h4>

<h4 id="toc_21">可重复读与命名混淆</h4>

<h3 id="toc_22">7.2.3 防止丢失更新</h3>

<h4 id="toc_23">原子写</h4>

<h4 id="toc_24">显式锁定</h4>

<h4 id="toc_25">自动检测丢失的更新</h4>

<h4 id="toc_26">比较并设置（CAS）</h4>

<h4 id="toc_27">冲突解决和复制</h4>

<h4 id="toc_28">写入偏差与幻读</h4>

<h4 id="toc_29">写偏差的特征</h4>

<h4 id="toc_30">写偏差的更多例子</h4>

<h4 id="toc_31">导致写入偏差的幻读</h4>

<h4 id="toc_32">物化冲突</h4>

<h2 id="toc_33">7.3 可序列化</h2>

<h4 id="toc_34">真的串行执行</h4>

<h4 id="toc_35">在存储过程中封装事务</h4>

<h4 id="toc_36">存储过程的优点和缺点</h4>

<h4 id="toc_37">分区</h4>

<h4 id="toc_38">串行执行小结</h4>

<h3 id="toc_39">7.3.1 两阶段锁定（2PL）</h3>

<h4 id="toc_40">实现两阶段锁</h4>

<h4 id="toc_41">两阶段锁定的性能</h4>

<h4 id="toc_42">谓词锁</h4>

<h4 id="toc_43">索引范围锁</h4>

<h3 id="toc_44">7.3.2 序列化快照隔离（SSI）</h3>

<h4 id="toc_45">悲观与乐观的并发控制</h4>

<h4 id="toc_46">基于过时前提的决策</h4>

<h4 id="toc_47">检测旧MVCC读取</h4>

<h4 id="toc_48">检测影响之前读取的写入</h4>

<h4 id="toc_49">可序列化的快照隔离的性能</h4>

<h2 id="toc_50">本章小结</h2>

<h1 id="toc_51">7. 事务</h1>

<p><img src="img/ch7.png" alt=""/></p>

<blockquote>
<p>​ 一些作者声称，支持通用的两阶段提交代价太大，会带来性能与可用性的问题。让程序员来处理过度使用事务导致的性能问题，总比缺少事务编程好得多。</p>

<p>​ ——James Corbett等人，Spanner：Google的全球分布式数据库（2012）</p>
</blockquote>

<hr/>

<ul>
<li>
<a href="#toc_0">7. 事务</a>
<ul>
<li>
<a href="#toc_1">7.1 事务的棘手概念</a>
<ul>
<li>
<a href="#toc_2">7.1.1 ACID的含义</a>
<ul>
<li>
<a href="#toc_3">原子性（Atomicity）</a>
</li>
<li>
<a href="#toc_4">一致性（Consistency）</a>
</li>
<li>
<a href="#toc_5">隔离性（Isolation）</a>
</li>
<li>
<a href="#toc_6">持久性（Durability）</a>
</li>
<li>
<a href="#toc_7">复制和持久性</a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">7.1.2 单对象和多对象操作</a>
<ul>
<li>
<a href="#toc_9">单对象写入</a>
</li>
<li>
<a href="#toc_10">多对象事务的需求</a>
</li>
<li>
<a href="#toc_11">处理错误和中止</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_12">7.2 弱隔离级别</a>
<ul>
<li>
<a href="#toc_13">7.2.1 读已提交</a>
<ul>
<li>
<a href="#toc_14">没有脏读</a>
</li>
<li>
<a href="#toc_15">没有脏写</a>
</li>
<li>
<a href="#toc_16">实现读已提交</a>
</li>
</ul>
</li>
<li>
<a href="#toc_17">7.2.2 快照隔离和可重复读</a>
<ul>
<li>
<a href="#toc_18">实现快照隔离</a>
</li>
<li>
<a href="#toc_19">观察一致性快照的可见性规则</a>
</li>
<li>
<a href="#toc_20">索引和快照隔离</a>
</li>
<li>
<a href="#toc_21">可重复读与命名混淆</a>
</li>
</ul>
</li>
<li>
<a href="#toc_22">7.2.3 防止丢失更新</a>
<ul>
<li>
<a href="#toc_23">原子写</a>
</li>
<li>
<a href="#toc_24">显式锁定</a>
</li>
<li>
<a href="#toc_25">自动检测丢失的更新</a>
</li>
<li>
<a href="#toc_26">比较并设置（CAS）</a>
</li>
<li>
<a href="#toc_27">冲突解决和复制</a>
</li>
<li>
<a href="#toc_28">写入偏差与幻读</a>
</li>
<li>
<a href="#toc_29">写偏差的特征</a>
</li>
<li>
<a href="#toc_30">写偏差的更多例子</a>
</li>
<li>
<a href="#toc_31">导致写入偏差的幻读</a>
</li>
<li>
<a href="#toc_32">物化冲突</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_33">7.3 可序列化</a>
<ul>
<li>
<ul>
<li>
<a href="#toc_34">真的串行执行</a>
</li>
<li>
<a href="#toc_35">在存储过程中封装事务</a>
</li>
<li>
<a href="#toc_36">存储过程的优点和缺点</a>
</li>
<li>
<a href="#toc_37">分区</a>
</li>
<li>
<a href="#toc_38">串行执行小结</a>
</li>
</ul>
</li>
<li>
<a href="#toc_39">7.3.1 两阶段锁定（2PL）</a>
<ul>
<li>
<a href="#toc_40">实现两阶段锁</a>
</li>
<li>
<a href="#toc_41">两阶段锁定的性能</a>
</li>
<li>
<a href="#toc_42">谓词锁</a>
</li>
<li>
<a href="#toc_43">索引范围锁</a>
</li>
</ul>
</li>
<li>
<a href="#toc_44">7.3.2 序列化快照隔离（SSI）</a>
<ul>
<li>
<a href="#toc_45">悲观与乐观的并发控制</a>
</li>
<li>
<a href="#toc_46">基于过时前提的决策</a>
</li>
<li>
<a href="#toc_47">检测旧MVCC读取</a>
</li>
<li>
<a href="#toc_48">检测影响之前读取的写入</a>
</li>
<li>
<a href="#toc_49">可序列化的快照隔离的性能</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_50">本章小结</a>
</li>
</ul>
</li>
<li>
<a href="#toc_51">7. 事务</a>
<ul>
<li>
<a href="#toc_52">事务的棘手概念</a>
<ul>
<li>
<a href="#toc_53">ACID的含义</a>
<ul>
<li>
<a href="#toc_54">原子性（Atomicity）</a>
</li>
<li>
<a href="#toc_55">一致性（Consistency）</a>
</li>
<li>
<a href="#toc_56">隔离性（Isolation）</a>
</li>
<li>
<a href="#toc_57">持久性（Durability）</a>
</li>
</ul>
</li>
<li>
<a href="#toc_59">单对象和多对象操作</a>
<ul>
<li>
<a href="#toc_60">单对象写入</a>
</li>
<li>
<a href="#toc_61">多对象事务的需求</a>
</li>
<li>
<a href="#toc_62">处理错误和中止</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_63">弱隔离级别</a>
<ul>
<li>
<a href="#toc_64">读已提交</a>
<ul>
<li>
<a href="#toc_65">没有脏读</a>
</li>
<li>
<a href="#toc_66">没有脏写</a>
</li>
<li>
<a href="#toc_67">实现读已提交</a>
</li>
</ul>
</li>
<li>
<a href="#toc_68">快照隔离和可重复读</a>
<ul>
<li>
<a href="#toc_69">实现快照隔离</a>
</li>
<li>
<a href="#toc_70">观察一致性快照的可见性规则</a>
</li>
<li>
<a href="#toc_71">索引和快照隔离</a>
</li>
<li>
<a href="#toc_72">可重复读与命名混淆</a>
</li>
</ul>
</li>
<li>
<a href="#toc_73">防止丢失更新</a>
<ul>
<li>
<a href="#toc_74">原子写</a>
</li>
<li>
<a href="#toc_75">显式锁定</a>
</li>
<li>
<a href="#toc_76">自动检测丢失的更新</a>
</li>
<li>
<a href="#toc_77">比较并设置（CAS）</a>
</li>
<li>
<a href="#toc_78">冲突解决和复制</a>
</li>
<li>
<a href="#toc_79">写入偏差与幻读</a>
</li>
<li>
<a href="#toc_80">写偏差的特征</a>
</li>
<li>
<a href="#toc_81">写偏差的更多例子</a>
</li>
<li>
<a href="#toc_82">导致写入偏差的幻读</a>
</li>
<li>
<a href="#toc_83">物化冲突</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_84">可序列化</a>
<ul>
<li>
<ul>
<li>
<a href="#toc_85">真的串行执行</a>
</li>
<li>
<a href="#toc_86">在存储过程中封装事务</a>
</li>
<li>
<a href="#toc_87">存储过程的优点和缺点</a>
</li>
<li>
<a href="#toc_88">分区</a>
</li>
<li>
<a href="#toc_89">串行执行小结</a>
</li>
</ul>
</li>
<li>
<a href="#toc_90">两阶段锁定（2PL）</a>
</li>
<li>
<a href="#toc_92">实现两阶段锁</a>
</li>
<li>
<a href="#toc_93">两阶段锁定的性能</a>
</li>
<li>
<a href="#toc_94">谓词锁</a>
</li>
<li>
<a href="#toc_95">索引范围锁</a>
</li>
</ul>
</li>
<li>
<a href="#toc_96">序列化快照隔离（SSI）</a>
<ul>
<li>
<a href="#toc_97">悲观与乐观的并发控制</a>
</li>
<li>
<a href="#toc_98">基于过时前提的决策</a>
</li>
<li>
<a href="#toc_99">检测旧MVCC读取</a>
</li>
<li>
<a href="#toc_100">检测影响之前读取的写入</a>
</li>
<li>
<a href="#toc_101">可序列化的快照隔离的性能</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_102">本章小结</a>
</li>
<li>
<a href="#toc_103">参考文献</a>
</li>
</ul>
</li>
</ul>


<p>在数据系统的残酷现实中，很多事情都可能出错：</p>

<ul>
<li>数据库软件、硬件可能在任意时刻发生故障（包括写操作进行到一半时）。</li>
<li>应用程序可能在任意时刻崩溃（包括一系列操作的中间）。</li>
<li>网络中断可能会意外切断数据库与应用的连接，或数据库之间的连接。</li>
<li>多个客户端可能会同时写入数据库，覆盖彼此的更改。</li>
<li>客户端可能读取到无意义的数据，因为数据只更新了一部分。</li>
<li>客户之间的竞争条件可能导致令人惊讶的错误。</li>
</ul>

<p>为了实现可靠性，系统必须处理这些故障，确保它们不会导致整个系统的灾难性故障。但是实现容错机制工作量巨大。需要仔细考虑所有可能出错的事情，并进行大量的测试，以确保解决方案真正管用。</p>

<p>​   数十年来，<strong>事务（transaction）</strong> 一直是简化这些问题的首选机制。事务是应用程序将多个读写操作组合成一个逻辑单元的一种方式。从概念上讲，事务中的所有读写操作被视作单个操作来执行：整个事务要么成功（<strong>提交（commit）</strong>）要么失败（<strong>中止（abort）</strong>，<strong>回滚（rollback）</strong>）。如果失败，应用程序可以安全地重试。对于事务来说，应用程序的错误处理变得简单多了，因为它不用再担心部分失败的情况了，即某些操作成功，某些失败（无论出于何种原因）。</p>

<p>​   和事务打交道时间长了，你可能会觉得它显而易见。但我们不应将其视为理所当然。事务不是天然存在的；它们是为了<strong>简化应用编程模型</strong>而创建的。通过使用事务，应用程序可以自由地忽略某些潜在的错误情况和并发问题，因为数据库会替应用处理好这些。（我们称之为<strong>安全保证（safety guarantees）</strong>）。</p>

<p>​   并不是所有的应用都需要事务，有时候弱化事务保证、或完全放弃事务也是有好处的（例如，为了获得更高性能或更高可用性）。一些安全属性也可以在没有事务的情况下实现。</p>

<p>​   怎样知道你是否需要事务？为了回答这个问题，首先需要确切理解事务可以提供的安全保障，以及它们的代价。尽管乍看事务似乎很简单，但实际上有许多微妙但重要的细节在起作用。</p>

<p>​   本章将研究许多出错案例，并探索数据库用于防范这些问题的算法。尤其会深入<strong>并发控制</strong>的领域，讨论各种可能发生的竞争条件，以及数据库如何实现<strong>读已提交</strong>，<strong>快照隔离</strong>和<strong>可串行化</strong>等隔离级别。</p>

<p>​   本章同时适用于单机数据库与分布式数据库；在<a href="ch8.md">第8章</a>中将重点讨论仅出现在分布式系统中的特殊挑战。</p>

<h2 id="toc_52">事务的棘手概念</h2>

<p>​   现今，几乎所有的关系型数据库和一些非关系数据库都支持<strong>事务</strong>。其中大多数遵循IBM System R（第一个SQL数据库）在1975年引入的风格【1,2,3】。40年里，尽管一些实现细节发生了变化，但总体思路大同小异：MySQL，PostgreSQL，Oracle，SQL Server等数据库中的事务支持与System R异乎寻常地相似。</p>

<p>2000年以后，非关系（NoSQL）数据库开始普及。它们的目标是通过提供新的数据模型选择（参见第2章），并通过默认包含复制（第5章）和分区（第6章）来改善关系现状。事务是这种运动的主要原因：这些新一代数据库中的许多数据库完全放弃了事务，或者重新定义了这个词，描述比以前理解所更弱的一套保证【4】。</p>

<p>随着这种新型分布式数据库的炒作，人们普遍认为事务是可扩展性的对立面，任何大型系统都必须放弃事务以保持良好的性能和高可用性【5,6】。另一方面，数据库厂商有时将事务保证作为“重要应用”和“有价值数据”的基本要求。这两种观点都是<strong>纯粹的夸张</strong>。</p>

<p>事实并非如此简单：与其他技术设计选择一样，事务有其优势和局限性。为了理解这些权衡，让我们了解事务所提供保证的细节——无论是在正常运行中还是在各种极端（但是现实存在）情况下。</p>

<h3 id="toc_53">ACID的含义</h3>

<p>事务所提供的安全保证，通常由众所周知的首字母缩略词ACID来描述，ACID代表<strong>原子性（Atomicity）</strong>，<strong>一致性（Consistency）</strong>，<strong>隔离性（Isolation）</strong>和<strong>持久性（Durability）</strong>。它由TheoHärder和Andreas Reuter于1983年创建，旨在为数据库中的容错机制建立精确的术语。</p>

<p>但实际上，不同数据库的ACID实现并不相同。例如，我们将会看到，关于<strong>隔离性（Isolation）</strong> 的含义就有许多含糊不清【8】。高层次上的想法很美好，但魔鬼隐藏在细节里。今天，当一个系统声称自己“符合ACID”时，实际上能期待的是什么保证并不清楚。不幸的是，ACID现在几乎已经变成了一个营销术语。</p>

<p>（不符合ACID标准的系统有时被称为BASE，它代表<strong>基本可用性（Basically Available）</strong>，<strong>软状态（Soft State）</strong>和<strong>最终一致性（Eventual consistency）</strong>【9】，这比ACID的定义更加模糊，似乎BASE的唯一合理的定义是“不是ACID”，即它几乎可以代表任何你想要的东西。）</p>

<p>让我们深入了解原子性，一致性，隔离性和持久性的定义，这可以让我们提炼出事务的思想。</p>

<h4 id="toc_54">原子性（Atomicity）</h4>

<p>一般来说，原子是指不能分解成小部分的东西。这个词在计算机的不同领域中意味着相似但又微妙不同的东西。例如，在多线程编程中，如果一个线程执行一个原子操作，这意味着另一个线程无法看到该操作的一半结果。系统只能处于操作之前或操作之后的状态，而不是介于两者之间的状态。</p>

<p>相比之下，ACID的原子性并<strong>不</strong>是关于<strong>并发（concurrent）</strong>的。它并不是在描述如果几个进程试图同时访问相同的数据会发生什么情况，这种情况包含在缩写 <strong><em>I</em></strong> 中，即<a href="#%E9%9A%94%E7%A6%BB%E6%80%A7%EF%BC%88Isolation%EF%BC%89"><strong>隔离性（Isolation）</strong></a></p>

<p>ACID的原子性而是描述了当客户想进行多次写入，但在一些写操作处理完之后出现故障的情况。例如进程崩溃，网络连接中断，磁盘变满或者某种完整性约束被违反。如果这些写操作被分组到一个原子事务中，并且该事务由于错误而不能完成（提交），则该事务将被中止，并且数据库必须丢弃或撤消该事务中迄今为止所做的任何写入。</p>

<p>如果没有原子性，在多处更改进行到一半时发生错误，很难知道哪些更改已经生效，哪些没有生效。该应用程序可以再试一次，但冒着进行两次相同变更的风险，可能会导致数据重复或错误的数据。原子性简化了这个问题：如果事务被<strong>中止（abort）</strong>，应用程序可以确定它没有改变任何东西，所以可以安全地重试。</p>

<p>ACID原子性的定义特征是：<strong>能够在错误时中止事务，丢弃该事务进行的所有写入变更的能力。</strong> 或许 <strong>可中止性（abortability）</strong> 是更好的术语，但本书将继续使用原子性，因为这是惯用词。</p>

<h4 id="toc_55">一致性（Consistency）</h4>

<p>一致性这个词被赋予太多含义：</p>

<ul>
<li>在<a href="ch5.md">第5章</a>中，我们讨论了副本一致性，以及异步复制系统中的最终一致性问题（参阅“<a href="ch5.md#%E5%A4%8D%E5%88%B6%E5%BB%B6%E8%BF%9F%E9%97%AE%E9%A2%98">复制延迟问题</a>”）。</li>
<li><a href="ch6.md#%E4%B8%80%E8%87%B4%E6%80%A7%E6%95%A3%E5%88%97">一致性散列（Consistency Hash）</a>)是某些系统用于重新分区的一种分区方法。</li>
<li>在<a href="ch9.md#CAP%E5%AE%9A%E7%90%86">CAP定理</a>中，一致性一词用于表示<a href="ch9.md#%E7%BA%BF%E6%80%A7%E5%8C%96">可线性化</a>。</li>
<li>在ACID的上下文中，<strong>一致性</strong>是指数据库在应用程序的特定概念中处于“良好状态”。</li>
</ul>

<p>很不幸，这一个词就至少有四种不同的含义。</p>

<p>ACID一致性的概念是，<strong>对数据的一组特定约束必须始终成立</strong>。即<strong>不变量（invariants）</strong>。例如，在会计系统中，所有账户整体上必须借贷相抵。如果一个事务开始于一个满足这些不变量的有效数据库，且在事务处理期间的任何写入操作都保持这种有效性，那么可以确定，不变量总是满足的。</p>

<p>但是，一致性的这种概念取决于应用程序对不变量的观念，应用程序负责正确定义它的事务，并保持一致性。这并不是数据库可以保证的事情：如果你写入违反不变量的脏数据，数据库也无法阻止你。 （一些特定类型的不变量可以由数据库检查，例如外键约束或唯一约束，但是一般来说，是应用程序来定义什么样的数据是有效的，什么样是无效的。—— 数据库只管存储。）</p>

<p>原子性，隔离性和持久性是数据库的属性，而一致性（在ACID意义上）是应用程序的属性。应用可能依赖数据库的原子性和隔离属性来实现一致性，但这并不仅取决于数据库。因此，字母C不属于ACID<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>。</p>

<h4 id="toc_56">隔离性（Isolation）</h4>

<p>大多数数据库都会同时被多个客户端访问。如果它们各自读写数据库的不同部分，这是没有问题的，但是如果它们访问相同的数据库记录，则可能会遇到<strong>并发</strong>问题（<strong>竞争条件（race conditions）</strong>）。</p>

<p><a href="img/fig7-1.png">图7-1</a>是这类问题的一个简单例子。假设你有两个客户端同时在数据库中增长一个计数器。（假设数据库中没有自增操作）每个客户端需要读取计数器的当前值，加 1 ，再回写新值。<a href="img/fig7-1.png">图7-1</a> 中，因为发生了两次增长，计数器应该从42增至44；但由于竞态条件，实际上只增至 43 。</p>

<p>ACID意义上的隔离性意味着，<strong>同时执行的事务是相互隔离的</strong>：它们不能相互冒犯。传统的数据库教科书将隔离性形式化为<strong>可序列化（Serializability）</strong>，这意味着每个事务可以假装它是唯一在整个数据库上运行的事务。数据库确保当事务已经提交时，结果与它们按顺序运行（一个接一个）是一样的，尽管实际上它们可能是并发运行的【10】。</p>

<p><img src="img/fig7-1.png" alt=""/></p>

<p><strong>图7-1 两个客户之间的竞争状态同时递增计数器</strong></p>

<p>然而实践中很少会使用可序列化隔离，因为它有性能损失。一些流行的数据库如Oracle 11g，甚至没有实现它。在Oracle中有一个名为“可序列化”的隔离级别，但实际上它实现了一种叫做<strong>快照隔离（snapshot isolation）</strong> 的功能，<strong>这是一种比可序列化更弱的保证</strong>【8,11】。我们将在“<a href="">弱隔离等级</a>”中研究快照隔离和其他形式的隔离。</p>

<h4 id="toc_57">持久性（Durability）</h4>

<p>数据库系统的目的是，提供一个安全的地方存储数据，而不用担心丢失。<strong>持久性</strong> 是一个承诺，即一旦事务成功完成，即使发生硬件故障或数据库崩溃，写入的任何数据也不会丢失。</p>

<p>在单节点数据库中，持久性通常意味着数据已被写入非易失性存储设备，如硬盘或SSD。它通常还包括预写日志或类似的文件（参阅“<a href="ch3.md#%E8%AE%A9B%E6%A0%91%E6%9B%B4%E5%8F%AF%E9%9D%A0">让B树更可靠</a>”），以便在磁盘上的数据结构损坏时进行恢复。在带复制的数据库中，持久性可能意味着数据已成功复制到一些节点。为了提供持久性保证，数据库必须等到这些写入或复制完成后，才能报告事务成功提交。</p>

<p>如“<a href="ch1.md#%E5%8F%AF%E9%9D%A0%E6%80%A7">可靠性</a>”一节所述，<strong>完美的持久性是不存在的</strong> ：如果所有硬盘和所有备份同时被销毁，那显然没有任何数据库能救得了你。</p>

<blockquote>
<h4 id="toc_58">复制和持久性</h4>

<p>在历史上，持久性意味着写入归档磁带。后来它被理解为写入硬盘或SSD。最近它已经适应了“复制（replication）”的新内涵。哪种实现更好一些？</p>

<p>真相是，没有什么是完美的：</p>

<ul>
<li>如果你写入磁盘然后机器宕机，即使数据没有丢失，在修复机器或将磁盘转移到其他机器之前，也是无法访问的。这种情况下，复制系统可以保持可用性。</li>
<li>一个相关性故障（停电，或一个特定输入导致所有节点崩溃的Bug）可能会一次性摧毁所有副本（参阅「<a href="ch1.md#%E5%8F%AF%E9%9D%A0%E6%80%A7">可靠性</a>」），任何仅存储在内存中的数据都会丢失，故内存数据库仍然要和磁盘写入打交道。</li>
<li>在异步复制系统中，当主库不可用时，最近的写入操作可能会丢失（参阅「<a href="ch5.md#%E5%A4%84%E7%90%86%E8%8A%82%E7%82%B9%E5%AE%95%E6%9C%BA">处理节点宕机</a>」）。</li>
<li>当电源突然断电时，特别是固态硬盘，有证据显示有时会违反应有的保证：甚至fsync也不能保证正常工作【12】。硬盘固件可能有错误，就像任何其他类型的软件一样【13,14】。</li>
<li>存储引擎和文件系统之间的微妙交互可能会导致难以追踪的错误，并可能导致磁盘上的文件在崩溃后被损坏【15,16】。</li>
<li>磁盘上的数据可能会在没有检测到的情况下逐渐损坏【17】。如果数据已损坏一段时间，副本和最近的备份也可能损坏。这种情况下，需要尝试从历史备份中恢复数据。</li>
<li>一项关于固态硬盘的研究发现，在运行的前四年中，30％到80％的硬盘会产生至少一个坏块【18】。相比固态硬盘，磁盘的坏道率较低，但完全失效的概率更高。</li>
<li>如果SSD断电，可能会在几周内开始丢失数据，具体取决于温度【19】。</li>
</ul>

<p>在实践中，没有一种技术可以提供绝对保证。只有各种降低风险的技术，包括写入磁盘，复制到远程机器和备份——它们可以且应该一起使用。与往常一样，最好抱着怀疑的态度接受任何理论上的“保证”</p>
</blockquote>

<h3 id="toc_59">单对象和多对象操作</h3>

<p>回顾一下，在ACID中，原子性和隔离性描述了客户端在同一事务中执行多次写入时，数据库应该做的事情：</p>

<p><strong><em>原子性</em></strong></p>

<p>如果在一系列写操作的中途发生错误，则应中止事务处理，并丢弃当前事务的所有写入。换句话说，数据库免去了用户对部分失败的担忧——通过提供“<strong>宁为玉碎，不为瓦全（all-or-nothing）</strong>”的保证。</p>

<p><strong><em>隔离性</em></strong></p>

<p>同时运行的事务不应该互相干扰。例如，如果一个事务进行多次写入，则另一个事务要么看到全部写入结果，要么什么都看不到，但不应该是一些子集。</p>

<p>这些定义假设你想同时修改多个对象（行，文档，记录）。通常需要<strong>多对象事务（multi-object transaction）</strong> 来保持多块数据同步。<a href="img/fig7-2.png">图7-2</a>展示了一个来自电邮应用的例子。执行以下查询来显示用户未读邮件数量：</p>

<pre><code class="language-sql">SELECT COUNT（*）FROM emails WHERE recipient_id = 2 AND unread_flag = true
</code></pre>

<p>但如果邮件太多，你可能会觉得这个查询太慢，并决定用单独的字段存储未读邮件的数量（一种反规范化）。现在每当一个新消息写入时，必须也增长未读计数器，每当一个消息被标记为已读时，也必须减少未读计数器。</p>

<p>在<a href="img/fig7-2.png">图7-2</a>中，用户2 遇到异常情况：邮件列表里显示有未读消息，但计数器显示为零未读消息，因为计数器增长还没有发生<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup>。隔离性可以避免这个问题：通过确保用户2 要么同时看到新邮件和增长后的计数器，要么都看不到。反正不会看到执行到一半的中间结果。</p>

<p><img src="img/fig7-2.png" alt=""/></p>

<p><strong>图7-2 违反隔离性：一个事务读取另一个事务的未被执行的写入（“脏读”）。</strong></p>

<p><a href="img/fig7-3.png">图7-3</a>说明了对原子性的需求：如果在事务过程中发生错误，邮箱和未读计数器的内容可能会失去同步。在原子事务中，如果对计数器的更新失败，事务将被中止，并且插入的电子邮件将被回滚。</p>

<p><img src="img/fig7-3.png" alt=""/></p>

<p><strong>图7-3 原子性确保发生错误时，事务先前的任何写入都会被撤消，以避免状态不一致</strong></p>

<p>多对象事务需要某种方式来确定哪些读写操作属于同一个事务。在关系型数据库中，通常基于客户端与数据库服务器的TCP连接：在任何特定连接上，<code>BEGIN TRANSACTION</code> 和 <code>COMMIT</code> 语句之间的所有内容，被认为是同一事务的一部分.<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup></p>

<p>另一方面，许多非关系数据库并没有将这些操作组合在一起的方法。即使存在多对象API（例如，键值存储可能具有在一个操作中更新几个键的数个put操作），但这并不一定意味着它具有事务语义：该命令可能在一些键上成功，在其他的键上失败，使数据库处于部分更新的状态。</p>

<h4 id="toc_60">单对象写入</h4>

<p>当单个对象发生改变时，原子性和隔离也是适用的。例如，假设您正在向数据库写入一个 20 KB的 JSON文档：</p>

<ul>
<li>如果在发送第一个10 KB之后网络连接中断，数据库是否存储了不可解析的10KB JSON片段？</li>
<li>如果在数据库正在覆盖磁盘上的前一个值的过程中电源发生故障，是否最终将新旧值拼接在一起？</li>
<li>如果另一个客户端在写入过程中读取该文档，是否会看到部分更新的值？</li>
</ul>

<p>这些问题非常让人头大，故存储引擎一个几乎普遍的目标是：对单节点上的单个对象（例如键值对）上提供原子性和隔离性。原子性可以通过使用日志来实现崩溃恢复（参阅“<a href="">使B树可靠</a>”），并且可以使用每个对象上的锁来实现隔离（每次只允许一个线程访问对象） ）。</p>

<p>一些数据库也提供更复杂的原子操作，例如自增操作，这样就不再需要像 <a href="img/fig7-1.png">图7-1</a> 那样的读取-修改-写入序列了。同样流行的是 <strong><a href="#%E6%AF%94%E8%BE%83%E5%B9%B6%E8%AE%BE%E7%BD%AE%EF%BC%88CAS%EF%BC%89">比较和设置（CAS, compare-and-set）</a></strong> 操作，当值没有被其他并发修改过时，才允许执行写操作。</p>

<p>这些单对象操作很有用，因为它们可以防止在多个客户端尝试同时写入同一个对象时丢失更新（参阅“<a href="#%E9%98%B2%E6%AD%A2%E4%B8%A2%E5%A4%B1%E6%9B%B4%E6%96%B0">防止丢失更新</a>”）。但它们不是通常意义上的事务。CAS以及其他单一对象操作被称为“轻量级事务”，甚至出于营销目的被称为“ACID”【20,21,22】，但是这个术语是误导性的。事务通常被理解为，<strong>将多个对象上的多个操作合并为一个执行单元的机制</strong>。<sup id="fnref4"><a href="#fn4" rel="footnote">4</a></sup></p>

<h4 id="toc_61">多对象事务的需求</h4>

<p>许多分布式数据存储已经放弃了多对象事务，因为多对象事务很难跨分区实现，而且在需要高可用性或高性能的情况下，它们可能会碍事。但说到底，在分布式数据库中实现事务，并没有什么根本性的障碍。<a href="ch9.md">第9章</a> 将讨论分布式事务的实现。</p>

<p>但是我们是否需要多对象事务？<strong>是否有可能只用键值数据模型和单对象操作来实现任何应用程序？</strong></p>

<p>有一些场景中，单对象插入，更新和删除是足够的。但是许多其他场景需要协调写入几个不同的对象：</p>

<ul>
<li>在关系数据模型中，一个表中的行通常具有对另一个表中的行的外键引用。（类似的是，在一个图数据模型中，一个顶点有着到其他顶点的边）。多对象事务使你确信这些引用始终有效：当插入几个相互引用的记录时，外键必须是正确的，最新的，不然数据就没有意义。</li>
<li>在文档数据模型中，需要一起更新的字段通常在同一个文档中，这被视为单个对象——更新单个文档时不需要多对象事务。但是，缺乏连接功能的文档数据库会鼓励非规范化（参阅“<a href="ch2.md#%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9C%A8%E4%BB%8A%E6%97%A5%E7%9A%84%E5%AF%B9%E6%AF%94">关系型数据库与文档数据库在今日的对比</a>”）。当需要更新非规范化的信息时，如 <a href="img/fig7-2.png">图7-2</a> 所示，需要一次更新多个文档。事务在这种情况下非常有用，可以防止非规范化的数据不同步。</li>
<li>在具有二级索引的数据库中（除了纯粹的键值存储以外几乎都有），每次更改值时都需要更新索引。从事务角度来看，这些索引是不同的数据库对象：例如，如果没有事务隔离性，记录可能出现在一个索引中，但没有出现在另一个索引中，因为第二个索引的更新还没有发生。</li>
</ul>

<p>这些应用仍然可以在没有事务的情况下实现。然而，<strong>没有原子性，错误处理就要复杂得多，缺乏隔离性，就会导致并发问题</strong>。我们将在“<a href="#%E5%BC%B1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB">弱隔离级别</a>”中讨论这些问题，并在<a href="">第12章</a>中探讨其他方法。</p>

<h4 id="toc_62">处理错误和中止</h4>

<p>事务的一个关键特性是，如果发生错误，它可以中止并安全地重试。 ACID数据库基于这样的哲学：如果数据库有违反其原子性，隔离性或持久性的危险，则宁愿完全放弃事务，而不是留下半成品。</p>

<p>然而并不是所有的系统都遵循这个哲学。特别是具有<a href="ch6.md#%E6%97%A0%E4%B8%BB%E5%A4%8D%E5%88%B6">无主复制</a>的数据存储，主要是在“尽力而为”的基础上进行工作。可以概括为“数据库将做尽可能多的事，运行遇到错误时，它不会撤消它已经完成的事情“ ——所以，从错误中恢复是应用程序的责任。</p>

<p>错误发生不可避免，但许多软件开发人员倾向于只考虑乐观情况，而不是错误处理的复杂性。例如，像Rails的ActiveRecord和Django这样的<strong>对象关系映射（ORM, object-relation Mapping）</strong> 框架不会重试中断的事务—— 这个错误通常会导致一个从堆栈向上传播的异常，所以任何用户输入都会被丢弃，用户拿到一个错误信息。这实在是太耻辱了，因为中止的重点就是允许安全的重试。</p>

<p>尽管重试一个中止的事务是一个简单而有效的错误处理机制，但它并不完美：</p>

<ul>
<li>如果事务实际上成功了，但是在服务器试图向客户端确认提交成功时网络发生故障（所以客户端认为提交失败了），那么重试事务会导致事务被执行两次——除非你有一个额外的应用级除重机制。</li>
<li>如果错误是由于负载过大造成的，则重试事务将使问题变得更糟，而不是更好。为了避免这种正反馈循环，可以限制重试次数，使用指数退避算法，并单独处理与过载相关的错误（如果允许）。</li>
<li>仅在临时性错误（例如，由于死锁，异常情况，临时性网络中断和故障切换）后才值得重试。在发生永久性错误（例如，违反约束）之后重试是毫无意义的。</li>
<li>如果事务在数据库之外也有副作用，即使事务被中止，也可能发生这些副作用。例如，如果你正在发送电子邮件，那你肯定不希望每次重试事务时都重新发送电子邮件。如果你想确保几个不同的系统一起提交或放弃，<strong>二阶段提交（2PC, two-phase commit）</strong> 可以提供帮助（“<a href="ch9.md#%E5%8E%9F%E5%AD%90%E6%8F%90%E4%BA%A4%E4%B8%8E%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%EF%BC%882PC%EF%BC%89">原子提交和两阶段提交（2PC）</a>”中将讨论这个问题）。</li>
<li>如果客户端进程在重试中失效，任何试图写入数据库的数据都将丢失。<br/></li>
</ul>

<h2 id="toc_63">弱隔离级别</h2>

<p>如果两个事务不触及相同的数据，它们可以安全地<strong>并行（parallel）</strong> 运行，因为两者都不依赖于另一个。当一个事务读取由另一个事务同时修改的数据时，或者当两个事务试图同时修改相同的数据时，并发问题（竞争条件）才会出现。</p>

<p>并发BUG很难通过测试找到，因为这样的错误只有在特殊时机下才会触发。这样的时机可能很少，通常很难重现<sup id="fnref5"><a href="#fn5" rel="footnote">5</a></sup>。并发性也很难推理，特别是在大型应用中，你不一定知道哪些其他代码正在访问数据库。在一次只有一个用户时，应用开发已经很麻烦了，有许多并发用户使得它更加困难，因为任何一个数据都可能随时改变。</p>

<p>出于这个原因，数据库一直试图通过提供<strong>事务隔离（transaction isolation）</strong> 来隐藏应用程序开发者的并发问题。从理论上讲，隔离可以通过假装没有并发发生，让你的生活更加轻松：<strong>可序列化（serializable）</strong> 的隔离等级意味着数据库保证事务的效果如同连续运行（即一次一个，没有任何并发）。</p>

<p>实际上不幸的是：隔离并没有那么简单。<strong>可序列化</strong> 会有性能损失，许多数据库不愿意支付这个代价【8】。因此，系统通常使用较弱的隔离级别来防止一部分，而不是全部的并发问题。这些隔离级别难以理解，并且会导致微妙的错误，但是它们仍然在实践中被使用【23】。</p>

<p>弱事务隔离级别导致的并发性错误不仅仅是一个理论问题。它们造成了很多的资金损失【24,25】，耗费了财务审计人员的调查【26】，并导致客户数据被破坏【27】。关于这类问题的一个流行的评论是“如果你正在处理财务数据，请使用ACID数据库！” —— 但是这一点没有提到。即使是很多流行的关系型数据库系统（通常被认为是“ACID”）也使用弱隔离级别，所以它们也不一定能防止这些错误的发生。</p>

<p>比起盲目地依赖工具，我们应该对存在的并发问题的种类，以及如何防止这些问题有深入的理解。然后就可以使用我们所掌握的工具来构建可靠和正确的应用程序。</p>

<p>在本节中，我们将看几个在实践中使用的弱（<strong>不可串行化（nonserializable）</strong>）隔离级别，并详细讨论哪种竞争条件可能发生也可能不发生，以便您可以决定什么级别适合您的应用程序。一旦我们完成了这个工作，我们将详细讨论可串行性（请参阅“<a href="#%E5%8F%AF%E5%BA%8F%E5%88%97%E5%8C%96">可序列化</a>”）。我们讨论的隔离级别将是非正式的，使用示例。如果你需要严格的定义和分析它们的属性，你可以在学术文献中找到它们[28,29,30]。</p>

<h3 id="toc_64">读已提交</h3>

<p>最基本的事务隔离级别是<strong>读已提交（Read Committed）</strong><sup id="fnref6"><a href="#fn6" rel="footnote">6</a></sup>，它提供了两个保证：</p>

<ol>
<li>从数据库读时，只能看到已提交的数据（没有<strong>脏读（dirty reads）</strong>）。</li>
<li>写入数据库时，只会覆盖已经写入的数据（没有<strong>脏写（dirty writes）</strong>）。</li>
</ol>

<p>我们来更详细地讨论这两个保证。</p>

<h4 id="toc_65">没有脏读</h4>

<p>设想一个事务已经将一些数据写入数据库，但事务还没有提交或中止。另一个事务可以看到未提交的数据吗？如果是的话，那就叫做<strong>脏读（dirty reads）</strong>【2】。</p>

<p>在<strong>读已提交</strong>隔离级别运行的事务必须防止脏读。这意味着事务的任何写入操作只有在该事务提交时才能被其他人看到（然后所有的写入操作都会立即变得可见）。如<a href="">图7-4</a>所示，用户1 设置了<code>x = 3</code>，但用户2 的 <code>get x</code>仍旧返回旧值2 ，而用户1 尚未提交。</p>

<p><img src="img/fig7-4.png" alt=""/></p>

<p><strong>图7-4 没有脏读：用户2只有在用户1的事务已经提交后才能看到x的新值。</strong></p>

<p>为什么要防止脏读，有几个原因：</p>

<ul>
<li>如果事务需要更新多个对象，脏读取意味着另一个事务可能会只看到一部分更新。例如，在<a href="img/fig7-2.png">图7-2</a>中，用户看到新的未读电子邮件，但看不到更新的计数器。这就是电子邮件的脏读。看到处于部分更新状态的数据库会让用户感到困惑，并可能导致其他事务做出错误的决定。</li>
<li>如果事务中止，则所有写入操作都需要回滚（如<a href="img/fig7-3.png">图7-3</a>所示）。如果数据库允许脏读，那就意味着一个事务可能会看到稍后需要回滚的数据，即从未实际提交给数据库的数据。想想后果就让人头大。</li>
</ul>

<h4 id="toc_66">没有脏写</h4>

<p>如果两个事务同时尝试更新数据库中的相同对象，会发生什么情况？我们不知道写入的顺序是怎样的，但是我们通常认为后面的写入会覆盖前面的写入。</p>

<p>但是，如果先前的写入是尚未提交事务的一部分，又会发生什么情况，后面的写入会覆盖一个尚未提交的值？这被称作<strong>脏写（dirty write）</strong>【28】。在<strong>读已提交</strong>的隔离级别上运行的事务必须防止脏写，通常是延迟第二次写入，直到第一次写入事务提交或中止为止。</p>

<p>通过防止脏写，这个隔离级别避免了一些并发问题：</p>

<ul>
<li>如果事务更新多个对象，脏写会导致不好的结果。例如，考虑 <a href="img/fig7-5.png">图7-5</a>，<a href="img/fig7-5.png">图7-5</a> 以一个二手车销售网站为例，Alice和Bob两个人同时试图购买同一辆车。购买汽车需要两次数据库写入：网站上的商品列表需要更新，以反映买家的购买，销售发票需要发送给买家。在<a href="img/fig7-5.png">图7-5</a>的情况下，销售是属于Bob的（因为他成功更新了商品列表），但发票却寄送给了爱丽丝（因为她成功更新了发票表）。读已提交会阻止这样这样的事故。</li>
<li>但是，提交读取并不能防止<a href="">图7-1</a>中两个计数器增量之间的竞争状态。在这种情况下，第二次写入发生在第一个事务提交后，所以它不是一个脏写。这仍然是不正确的，但是出于不同的原因，在“<a href="#%E9%98%B2%E6%AD%A2%E4%B8%A2%E5%A4%B1%E6%9B%B4%E6%96%B0">防止更新丢失</a>”中将讨论如何使这种计数器增量安全。</li>
</ul>

<p><img src="img/fig7-5.png" alt=""/></p>

<p><strong>图7-5 如果存在脏写，来自不同事务的冲突写入可能会混淆在一起</strong></p>

<h4 id="toc_67">实现读已提交</h4>

<p><strong>读已提交</strong>是一个非常流行的隔离级别。这是Oracle 11g，PostgreSQL，SQL Server 2012，MemSQL和其他许多数据库的默认设置【8】。</p>

<p>最常见的情况是，数据库通过使用<strong>行锁（row-level lock）</strong> 来防止脏写：当事务想要修改特定对象（行或文档）时，它必须首先获得该对象的锁。然后必须持有该锁直到事务被提交或中止。一次只有一个事务可持有任何给定对象的锁；如果另一个事务要写入同一个对象，则必须等到第一个事务提交或中止后，才能获取该锁并继续。这种锁定是读已提交模式（或更强的隔离级别）的数据库自动完成的。</p>

<p>如何防止脏读？一种选择是使用相同的锁，并要求任何想要读取对象的事务来简单地获取该锁，然后在读取之后立即再次释放该锁。这能确保不会读取进行时，对象不会在脏的状态，有未提交的值（因为在那段时间锁会被写入该对象的事务持有）。</p>

<p>但是要求读锁的办法在实践中效果并不好。因为一个长时间运行的写入事务会迫使许多只读事务等到这个慢写入事务完成。这会损失只读事务的响应时间，并且不利于可操作性：因为等待锁，应用某个部分的迟缓可能由于连锁效应，导致其他部分出现问题。</p>

<p>出于这个原因，大多数数据库<sup id="fnref7"><a href="#fn7" rel="footnote">7</a></sup>使用<a href="">图7-4</a>的方式防止脏读：对于写入的每个对象，数据库都会记住旧的已提交值，和由当前持有写入锁的事务设置的新值。当事务正在进行时，任何其他读取对象的事务都会拿到旧值。 只有当新值提交后，事务才会切换到读取新值。</p>

<h3 id="toc_68">快照隔离和可重复读</h3>

<p>如果只从表面上看读已提交隔离级别你就认为它完成了事务所需的一切，那是可以原谅的。它允许<strong>中止</strong>（原子性的要求）；它防止读取不完整的事务结果，并且防止并发写入造成的混合。事实上这些功能非常有用，比起没有事务的系统来，可以提供更多的保证。</p>

<p>但是在使用此隔离级别时，仍然有很多地方可能会产生并发错误。例如<a href="img/fig7-6.png">图7-6</a>说明了读已提交时可能发生的问题。</p>

<p><img src="img/fig7-6.png" alt=""/></p>

<p><strong>图7-6 读取偏差：Alice观察数据库处于不一致的状态</strong></p>

<p>爱丽丝在银行有1000美元的储蓄，分为两个账户，每个500美元。现在一笔事务从她的一个账户中转移了100美元到另一个账户。如果她在事务处理的同时查看其账户余额列表，不幸地在转账事务完成前看到收款账户余额（余额为500美元），而在转账完成后看到另一个转出账户（已经转出100美元，余额400美元）。对爱丽丝来说，现在她的账户似乎只有900美元——看起来100美元已经消失了。</p>

<p>这种异常被称为<strong>不可重复读（nonrepeatable read）</strong>或<strong>读取偏差（read skew）</strong>：如果Alice在事务结束时再次读取账户1的余额，她将看到与她之前的查询中看到的不同的值（600美元）。在读已提交的隔离条件下，<strong>不可重复读</strong>被认为是可接受的：Alice看到的帐户余额时确实在阅读时已经提交了。</p>

<blockquote>
<p>不幸的是，术语<strong>偏差（skew）</strong> 这个词是过载的：以前使用它是因为热点的不平衡工作量（参阅“<a href="ch6.md#%E8%B4%9F%E8%BD%BD%E5%80%BE%E6%96%9C%E4%B8%8E%E6%B6%88%E9%99%A4%E7%83%AD%E7%82%B9">偏斜的负载倾斜与消除热点</a>”），而这里偏差意味着异常的时机。</p>
</blockquote>

<p>对于Alice的情况，这不是一个长期持续的问题。因为如果她几秒钟后刷新银行网站的页面，她很可能会看到一致的帐户余额。但是有些情况下，不能容忍这种暂时的不一致：</p>

<p><strong><em>备份</em></strong></p>

<p>​   进行备份需要复制整个数据库，对大型数据库而言可能需要花费数小时才能完成。备份进程运行时，数据库仍然会接受写入操作。因此备份可能会包含一些旧的部分和一些新的部分。如果从这样的备份中恢复，那么不一致（如消失的钱）就会变成永久的。</p>

<p><strong><em>分析查询和完整性检查</em></strong></p>

<p>​   有时，您可能需要运行一个查询，扫描大部分的数据库。这样的查询在分析中很常见（参阅“<a href="ch3.md#%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86%E8%BF%98%E6%98%AF%E5%88%86%E6%9E%90%EF%BC%9F">事务处理或分析？</a>”），也可能是定期完整性检查（即监视数据损坏）的一部分。如果这些查询在不同时间点观察数据库的不同部分，则可能会返回毫无意义的结果。</p>

<p><strong>快照隔离（snapshot isolation）</strong>【28】是这个问题最常见的解决方案。想法是，每个事务都从数据库的<strong>一致快照（consistent snapshot）</strong> 中读取——也就是说，事务可以看到事务开始时在数据库中提交的所有数据。即使这些数据随后被另一个事务更改，每个事务也只能看到该特定时间点的旧数据。</p>

<p>快照隔离对长时间运行的只读查询（如备份和分析）非常有用。如果查询的数据在查询执行的同时发生变化，则很难理解查询的含义。当一个事务可以看到数据库在某个特定时间点冻结时的一致快照，理解起来就很容易了。</p>

<p>快照隔离是一个流行的功能：PostgreSQL，使用InnoDB引擎的MySQL，Oracle，SQL Server等都支持【23,31,32】。</p>

<h4 id="toc_69">实现快照隔离</h4>

<p>与读取提交的隔离类似，快照隔离的实现通常使用写锁来防止脏写（请参阅“<a href="#%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4">读已提交</a>”），这意味着进行写入的事务会阻止另一个事务修改同一个对象。但是读取不需要任何锁定。从性能的角度来看，快照隔离的一个关键原则是：<strong>读不阻塞写，写不阻塞读</strong>。这允许数据库在处理一致性快照上的长时间查询时，可以正常地同时处理写入操作。且两者间没有任何锁定争用。</p>

<p>为了实现快照隔离，数据库使用了我们看到的用于防止<a href="">图7-4</a>中的脏读的机制的一般化。数据库必须可能保留一个对象的几个不同的提交版本，因为各种正在进行的事务可能需要看到数据库在不同的时间点的状态。因为它并排维护着多个版本的对象，所以这种技术被称为<strong>多版本并发控制（MVCC, multi-version concurrentcy control）</strong>。</p>

<p>如果一个数据库只需要提供<strong>读已提交</strong>的隔离级别，而不提供<strong>快照隔离</strong>，那么保留一个对象的两个版本就足够了：提交的版本和被覆盖但尚未提交的版本。支持快照隔离的存储引擎通常也使用MVCC来实现<strong>读已提交</strong>隔离级别。一种典型的方法是<strong>读已提交</strong>为每个查询使用单独的快照，而<strong>快照隔离</strong>对整个事务使用相同的快照。</p>

<p><a href="">图7-7</a>说明了如何在PostgreSQL中实现基于MVCC的快照隔离【31】（其他实现类似）。当一个事务开始时，它被赋予一个唯一的，永远增长<sup id="fnref8"><a href="#fn8" rel="footnote">8</a></sup>的事务ID（<code>txid</code>）。每当事务向数据库写入任何内容时，它所写入的数据都会被标记上写入者的事务ID。</p>

<p><img src="img/fig7-7.png" alt=""/></p>

<p><strong>图7-7 使用多版本对象实现快照隔离</strong></p>

<p>表中的每一行都有一个 <code>created_by</code> 字段，其中包含将该行插入到表中的的事务ID。此外，每行都有一个 <code>deleted_by</code> 字段，最初是空的。如果某个事务删除了一行，那么该行实际上并未从数据库中删除，而是通过将 <code>deleted_by</code> 字段设置为请求删除的事务的ID来标记为删除。在稍后的时间，当确定没有事务可以再访问已删除的数据时，数据库中的垃圾收集过程会将所有带有删除标记的行移除，并释放其空间。<sup id="fnref9"><a href="#fn9" rel="footnote">9</a></sup></p>

<p><code>UPDATE</code> 操作在内部翻译为 <code>DELETE</code> 和 <code>INSERT</code> 。例如，在<a href="">图7-7</a>中，事务13 从账户2 中扣除100美元，将余额从500美元改为400美元。实际上包含两条账户2 的记录：余额为 $500 的行被标记为<strong>被事务13删除</strong>，余额为 $400 的行<strong>由事务13创建</strong>。</p>

<h4 id="toc_70">观察一致性快照的可见性规则</h4>

<p>当一个事务从数据库中读取时，事务ID用于决定它可以看见哪些对象，看不见哪些对象。通过仔细定义可见性规则，数据库可以向应用程序呈现一致的数据库快照。工作如下：</p>

<ol>
<li>在每次事务开始时，数据库列出当时所有其他（尚未提交或尚未中止）的事务清单，即使之后提交了，这些事务已执行的任何写入也都会被忽略。</li>
<li>被中止事务所执行的任何写入都将被忽略。</li>
<li>由具有较晚事务ID（即，在当前事务开始之后开始的）的事务所做的任何写入都被忽略，而不管这些事务是否已经提交。</li>
<li>所有其他写入，对应用都是可见的。</li>
</ol>

<p>这些规则适用于创建和删除对象。在<a href="">图7-7</a>中，当事务12 从账户2 读取时，它会看到 $500 的余额，因为 $500 余额的删除是由事务13 完成的（根据规则3，事务12 看不到事务13 执行的删除），且400美元记录的创建也是不可见的（按照相同的规则）。</p>

<p>换句话说，如果以下两个条件都成立，则可见一个对象：</p>

<ul>
<li>读事务开始时，创建该对象的事务已经提交。</li>
<li>对象未被标记为删除，或如果被标记为删除，请求删除的事务在读事务开始时尚未提交。</li>
</ul>

<p>长时间运行的事务可能会长时间使用快照，并继续读取（从其他事务的角度来看）早已被覆盖或删除的值。由于从来不原地更新值，而是每次值改变时创建一个新的版本，数据库可以在提供一致快照的同时只产生很小的额外开销。</p>

<h4 id="toc_71">索引和快照隔离</h4>

<p>索引如何在多版本数据库中工作？一种选择是使索引简单地指向对象的所有版本，并且需要索引查询来过滤掉当前事务不可见的任何对象版本。当垃圾收集删除任何事务不再可见的旧对象版本时，相应的索引条目也可以被删除。</p>

<p>在实践中，许多实现细节决定了多版本并发控制的性能。例如，如果同一对象的不同版本可以放入同一个页面中，PostgreSQL的优化可以避免更新索引【31】。</p>

<p>在CouchDB，Datomic和LMDB中使用另一种方法。虽然它们也使用<a href="ch2.md#B%E6%A0%91">B树</a>，但它们使用的是一种<strong>仅追加/写时拷贝（append-only/copy-on-write）</strong> 的变体，它们在更新时不覆盖树的页面，而为每个修改页面创建一份副本。从父页面直到树根都会级联更新，以指向它们子页面的新版本。任何不受写入影响的页面都不需要被复制，并且保持不变【33,34,35】。</p>

<p>使用仅追加的B树，每个写入事务（或一批事务）都会创建一颗新的B树，当创建时，从该特定树根生长的树就是数据库的一个一致性快照。没必要根据事务ID过滤掉对象，因为后续写入不能修改现有的B树；它们只能创建新的树根。但这种方法也需要一个负责压缩和垃圾收集的后台进程。</p>

<h4 id="toc_72">可重复读与命名混淆</h4>

<p>快照隔离是一个有用的隔离级别，特别对于只读事务而言。但是，许多数据库实现了它，却用不同的名字来称呼。在Oracle中称为<strong>可序列化（Serializable）</strong>的，在PostgreSQL和MySQL中称为<strong>可重复读（repeatable read）</strong>【23】。</p>

<p>这种命名混淆的原因是SQL标准没有<strong>快照隔离</strong>的概念，因为标准是基于System R 1975年定义的隔离级别【2】，那时候<strong>快照隔离</strong>尚未发明。相反，它定义了<strong>可重复读</strong>，表面上看起来与快照隔离很相似。 PostgreSQL和MySQL称其<strong>快照隔离</strong>级别为<strong>可重复读（repeatable read）</strong>，因为这样符合标准要求，所以它们可以声称自己“标准兼容”。</p>

<p>不幸的是，SQL标准对隔离级别的定义是有缺陷的——模糊，不精确，并不像标准应有的样子独立于实现【28】。有几个数据库实现了可重复读，但它们实际提供的保证存在很大的差异，尽管表面上是标准化的【23】。在研究文献【29,30】中已经有了可重复读的正式定义，但大多数的实现并不能满足这个正式定义。最后，IBM DB2使用“可重复读”来引用可串行化【8】。</p>

<p>结果，没有人真正知道<strong>可重复读</strong>的意思。</p>

<h3 id="toc_73">防止丢失更新</h3>

<p>到目前为止已经讨论的<strong>读已提交</strong>和<strong>快照隔离</strong>级别，主要保证了<strong>只读事务在并发写入时</strong>可以看到什么。却忽略了两个事务并发写入的问题——我们只讨论了<a href="#%E8%84%8F%E5%86%99">脏写</a>，一种特定类型的写-写冲突是可能出现的。</p>

<p>并发的写入事务之间还有其他几种有趣的冲突。其中最着名的是<strong>丢失更新（lost update）</strong> 问题，如<a href="">图7-1</a>所示，以两个并发计数器增量为例。</p>

<p>如果应用从数据库中读取一些值，修改它并写回修改的值（读取-修改-写入序列），则可能会发生丢失更新的问题。如果两个事务同时执行，则其中一个的修改可能会丢失，因为第二个写入的内容并没有包括第一个事务的修改（有时会说后面写入<strong>狠揍（clobber）</strong> 了前面的写入）这种模式发生在各种不同的情况下：</p>

<ul>
<li>增加计数器或更新账户余额（需要读取当前值，计算新值并写回更新后的值）</li>
<li>在复杂值中进行本地修改：例如，将元素添加到JSON文档中的一个列表（需要解析文档，进行更改并写回修改的文档）</li>
<li>两个用户同时编辑wiki页面，每个用户通过将整个页面内容发送到服务器来保存其更改，覆写数据库中当前的任何内容。</li>
</ul>

<p>这是一个普遍的问题，所以已经开发了各种解决方案。</p>

<h4 id="toc_74">原子写</h4>

<p>许多数据库提供了原子更新操作，从而消除了在应用程序代码中执行读取-修改-写入序列的需要。如果你的代码可以用这些操作来表达，那这通常是最好的解决方案。例如，下面的指令在大多数关系数据库中是并发安全的：</p>

<pre><code class="language-sql">UPDATE counters SET value = value + 1 WHERE key = &#39;foo&#39;;
</code></pre>

<p>类似地，像MongoDB这样的文档数据库提供了对JSON文档的一部分进行本地修改的原子操作，Redis提供了修改数据结构（如优先级队列）的原子操作。并不是所有的写操作都可以用原子操作的方式来表达，例如维基页面的更新涉及到任意文本编辑<sup id="fnref10"><a href="#fn10" rel="footnote">10</a></sup>，但是在可以使用原子操作的情况下，它们通常是最好的选择。</p>

<p>原子操作通常通过在读取对象时，获取其上的排它锁来实现。以便更新完成之前没有其他事务可以读取它。这种技术有时被称为<strong>游标稳定性（cursor stability）</strong>【36,37】。另一个选择是简单地强制所有的原子操作在单一线程上执行。</p>

<p>不幸的是，ORM框架很容易意外地执行不安全的读取-修改-写入序列，而不是使用数据库提供的原子操作【38】。如果你知道自己在做什么那当然不是问题，但它经常产生那种很难测出来的微妙Bug。</p>

<h4 id="toc_75">显式锁定</h4>

<p>如果数据库的内置原子操作没有提供必要的功能，防止丢失更新的另一个选择是让应用程序显式地锁定将要更新的对象。然后应用程序可以执行读取-修改-写入序列，如果任何其他事务尝试同时读取同一个对象，则强制等待，直到第一个<strong>读取-修改-写入序列</strong>完成。</p>

<p>例如，考虑一个多人游戏，其中几个玩家可以同时移动相同的棋子。在这种情况下，一个原子操作可能是不够的，因为应用程序还需要确保玩家的移动符合游戏规则，这可能涉及到一些不能合理地用数据库查询实现的逻辑。但你可以使用锁来防止两名玩家同时移动相同的棋子，如例7-1所示。</p>

<p><strong>例7-1 显式锁定行以防止丢失更新</strong></p>

<pre><code class="language-plsql">BEGIN TRANSACTION;
SELECT * FROM figures
    WHERE name = &#39;robot&#39; AND game_id = 222
FOR UPDATE;

-- 检查玩家的操作是否有效，然后更新先前SELECT返回棋子的位置。
UPDATE figures SET position = &#39;c4&#39; WHERE id = 1234;
COMMIT;
</code></pre>

<ul>
<li><code>FOR UPDATE</code>子句告诉数据库应该对该查询返回的所有行加锁。</li>
</ul>

<p>这是有效的，但要做对，你需要仔细考虑应用逻辑。忘记在代码某处加锁很容易引入竞争条件。</p>

<h4 id="toc_76">自动检测丢失的更新</h4>

<p>原子操作和锁是通过强制<strong>读取-修改-写入序列</strong>按顺序发生，来防止丢失更新的方法。另一种方法是允许它们并行执行，如果事务管理器检测到丢失更新，则中止事务并强制它们重试其<strong>读取-修改-写入序列</strong>。</p>

<p>这种方法的一个优点是，数据库可以结合快照隔离高效地执行此检查。事实上，PostgreSQL的可重复读，Oracle的可串行化和SQL Server的快照隔离级别，都会自动检测到丢失更新，并中止惹麻烦的事务。但是，MySQL/InnoDB的可重复读并不会检测<strong>丢失更新</strong>【23】。一些作者【28,30】认为，数据库必须能防止丢失更新才称得上是提供了<strong>快照隔离</strong>，所以在这个定义下，MySQL下不提供快照隔离。</p>

<p>丢失更新检测是一个很好的功能，因为它不需要应用代码使用任何特殊的数据库功能，你可能会忘记使用锁或原子操作，从而引入错误；但丢失更新的检测是自动发生的，因此不太容易出错。</p>

<h4 id="toc_77">比较并设置（CAS）</h4>

<p>在不提供事务的数据库中，有时会发现一种原子操作：<strong>比较并设置（CAS, Compare And Set）</strong>（先前在“<a href="">单对象写入</a>”中提到）。此操作的目的是为了避免丢失更新：只有当前值从上次读取时一直未改变，才允许更新发生。如果当前值与先前读取的值不匹配，则更新不起作用，且必须重试读取-修改-写入序列。</p>

<p>例如，为了防止两个用户同时更新同一个wiki页面，可以尝试类似这样的方式，只有当用户开始编辑页面内容时，才会发生更新：</p>

<pre><code class="language-sql">-- 根据数据库的实现情况，这可能也可能不安全
UPDATE wiki_pages SET content = &#39;新内容&#39;
  WHERE id = 1234 AND content = &#39;旧内容&#39;;
</code></pre>

<p>如果内容已经更改并且不再与“旧内容”相匹配，则此更新将不起作用，因此您需要检查更新是否生效，必要时重试。但是，如果数据库允许<code>WHERE</code>子句从旧快照中读取，则此语句可能无法防止丢失更新，因为即使发生了另一个并发写入，<code>WHERE</code>条件也可能为真。在依赖数据库的CAS操作前要检查其是否安全。</p>

<h4 id="toc_78">冲突解决和复制</h4>

<p>在复制数据库中（参见<a href="ch5.md">第5章</a>），防止丢失的更新需要考虑另一个维度：由于在多个节点上存在数据副本，并且在不同节点上的数据可能被并发地修改，因此需要采取一些额外的步骤来防止丢失更新。</p>

<p>锁和CAS操作假定只有一个最新的数据副本。但是多主或无主复制的数据库通常允许多个写入并发执行，并异步复制到副本上，因此无法保证只有一个最新数据的副本。所以基于锁或CAS操作的技术不适用于这种情况。 （我们将在“<a href="ch9.md#%E7%BA%BF%E6%80%A7%E5%8C%96">线性化</a>”中更详细地讨论这个问题。）</p>

<p>相反，如“<a href="ch5.md#%E6%A3%80%E6%B5%8B%E5%B9%B6%E5%8F%91%E5%86%99%E5%85%A5">检测并发写入</a>”一节所述，这种复制数据库中的一种常见方法是允许并发写入创建多个冲突版本的值（也称为兄弟），并使用应用代码或特殊数据结构在事实发生之后解决和合并这些版本。</p>

<p>原子操作可以在复制的上下文中很好地工作，尤其当它们具有可交换性时（即，可以在不同的副本上以不同的顺序应用它们，且仍然可以得到相同的结果）。例如，递增计数器或向集合添加元素是可交换的操作。这是Riak 2.0数据类型背后的思想，它可以防止复制副本丢失更新。当不同的客户端同时更新一个值时，Riak自动将更新合并在一起，以免丢失更新【39】。</p>

<p>另一方面，最后写入胜利（LWW）的冲突解决方法很容易丢失更新，如“<a href="ch5.md#%E6%9C%80%E5%90%8E%E5%86%99%E5%85%A5%E8%83%9C%E5%88%A9%EF%BC%88%E4%B8%A2%E5%BC%83%E5%B9%B6%E5%8F%91%E5%86%99%E5%85%A5%EF%BC%89">最后写入胜利（丢弃并发写入）</a>”中所述。不幸的是，LWW是许多复制数据库中的默认方案。</p>

<h4 id="toc_79">写入偏差与幻读</h4>

<p>前面的章节中，我们看到了<strong>脏写</strong>和<strong>丢失更新</strong>，当不同的事务并发地尝试写入相同的对象时，会出现这两种竞争条件。为了避免数据损坏，这些竞争条件需要被阻止——既可以由数据库自动执行，也可以通过锁和原子写操作这类手动安全措施来防止。</p>

<p>但是，并发写入间可能发生的竞争条件还没有完。在本节中，我们将看到一些更微妙的冲突例子。</p>

<p>首先，想象一下这个例子：你正在为医院写一个医生轮班管理程序。医院通常会同时要求几位医生待命，但底线是至少有一位医生在待命。医生可以放弃他们的班次（例如，如果他们自己生病了），只要至少有一个同事在这一班中继续工作【40,41】。</p>

<p>现在想象一下，Alice和Bob是两位值班医生。两人都感到不适，所以他们都决定请假。不幸的是，他们恰好在同一时间点击按钮下班。<a href="img/fig7-8.png">图7-8</a>说明了接下来的事情。</p>

<p><img src="img/fig7-8.png" alt=""/></p>

<p><strong>图7-8 写入偏差导致应用程序错误的示例</strong></p>

<p>在两个事务中，应用首先检查是否有两个或以上的医生正在值班；如果是的话，它就假定一名医生可以安全地休班。由于数据库使用快照隔离，两次检查都返回 2 ，所以两个事务都进入下一个阶段。Alice更新自己的记录休班了，而Bob也做了一样的事情。两个事务都成功提交了，现在没有医生值班了。违反了至少有一名医生在值班的要求。</p>

<h4 id="toc_80">写偏差的特征</h4>

<p>这种异常称为<strong>写偏差</strong>【28】。它既不是<strong>脏写</strong>，也不是<strong>丢失更新</strong>，因为这两个事务正在更新两个不同的对象（Alice和Bob各自的待命记录）。在这里发生的冲突并不是那么明显，但是这显然是一个竞争条件：如果两个事务一个接一个地运行，那么第二个医生就不能歇班了。异常行为只有在事务并发进行时才有可能。</p>

<p>可以将写入偏差视为丢失更新问题的一般化。如果两个事务读取相同的对象，然后更新其中一些对象（不同的事务可能更新不同的对象），则可能发生写入偏差。在多个事务更新同一个对象的特殊情况下，就会发生脏写或丢失更新（取决于时机）。</p>

<p>我们看到，有各种不同的方法来防止丢失的更新。随着写偏差，我们的选择更受限制：</p>

<ul>
<li>由于涉及多个对象，单对象的原子操作不起作用。</li>
<li>不幸的是，在一些快照隔离的实现中，自动检测丢失更新对此并没有帮助。在PostgreSQL的可重复读，MySQL/InnoDB的可重复读，Oracle可序列化或SQL Server的快照隔离级别中，都不会自动检测写入偏差【23】。自动防止写入偏差需要真正的可序列化隔离（请参见“<a href="#%E5%8F%AF%E5%BA%8F%E5%88%97%E5%8C%96">可序列化</a>”）。</li>
<li>某些数据库允许配置约束，然后由数据库强制执行（例如，唯一性，外键约束或特定值限制）。但是为了指定至少有一名医生必须在线，需要一个涉及多个对象的约束。大多数数据库没有内置对这种约束的支持，但是你可以使用触发器，或者物化视图来实现它们，这取决于不同的数据库【42】。</li>
<li>如果无法使用可序列化的隔离级别，则此情况下的次优选项可能是显式锁定事务所依赖的行。在例子中，你可以写下如下的代码：</li>
</ul>

<pre><code class="language-sql">BEGIN TRANSACTION;
SELECT * FROM doctors
  WHERE on_call = TRUE 
  AND shift_id = 1234 FOR UPDATE;

UPDATE doctors
  SET on_call = FALSE
  WHERE name = &#39;Alice&#39; 
  AND shift_id = 1234;
  
COMMIT;
</code></pre>

<ul>
<li>和以前一样，<code>FOR UPDATE</code>告诉数据库锁定返回的所有行用于更新。</li>
</ul>

<h4 id="toc_81">写偏差的更多例子</h4>

<p>写偏差乍看像是一个深奥的问题，但一旦意识到这一点，很容易会注意到更多可能的情况。以下是一些例子：</p>

<p><strong><em>会议室预订系统</em></strong></p>

<p>假设你想强制执行，同一时间不能同时在两个会议室预订【43】。当有人想要预订时，首先检查是否存在相互冲突的预订（即预订时间范围重叠的同一房间），如果没有找到，则创建会议（请参见示例7-2）<sup id="fnref11"><a href="#fn11" rel="footnote">11</a></sup>。</p>

<p><strong>例7-2 会议室预订系统试图避免重复预订（在快照隔离下不安全）</strong></p>

<pre><code class="language-sql">BEGIN TRANSACTION;

-- 检查所有现存的与12:00~13:00重叠的预定
SELECT COUNT(*) FROM bookings
WHERE room_id = 123 AND 
    end_time &gt; &#39;2015-01-01 12:00&#39; AND start_time &lt; &#39;2015-01-01 13:00&#39;;

-- 如果之前的查询返回0
INSERT INTO bookings(room_id, start_time, end_time, user_id)
  VALUES (123, &#39;2015-01-01 12:00&#39;, &#39;2015-01-01 13:00&#39;, 666);

COMMIT;
</code></pre>

<p>不幸的是，快照隔离并不能防止另一个用户同时插入冲突的会议。为了确保不会遇到调度冲突，你又需要可序列化的隔离级别了。</p>

<p><strong><em>多人游戏</em></strong></p>

<p>在<a href="">例7-1</a>中，我们使用一个锁来防止丢失更新（也就是确保两个玩家不能同时移动同一个棋子）。但是锁定并不妨碍玩家将两个不同的棋子移动到棋盘上的相同位置，或者采取其他违反游戏规则的行为。按照您正在执行的规则类型，也许可以使用唯一约束，否则您很容易发生写入偏差。</p>

<p><strong><em>抢注用户名</em></strong></p>

<p>在每个用户拥有唯一用户名的网站上，两个用户可能会尝试同时创建具有相同用户名的帐户。可以在事务检查名称是否被抢占，如果没有则使用该名称创建账户。但是像在前面的例子中那样，在快照隔离下这是不安全的。幸运的是，唯一约束是一个简单的解决办法（第二个事务在提交时会因为违反用户名唯一约束而被中止）。</p>

<p><strong><em>防止双重开支</em></strong></p>

<p>允许用户花钱或积分的服务，需要检查用户的支付数额不超过其余额。可以通过在用户的帐户中插入一个试探性的消费项目来实现这一点，列出帐户中的所有项目，并检查总和是否为正值【44】。有了写入偏差，可能会发生两个支出项目同时插入，一起导致余额变为负值，但这两个事务都不会注意到另一个。</p>

<h4 id="toc_82">导致写入偏差的幻读</h4>

<p>所有这些例子都遵循类似的模式：</p>

<ol>
<li><p>一个<code>SELECT</code>查询找出符合条件的行，并检查是否符合一些要求。（例如：至少有两名医生在值班；不存在对该会议室同一时段的预定；棋盘上的位置没有被其他棋子占据；用户名还没有被抢注；账户里还有足够余额）</p></li>
<li><p>按照第一个查询的结果，应用代码决定是否继续。（可能会继续操作，也可能中止并报错）</p></li>
<li><p>如果应用决定继续操作，就执行写入（插入、更新或删除），并提交事务。</p>

<p>这个写入的效果改变了步骤2 中的先决条件。换句话说，如果在提交写入后，重复执行一次步骤1 的SELECT查询，将会得到不同的结果。因为写入改变符合搜索条件的行集（现在少了一个医生值班，那时候的会议室现在已经被预订了，棋盘上的这个位置已经被占据了，用户名已经被抢注，账户余额不够了）。</p></li>
</ol>

<p>这些步骤可能以不同的顺序发生。例如可以首先进行写入，然后进行SELECT查询，最后根据查询结果决定是放弃还是提交。</p>

<p>在医生值班的例子中，在步骤3中修改的行，是步骤1中返回的行之一，所以我们可以通过锁定步骤1 中的行（<code>SELECT FOR UPDATE</code>）来使事务安全并避免写入偏差。但是其他四个例子是不同的：它们检查是否<strong>不存在</strong>某些满足条件的行，写入会<strong>添加</strong>一个匹配相同条件的行。如果步骤1中的查询没有返回任何行，则<code>SELECT FOR UPDATE</code>锁不了任何东西。</p>

<p>这种效应：一个事务中的写入改变另一个事务的搜索查询的结果，被称为<strong>幻读</strong>【3】。快照隔离避免了只读查询中幻读，但是在像我们讨论的例子那样的读写事务中，幻读会导致特别棘手的写入偏差情况。</p>

<h4 id="toc_83">物化冲突</h4>

<p>如果幻读的问题是没有对象可以加锁，也许可以人为地在数据库中引入一个锁对象？</p>

<p>例如，在会议室预订的场景中，可以想象创建一个关于时间槽和房间的表。此表中的每一行对应于特定时间段（例如15分钟）的特定房间。可以提前插入房间和时间的所有可能组合行（例如接下来的六个月）。</p>

<p>现在，要创建预订的事务可以锁定（<code>SELECT FOR UPDATE</code>）表中与所需房间和时间段对应的行。在获得锁定之后，它可以检查重叠的预订并像以前一样插入新的预订。请注意，这个表并不是用来存储预订相关的信息——它完全就是一组锁，用于防止同时修改同一房间和时间范围内的预订。</p>

<p>这种方法被称为<strong>物化冲突（materializing conflicts）</strong>，因为它将幻读变为数据库中一组具体行上的锁冲突【11】。不幸的是，弄清楚如何物化冲突可能很难，也很容易出错，而让并发控制机制泄漏到应用数据模型是很丑陋的做法。出于这些原因，如果没有其他办法可以实现，物化冲突应被视为最后的手段。在大多数情况下。<strong>可序列化（Serializable）</strong> 的隔离级别是更可取的。</p>

<h2 id="toc_84">可序列化</h2>

<p>在本章中，已经看到了几个易于出现竞争条件的事务例子。<strong>读已提交</strong>和<strong>快照隔离</strong>级别会阻止某些竞争条件，但不会阻止另一些。我们遇到了一些特别棘手的例子，<strong>写入偏差</strong>和<strong>幻读</strong>。这是一个可悲的情况：</p>

<ul>
<li>隔离级别难以理解，并且在不同的数据库中实现的不一致（例如，“可重复读”的含义天差地别）。</li>
<li>光检查应用代码很难判断在特定的隔离级别运行是否安全。 特别是在大型应用程序中，您可能并不知道并发发生的所有事情。</li>
<li>没有检测竞争条件的好工具。原则上来说，静态分析可能会有帮助【26】，但研究中的技术还没法实际应用。并发问题的测试是很难的，因为它们通常是非确定性的 —— 只有在倒霉的时机下才会出现问题。</li>
</ul>

<p>这不是一个新问题，从20世纪70年代以来就一直是这样了，当时首先引入了较弱的隔离级别【2】。一直以来，研究人员的答案都很简单：使用<strong>可序列化（serializable）</strong> 的隔离级别！</p>

<p><strong>可序列化（Serializability）</strong>隔离通常被认为是最强的隔离级别。它保证即使事务可以并行执行，最终的结果也是一样的，就好像它们没有任何并发性，连续挨个执行一样。因此数据库保证，如果事务在单独运行时正常运行，则它们在并发运行时继续保持正确 —— 换句话说，数据库可以防止<strong>所有</strong>可能的竞争条件。</p>

<p>但如果可序列化隔离级别比弱隔离级别的烂摊子要好得多，那为什么没有人见人爱？为了回答这个问题，我们需要看看实现可序列化的选项，以及它们如何执行。目前大多数提供可序列化的数据库都使用了三种技术之一，本章的剩余部分将会介绍这些技术。</p>

<ul>
<li>字面意义上地串行顺序执行事务（参见“<a href="#%E7%9C%9F%E7%9A%84%E4%B8%B2%E8%A1%8C%E6%89%A7%E8%A1%8C">真的串行执行</a>”）</li>
<li><strong>两相锁定（2PL, two-phase locking）</strong>，几十年来唯一可行的选择。（参见“<a href="#%E4%B8%A4%E9%98%B6%E6%AE%B5%E9%94%81%E5%AE%9A%EF%BC%882PL%EF%BC%89">两相锁定（2PL）</a>”）</li>
<li>乐观并发控制技术，例如<strong>可序列化的快照隔离（serializable snapshot isolation）</strong>（参阅“<a href="#%E5%BA%8F%E5%88%97%E5%8C%96%E5%BF%AB%E7%85%A7%E9%9A%94%E7%A6%BB%EF%BC%88SSI%EF%BC%89">可序列化的快照隔离（SSI）</a>”</li>
</ul>

<p>现在将主要在单节点数据库的背景下讨论这些技术；在<a href="ch9.md">第9章</a>中，我们将研究如何将它们推广到涉及分布式系统中多个节点的事务。</p>

<h4 id="toc_85">真的串行执行</h4>

<p>避免并发问题的最简单方法就是完全不要并发：在单个线程上按顺序一次只执行一个事务。这样做就完全绕开了检测/防止事务间冲突的问题，由此产生的隔离，正是可序列化的定义。</p>

<p>尽管这似乎是一个明显的主意，但数据库设计人员只是在2007年左右才决定，单线程循环执行事务是可行的【45】。如果多线程并发在过去的30年中被认为是获得良好性能的关键所在，那么究竟是什么改变致使单线程执行变为可能呢？</p>

<p>两个进展引发了这个反思：</p>

<ul>
<li>RAM足够便宜了，许多场景现在都可以将完整的活跃数据集保存在内存中。（参阅“<a href="ch3.md#%E5%9C%A8%E5%86%85%E5%AD%98%E4%B8%AD%E5%AD%98%E5%82%A8%E4%B8%80%E5%88%87">在内存中存储一切</a>”）。当事务需要访问的所有数据都在内存中时，事务处理的执行速度要比等待数据从磁盘加载时快得多。</li>
<li>数据库设计人员意识到OLTP事务通常很短，而且只进行少量的读写操作（参阅“<a href="ch3.md#%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86%E8%BF%98%E6%98%AF%E5%88%86%E6%9E%90%EF%BC%9F">事务处理或分析？</a>”）。相比之下，长时间运行的分析查询通常是只读的，因此它们可以在串行执行循环之外的一致快照（使用快照隔离）上运行。</li>
</ul>

<p>串行执行事务的方法在VoltDB/H-Store，Redis和Datomic中实现【46,47,48】。设计用于单线程执行的系统有时可以比支持并发的系统更好，因为它可以避免锁的协调开销。但是其吞吐量仅限于单个CPU核的吞吐量。为了充分利用单一线程，需要与传统形式不同的结构的事务。</p>

<h4 id="toc_86">在存储过程中封装事务</h4>

<p>​   在数据库的早期阶段，意图是数据库事务可以包含整个用户活动流程。例如，预订机票是一个多阶段的过程（搜索路线，票价和可用座位，决定行程，在每段行程的航班上订座，输入乘客信息，付款）。数据库设计者认为，如果整个过程是一个事务，那么它就可以被原子化地执行。</p>

<p>​   不幸的是，人类做出决定和回应的速度非常缓慢。如果数据库事务需要等待来自用户的输入，则数据库需要支持潜在的大量并发事务，其中大部分是空闲的。大多数数据库不能高效完成这项工作，因此几乎所有的OLTP应用程序都避免在事务中等待交互式的用户输入，以此来保持事务的简短。在Web上，这意味着事务在同一个HTTP请求中被提交——一个事务不会跨越多个请求。一个新的HTTP请求开始一个新的事务。</p>

<p>​   即使人类已经找到了关键路径，事务仍然以交互式的客户端/服务器风格执行，一次一个语句。应用程序进行查询，读取结果，可能根据第一个查询的结果进行另一个查询，依此类推。查询和结果在应用程序代码（在一台机器上运行）和数据库服务器（在另一台机器上）之间来回发送。</p>

<p>​   在这种交互式的事务方式中，应用程序和数据库之间的网络通信耗费了大量的时间。如果不允许在数据库中进行并发处理，且一次只处理一个事务，则吞吐量将会非常糟糕，因为数据库大部分的时间都花费在等待应用程序发出当前事务的下一个查询。在这种数据库中，为了获得合理的性能，需要同时处理多个事务。</p>

<p>​   出于这个原因，具有单线程串行事务处理的系统不允许交互式的多语句事务。取而代之，应用程序必须提前将整个事务代码作为存储过程提交给数据库。这些方法之间的差异如<a href="img/fig7-9.png">图7-9</a> 所示。如果事务所需的所有数据都在内存中，则存储过程可以非常快地执行，而不用等待任何网络或磁盘I/O。</p>

<p><img src="img/fig7-9.png" alt=""/></p>

<p><strong>图7-9 交互式事务和存储过程之间的区别（使用图7-8的示例事务）</strong></p>

<h4 id="toc_87">存储过程的优点和缺点</h4>

<p>存储过程在关系型数据库中已经存在了一段时间了，自1999年以来它们一直是SQL标准（SQL/PSM）的一部分。出于各种原因，它们的名声有点不太好：</p>

<ul>
<li>每个数据库厂商都有自己的存储过程语言（Oracle有PL/SQL，SQL Server有T-SQL，PostgreSQL有PL/pgSQL等）。这些语言并没有跟上通用编程语言的发展，所以从今天的角度来看，它们看起来相当丑陋和陈旧，而且缺乏大多数编程语言中能找到的库的生态系统。</li>
<li>与应用服务器相，比在数据库中运行的管理困难，调试困难，版本控制和部署起来也更为尴尬，更难测试，更难和用于监控的指标收集系统相集成。</li>
<li>数据库通常比应用服务器对性能敏感的多，因为单个数据库实例通常由许多应用服务器共享。数据库中一个写得不好的存储过程（例如，占用大量内存或CPU时间）会比在应用服务器中相同的代码造成更多的麻烦。</li>
</ul>

<p>但是这些问题都是可以克服的。现代的存储过程实现放弃了PL/SQL，而是使用现有的通用编程语言：VoltDB使用Java或Groovy，Datomic使用Java或Clojure，而Redis使用Lua。</p>

<p><strong>存储过程与内存存储</strong>，使得在单个线程上执行所有事务变得可行。由于不需要等待I/O，且避免了并发控制机制的开销，它们可以在单个线程上实现相当好的吞吐量。</p>

<p>VoltDB还使用存储过程进行复制：但不是将事务的写入结果从一个节点复制到另一个节点，而是在每个节点上执行相同的存储过程。因此VoltDB要求存储过程是<strong>确定性的</strong>（在不同的节点上运行时，它们必须产生相同的结果）。举个例子，如果事务需要使用当前的日期和时间，则必须通过特殊的确定性API来实现。</p>

<h4 id="toc_88">分区</h4>

<p>顺序执行所有事务使并发控制简单多了，但数据库的事务吞吐量被限制为单机单核的速度。只读事务可以使用快照隔离在其它地方执行，但对于写入吞吐量较高的应用，单线程事务处理器可能成为一个严重的瓶颈。</p>

<p>为了扩展到多个CPU核心和多个节点，可以对数据进行分区（参见<a href="ch6.md">第6章</a>），在VoltDB中支持这样做。如果你可以找到一种对数据集进行分区的方法，以便每个事务只需要在单个分区中读写数据，那么每个分区就可以拥有自己独立运行的事务处理线程。在这种情况下可以为每个分区指派一个独立的CPU核，事务吞吐量就可以与CPU核数保持线性扩展【47】。</p>

<p>但是，对于需要访问多个分区的任何事务，数据库必须在触及的所有分区之间协调事务。存储过程需要跨越所有分区锁定执行，以确保整个系统的可串行性。</p>

<p>由于跨分区事务具有额外的协调开销，所以它们比单分区事务慢得多。 VoltDB报告的吞吐量大约是每秒1000个跨分区写入，比单分区吞吐量低几个数量级，并且不能通过增加更多的机器来增加【49】。</p>

<p>事务是否可以是划分至单个分区很大程度上取决于应用数据的结构。简单的键值数据通常可以非常容易地进行分区，但是具有多个二级索引的数据可能需要大量的跨分区协调（参阅“<a href="ch6.md#%E5%88%86%E7%89%87%E4%B8%8E%E6%AC%A1%E7%BA%A7%E7%B4%A2%E5%BC%95">分片与次级索引</a>”）。</p>

<h4 id="toc_89">串行执行小结</h4>

<p>在特定约束条件下，真的串行执行事务，已经成为一种实现可序列化隔离等级的可行办法。</p>

<ul>
<li>每个事务都必须小而快，只要有一个缓慢的事务，就会拖慢所有事务处理。</li>
<li>仅限于活跃数据集可以放入内存的情况。很少访问的数据可能会被移动到磁盘，但如果需要在单线程执行的事务中访问，系统就会变得非常慢<sup id="fnref12"><a href="#fn12" rel="footnote">12</a></sup>。</li>
<li>写入吞吐量必须低到能在单个CPU核上处理，如若不然，事务需要能划分至单个分区，且不需要跨分区协调。</li>
<li>跨分区事务是可能的，但是它们的使用程度有很大的限制。</li>
</ul>

<h3 id="toc_90">两阶段锁定（2PL）</h3>

<p>大约30年来，在数据库中只有一种广泛使用的序列化算法：<strong>两阶段锁定（2PL，two-phase locking）</strong> <sup id="fnref13"><a href="#fn13" rel="footnote">13</a></sup></p>

<blockquote>
<h4 id="toc_91">2PL不是2PC</h4>

<p>请注意，虽然两阶段锁定（2PL）听起来非常类似于两阶段提交（2PC），但它们是完全不同的东西。我们将在第9章讨论2PC。</p>
</blockquote>

<p>之前我们看到锁通常用于防止脏写（参阅“<a href="%E6%B2%A1%E6%9C%89%E8%84%8F%E5%86%99">没有脏写</a>”一节）：如果两个事务同时尝试写入同一个对象，则锁可确保第二个写入必须等到第一个写入完成事务（中止或提交），然后才能继续。</p>

<p>两阶段锁定定类似，但使锁的要求更强。只要没有写入，就允许多个事务同时读取同一个对象。但对象只要有写入（修改或删除），就需要<strong>独占访问（exclusive access）</strong> 权限：</p>

<ul>
<li>如果事务A读取了一个对象，并且事务B想要写入该对象，那么B必须等到A提交或中止才能继续。 （这确保B不能在A底下意外地改变对象。）</li>
<li>如果事务A写入了一个对象，并且事务B想要读取该对象，则B必须等到A提交或中止才能继续。 （像<a href="">图7-1</a>那样读取旧版本的对象在2PL下是不可接受的。）</li>
</ul>

<p>在2PL中，写入不仅会阻塞其他写入，也会阻塞读，反之亦然。快照隔离使得<strong>读不阻塞写，写也不阻塞读</strong>（参阅“<a href="#%E5%AE%9E%E7%8E%B0%E5%BF%AB%E7%85%A7%E9%9A%94%E7%A6%BB">实现快照隔离</a>”），这是2PL和快照隔离之间的关键区别。另一方面，因为2PL提供了可序列化的性质，它可以防止早先讨论的所有竞争条件，包括丢失更新和写入偏差。</p>

<h4 id="toc_92">实现两阶段锁</h4>

<p>2PL用于MySQL（InnoDB）和SQL Server中的可序列化隔离级别，以及DB2中的可重复读隔离级别【23,36】。</p>

<p>读与写的阻塞是通过为数据库中每个对象添加锁来实现的。锁可以处于<strong>共享模式（shared mode）</strong>或<strong>独占模式（exclusive mode）</strong>。锁使用如下：</p>

<ul>
<li>若事务要读取对象，则须先以共享模式获取锁。允许多个事务同时持有共享锁。但如果另一个事务已经在对象上持有排它锁，则这些事务必须等待。</li>
<li>若事务要写入一个对象，它必须首先以独占模式获取该锁。没有其他事务可以同时持有锁（无论是共享模式还是独占模式），所以如果对象上存在任何锁，该事务必须等待。</li>
<li>如果事务先读取再写入对象，则它可能会将其共享锁升级为独占锁。升级锁的工作与直接获得排他锁相同。</li>
<li>事务获得锁之后，必须继续持有锁直到事务结束（提交或中止）。这就是“两阶段”这个名字的来源：第一阶段（当事务正在执行时）获取锁，第二阶段（在事务结束时）释放所有的锁。</li>
</ul>

<p>由于使用了这么多的锁，因此很可能会发生：事务A等待事务B释放它的锁，反之亦然。这种情况叫做<strong>死锁（Deadlock）</strong>。数据库会自动检测事务之间的死锁，并中止其中一个，以便另一个继续执行。被中止的事务需要由应用程序重试。</p>

<h4 id="toc_93">两阶段锁定的性能</h4>

<p>两阶段锁定的巨大缺点，以及70年代以来没有被所有人使用的原因，是其性能问题。两阶段锁定下的事务吞吐量与查询响应时间要比弱隔离级别下要差得多。</p>

<p>这一部分是由于获取和释放所有这些锁的开销，但更重要的是由于并发性的降低。按照设计，如果两个并发事务试图做任何可能导致竞争条件的事情，那么必须等待另一个完成。</p>

<p>传统的关系数据库不限制事务的持续时间，因为它们是为等待人类输入的交互式应用而设计的。因此，当一个事务需要等待另一个事务时，等待的时长并没有限制。即使你保证所有的事务都很短，如果有多个事务想要访问同一个对象，那么可能会形成一个队列，所以事务可能需要等待几个其他事务才能完成。</p>

<p>因此，运行2PL的数据库可能具有相当不稳定的延迟，如果在工作负载中存在争用，那么可能高百分位点处的响应会非常的慢（参阅“<a href="ch1.md#%E6%8F%8F%E8%BF%B0%E6%80%A7%E8%83%BD">描述性能</a>”）。可能只需要一个缓慢的事务，或者一个访问大量数据并获取许多锁的事务，就能把系统的其他部分拖慢，甚至迫使系统停机。当需要稳健的操作时，这种不稳定性是有问题的。</p>

<p>基于锁实现的读已提交隔离级别可能发生死锁，但在基于2PL实现的可序列化隔离级别中，它们会出现的频繁的多（取决于事务的访问模式）。这可能是一个额外的性能问题：当事务由于死锁而被中止并被重试时，它需要从头重做它的工作。如果死锁很频繁，这可能意味着巨大的浪费。</p>

<h4 id="toc_94">谓词锁</h4>

<p>在前面关于锁的描述中，我们掩盖了一个微妙而重要的细节。在“<a href="#%E5%AF%BC%E8%87%B4%E5%86%99%E5%85%A5%E5%81%8F%E5%B7%AE%E7%9A%84%E5%B9%BB%E8%AF%BB">导致写入偏差的幻读</a>”中，我们讨论了<strong>幻读（phantoms）</strong>的问题。即一个事务改变另一个事务的搜索查询的结果。具有可序列化隔离级别的数据库必须防止<strong>幻读</strong>。</p>

<p>在会议室预订的例子中，这意味着如果一个事务在某个时间窗口内搜索了一个房间的现有预订（见<a href="">例7-2</a>），则另一个事务不能同时插入或更新同一时间窗口与同一房间的另一个预订 （可以同时插入其他房间的预订，或在不影响另一个预定的条件下预定同一房间的其他时间段）。</p>

<p>如何实现这一点？从概念上讲，我们需要一个<strong>谓词锁（predicate lock）</strong>【3】。它类似于前面描述的共享/排它锁，但不属于特定的对象（例如，表中的一行），它属于所有符合某些搜索条件的对象，如：</p>

<pre><code class="language-sql">SELECT * FROM bookings
WHERE room_id = 123 AND
      end_time &gt; &#39;2018-01-01 12:00&#39; AND 
      start_time &lt; &#39;2018-01-01 13:00&#39;;
</code></pre>

<p>谓词锁限制访问，如下所示：</p>

<ul>
<li>如果事务A想要读取匹配某些条件的对象，就像在这个 <code>SELECT</code> 查询中那样，它必须获取查询条件上的<strong>共享谓词锁（shared-mode predicate lock）</strong>。如果另一个事务B持有任何满足这一查询条件对象的排它锁，那么A必须等到B释放它的锁之后才允许进行查询。</li>
<li>如果事务A想要插入，更新或删除任何对象，则必须首先检查旧值或新值是否与任何现有的谓词锁匹配。如果事务B持有匹配的谓词锁，那么A必须等到B已经提交或中止后才能继续。</li>
</ul>

<p>这里的关键思想是，谓词锁甚至适用于数据库中尚不存在，但将来可能会添加的对象（幻象）。如果两阶段锁定包含谓词锁，则数据库将阻止所有形式的写入偏差和其他竞争条件，因此其隔离实现了可串行化。</p>

<h4 id="toc_95">索引范围锁</h4>

<p>不幸的是谓词锁性能不佳：<strong>如果活跃事务持有很多锁，检查匹配的锁会非常耗时。</strong>因此，大多数使用2PL的数据库实际上实现了索引范围锁（也称为<strong>间隙锁（next-key locking）</strong>），这是一个简化的近似版谓词锁【41,50】。</p>

<p>通过使谓词匹配到一个更大的集合来简化谓词锁是安全的。例如，如果你有在中午和下午1点之间预订123号房间的谓词锁，则锁定123号房间的所有时间段，或者锁定12:00~13:00时间段的所有房间（不只是123号房间）是一个安全的近似，因为任何满足原始谓词的写入也一定会满足这种更松散的近似。</p>

<p>在房间预订数据库中，您可能会在<code>room_id</code>列上有一个索引，并且/或者在<code>start_time</code> 和 <code>end_time</code>上有索引（否则前面的查询在大型数据库上的速度会非常慢）：</p>

<ul>
<li>假设您的索引位于<code>room_id</code>上，并且数据库使用此索引查找123号房间的现有预订。现在数据库可以简单地将共享锁附加到这个索引项上，指示事务已搜索123号房间用于预订。</li>
<li>或者，如果数据库使用基于时间的索引来查找现有预订，那么它可以将共享锁附加到该索引中的一系列值，指示事务已经将12:00~13:00时间段标记为用于预定。</li>
</ul>

<p>无论哪种方式，搜索条件的近似值都附加到其中一个索引上。现在，如果另一个事务想要插入，更新或删除同一个房间和/或重叠时间段的预订，则它将不得不更新索引的相同部分。在这样做的过程中，它会遇到共享锁，它将被迫等到锁被释放。</p>

<p>这种方法能够有效防止幻读和写入偏差。索引范围锁并不像谓词锁那样精确（它们可能会锁定更大范围的对象，而不是维持可串行化所必需的范围），但是由于它们的开销较低，所以是一个很好的折衷。</p>

<p>如果没有可以挂载间隙锁的索引，数据库可以退化到使用整个表上的共享锁。这对性能不利，因为它会阻止所有其他事务写入表格，但这是一个安全的回退位置。</p>

<h3 id="toc_96">序列化快照隔离（SSI）</h3>

<p>本章描绘了数据库中并发控制的黯淡画面。一方面，我们实现了性能不好（2PL）或者扩展性不好（串行执行）的可序列化隔离级别。另一方面，我们有性能良好的弱隔离级别，但容易出现各种竞争条件（丢失更新，写入偏差，幻读等）。序列化的隔离级别和高性能是从根本上相互矛盾的吗？</p>

<p>也许不是：一个称为<strong>可序列化快照隔离（SSI, serializable snapshot isolation）</strong> 的算法是非常有前途的。它提供了完整的可序列化隔离级别，但与快照隔离相比只有只有很小的性能损失。 SSI是相当新的：它在2008年首次被描述【40】，并且是Michael Cahill的博士论文【51】的主题。</p>

<p>今天，SSI既用于单节点数据库（PostgreSQL9.1 以后的可序列化隔离级别）和分布式数据库（FoundationDB使用类似的算法）。由于SSI与其他并发控制机制相比还很年轻，还处于在实践中证明自己表现的阶段。但它有可能因为足够快而在未来成为新的默认选项。</p>

<h4 id="toc_97">悲观与乐观的并发控制</h4>

<p>两阶段锁是一种所谓的<strong>悲观并发控制机制（pessimistic）</strong> ：它是基于这样的原则：如果有事情可能出错（如另一个事务所持有的锁所表示的），最好等到情况安全后再做任何事情。这就像互斥，用于保护多线程编程中的数据结构。</p>

<p>从某种意义上说，串行执行可以称为悲观到了极致：在事务持续期间，每个事务对整个数据库（或数据库的一个分区）具有排它锁，作为对悲观的补偿，我们让每笔事务执行得非常快，所以只需要短时间持有“锁”。</p>

<p>相比之下，<strong>序列化快照隔离</strong>是一种<strong>乐观（optimistic）</strong> 的并发控制技术。在这种情况下，乐观意味着，如果存在潜在的危险也不阻止事务，而是继续执行事务，希望一切都会好起来。当一个事务想要提交时，数据库检查是否有什么不好的事情发生（即隔离是否被违反）；如果是的话，事务将被中止，并且必须重试。只有可序列化的事务才被允许提交。</p>

<p>乐观并发控制是一个古老的想法【52】，其优点和缺点已经争论了很长时间【53】。如果存在很多<strong>争用（contention）</strong>（很多事务试图访问相同的对象），则表现不佳，因为这会导致很大一部分事务需要中止。如果系统已经接近最大吞吐量，来自重试事务的额外负载可能会使性能变差。</p>

<p>但是，如果有足够的备用容量，并且事务之间的争用不是太高，乐观的并发控制技术往往比悲观的要好。可交换的原子操作可以减少争用：例如，如果多个事务同时要增加一个计数器，那么应用增量的顺序（只要计数器不在同一个事务中读取）就无关紧要了，所以并发增量可以全部应用且无需冲突。</p>

<p>顾名思义，SSI基于快照隔离——也就是说，事务中的所有读取都是来自数据库的一致性快照（参见“<a href="#%E5%BF%AB%E7%85%A7%E9%9A%94%E7%A6%BB%E5%92%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB">快照隔离和可重复读取</a>”）。与早期的乐观并发控制技术相比这是主要的区别。在快照隔离的基础上，SSI添加了一种算法来检测写入之间的序列化冲突，并确定要中止哪些事务。</p>

<h4 id="toc_98">基于过时前提的决策</h4>

<p>先前讨论了快照隔离中的写入偏差（参阅“<a href="#%E5%86%99%E5%85%A5%E5%81%8F%E5%B7%AE%E4%B8%8E%E5%B9%BB%E8%AF%BB">写入偏差和幻像</a>”）时，我们观察到一个循环模式：事务从数据库读取一些数据，检查查询的结果，并根据它看到的结果决定采取一些操作（写入数据库）。但是，在快照隔离的情况下，原始查询的结果在事务提交时可能不再是最新的，因为数据可能在同一时间被修改。</p>

<p>换句话说，事务基于一个<strong>前提（premise）</strong> 采取行动（事务开始时候的事实，例如：“目前有两名医生正在值班”）。之后当事务要提交时，原始数据可能已经改变——前提可能不再成立。</p>

<p>当应用程序进行查询时（例如，“当前有多少医生正在值班？”），数据库不知道应用逻辑如何使用该查询结果。在这种情况下为了安全，数据库需要假设任何对该结果集的变更都可能会使该事务中的写入变得无效。 换而言之，事务中的查询与写入可能存在因果依赖。为了提供可序列化的隔离级别，如果事务在过时的前提下执行操作，数据库必须能检测到这种情况，并中止事务。</p>

<p>数据库如何知道查询结果是否可能已经改变？有两种情况需要考虑：</p>

<ul>
<li>检测对旧MVCC对象版本的读取（读之前存在未提交的写入）</li>
<li>检测影响先前读取的写入（读之后发生写入）</li>
</ul>

<h4 id="toc_99">检测旧MVCC读取</h4>

<p>回想一下，快照隔离通常是通过多版本并发控制（MVCC；见<a href="">图7-10</a>）来实现的。当一个事务从MVCC数据库中的一致快照读时，它将忽略取快照时尚未提交的任何其他事务所做的写入。在<a href="">图7-10</a>中，事务43 认为Alice的 <code>on_call = true</code> ，因为事务42（修改Alice的待命状态）未被提交。然而，在事务43想要提交时，事务42 已经提交。这意味着在读一致性快照时被忽略的写入已经生效，事务43 的前提不再为真。</p>

<p><img src="img/fig7-10.png" alt=""/></p>

<p><strong>图7-10 检测事务何时从MVCC快照读取过时的值</strong></p>

<p>为了防止这种异常，数据库需要跟踪一个事务由于MVCC可见性规则而忽略另一个事务的写入。当事务想要提交时，数据库检查是否有任何被忽略的写入现在已经被提交。如果是这样，事务必须中止。</p>

<p>为什么要等到提交？当检测到陈旧的读取时，为什么不立即中止事务43 ？因为如果事务43 是只读事务，则不需要中止，因为没有写入偏差的风险。当事务43 进行读取时，数据库还不知道事务是否要稍后执行写操作。此外，事务42 可能在事务43 被提交的时候中止或者可能仍然未被提交，因此读取可能终究不是陈旧的。通过避免不必要的中止，SSI 保留快照隔离对从一致快照中长时间运行的读取的支持。</p>

<h4 id="toc_100">检测影响之前读取的写入</h4>

<p>第二种情况要考虑的是另一个事务在读取数据之后修改数据。这种情况如<a href="img/fig7-11.png">图7-11</a>所示。</p>

<p><img src="img/fig7-11.png" alt=""/></p>

<p><strong>图7-11 在可序列化快照隔离中，检测一个事务何时修改另一个事务的读取。</strong></p>

<p>在两阶段锁定的上下文中，我们讨论了<a href="">索引范围锁</a>（请参阅“<a href="">索引范围锁</a>”），它允许数据库锁定与某个搜索查询匹配的所有行的访问权，例如 <code>WHERE shift_id = 1234</code>。可以在这里使用类似的技术，除了SSI锁不会阻塞其他事务。</p>

<p>在<a href="">图7-11</a>中，事务42 和43 都在班次1234 查找值班医生。如果在<code>shift_id</code>上有索引，则数据库可以使用索引项1234 来记录事务42 和43 读取这个数据的事实。 （如果没有索引，这个信息可以在表级别进行跟踪）。这个信息只需要保留一段时间：在一个事务完成（提交或中止），并且所有的并发事务完成之后，数据库就可以忘记它读取的数据了。</p>

<p>当事务写入数据库时，它必须在索引中查找最近曾读取受影响数据的其他事务。这个过程类似于在受影响的键范围上获取写锁，但锁并不会阻塞事务指导其他读事务完成，而是像警戒线一样只是简单通知其他事务：你们读过的数据可能不是最新的啦。</p>

<p>在<a href="">图7-11</a>中，事务43 通知事务42 其先前读已过时，反之亦然。事务42首先提交并成功，尽管事务43 的写影响了42 ，但因为事务43 尚未提交，所以写入尚未生效。然而当事务43 想要提交时，来自事务42 的冲突写入已经被提交，所以事务43 必须中止。</p>

<h4 id="toc_101">可序列化的快照隔离的性能</h4>

<p>与往常一样，许多工程细节会影响算法的实际表现。例如一个权衡是跟踪事务的读取和写入的<strong>粒度（granularity）</strong>。如果数据库详细地跟踪每个事务的活动（细粒度），那么可以准确地确定哪些事务需要中止，但是簿记开销可能变得很显著。简略的跟踪速度更快（粗粒度），但可能会导致更多不必要的事务中止。</p>

<p>在某些情况下，事务可以读取被另一个事务覆盖的信息：这取决于发生了什么，有时可以证明执行结果无论如何都是可序列化的。 PostgreSQL使用这个理论来减少不必要的中止次数【11,41】。</p>

<p>与两阶段锁定相比，可序列化快照隔离的最大优点是一个事务不需要阻塞等待另一个事务所持有的锁。就像在快照隔离下一样，写不会阻塞读，反之亦然。这种设计原则使得查询延迟更可预测，变量更少。特别是，只读查询可以运行在一致的快照上，而不需要任何锁定，这对于读取繁重的工作负载非常有吸引力。</p>

<p>与串行执行相比，可序列化快照隔离并不局限于单个CPU核的吞吐量：FoundationDB将检测到的序列化冲突分布在多台机器上，允许扩展到很高的吞吐量。即使数据可能跨多台机器进行分区，事务也可以在保证可序列化隔离等级的同时读写多个分区中的数据【54】。</p>

<p>中止率显著影响SSI的整体表现。例如，长时间读取和写入数据的事务很可能会发生冲突并中止，因此SSI要求同时读写的事务尽量短（只读长事务可能没问题）。对于慢事务，SSI可能比两阶段锁定或串行执行更不敏感。</p>

<h2 id="toc_102">本章小结</h2>

<p>事务是一个抽象层，允许应用程序假装某些并发问题和某些类型的硬件和软件故障不存在。各式各样的错误被简化为一种简单情况：<strong>事务中止（transaction abort）</strong>，而应用需要的仅仅是重试。</p>

<p>在本章中介绍了很多问题，事务有助于防止这些问题发生。并非所有应用都易受此类问题影响：具有非常简单访问模式的应用（例如每次读写单条记录）可能无需事务管理。但是对于更复杂的访问模式，事务可以大大减少需要考虑的潜在错误情景数量。</p>

<p>如果没有事务处理，各种错误情况（进程崩溃，网络中断，停电，磁盘已满，意外并发等）意味着数据可能以各种方式变得不一致。例如，非规范化的数据可能很容易与源数据不同步。如果没有事务处理，就很难推断复杂的交互访问可能对数据库造成的影响。</p>

<p>本章深入讨论了<strong>并发控制</strong>的话题。我们讨论了几个广泛使用的隔离级别，特别是<strong>读已提交</strong>，<strong>快照隔离</strong>（有时称为可重复读）和<strong>可序列化</strong>。并通过研究竞争条件的各种例子，来描述这些隔离等级：</p>

<p><strong><em>脏读</em></strong></p>

<p>​   一个客户端读取到另一个客户端尚未提交的写入。<strong>读已提交</strong>或更强的隔离级别可以防止脏读。</p>

<p><strong><em>脏写</em></strong></p>

<p>​   一个客户端覆盖写入了另一个客户端尚未提交的写入。几乎所有的事务实现都可以防止脏写。</p>

<p><strong><em>读取偏差（不可重复读）</em></strong></p>

<p>​   在同一个事务中，客户端在不同的时间点会看见数据库的不同状态。<strong>快照隔离</strong>经常用于解决这个问题，它允许事务从一个特定时间点的一致性快照中读取数据。快照隔离通常使用<strong>多版本并发控制（MVCC）</strong> 来实现。</p>

<p><strong><em>更新丢失</em></strong></p>

<p>​   两个客户端同时执行<strong>读取-修改-写入序列</strong>。其中一个写操作，在没有合并另一个写入变更情况下，直接覆盖了另一个写操作的结果。所以导致数据丢失。快照隔离的一些实现可以自动防止这种异常，而另一些实现则需要手动锁定（<code>SELECT FOR UPDATE</code>）。</p>

<p><strong><em>写偏差</em></strong></p>

<p>​   一个事务读取一些东西，根据它所看到的值作出决定，并将该决定写入数据库。但是，写入时，该决定的前提不再是真实的。只有可序列化的隔离才能防止这种异常。</p>

<p><strong><em>幻读</em></strong></p>

<p>​   事务读取符合某些搜索条件的对象。另一个客户端进行写入，影响搜索结果。快照隔离可以防止直接的幻像读取，但是写入偏差上下文中的幻读需要特殊处理，例如索引范围锁定。</p>

<p>​弱隔离级别可以防止其中一些异常情况，此外让应用程序开发人员手动处理剩余那些（例如，使用显式锁定）。只有可序列化的隔离才能防范所有这些问题。我们讨论了实现可序列化事务的三种不同方法：</p>

<p><strong><em>字面意义上的串行执行</em></strong></p>

<p>​   如果每个事务的执行速度非常快，并且事务吞吐量足够低，足以在单个CPU核上处理，这是一个简单而有效的选择。</p>

<p><strong><em>两阶段锁定</em></strong></p>

<p>​   数十年来，两阶段锁定一直是实现可序列化的标准方式，但是许多应用出于性能问题的考虑避免使用它。</p>

<p><strong><em>可串行化快照隔离（SSI）</em></strong></p>

<p>​   一个相当新的算法，避免了先前方法的大部分缺点。它使用乐观的方法，允许事务执行而无需阻塞。当一个事务想要提交时，它会进行检查，如果执行不可序列化，事务就会被中止。</p>

<p>​   本章中的示例主要是在关系数据模型的上下文中。使用关系数据模型。但是，正如在讨论中，无论使用哪种数据模型，如“<strong><a href="#%E5%A4%9A%E5%AF%B9%E8%B1%A1%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9C%80%E6%B1%82">多对象事务的需求</a></strong>”中所讨论的，事务都是重要的数据库功能。</p>

<p>​   本章主要是在单机数据库的上下文中，探讨了各种概念与想法。分布式数据库中的事务，则引入了一系列新的困难挑战，将在接下来的两章中讨论。</p>

<h2 id="toc_103">参考文献</h2>

<ol>
<li>Donald D. Chamberlin, Morton M. Astrahan, Michael W. Blasgen, et al.: “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.348&amp;rep=rep1&amp;type=pdf">A History and Evaluation of System R</a>,” <em>Communications of the ACM</em>, volume 24, number 10, pages 632–646, October 1981.
<a href="http://dx.doi.org/10.1145/358769.358784">doi:10.1145/358769.358784</a></li>
<li>Jim N. Gray, Raymond A. Lorie, Gianfranco R. Putzolu, and Irving L. Traiger: “<a href="http://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.92.8248&amp;rep=rep1&amp;type=pdf">Granularity of Locks and Degrees of Consistency in a Shared Data Base</a>,” in <em>Modelling in Data Base Management Systems: Proceedings of the IFIP Working Conference on Modelling in Data Base Management Systems</em>, edited by G. M. Nijssen, pages 364–394, Elsevier/North Holland Publishing, 1976. Also in <em>Readings in Database Systems</em>, 4th edition, edited by Joseph M. Hellerstein and Michael Stonebraker, MIT Press, 2005. ISBN: 978-0-262-69314-1</li>
<li>Kapali P. Eswaran, Jim N. Gray, Raymond A. Lorie, and Irving L. Traiger: “<a href="http://research.microsoft.com/en-us/um/people/gray/papers/On%20the%20Notions%20of%20Consistency%20and%20Predicate%20Locks%20in%20a%20Database%20System%20CACM.pdf">The Notions of Consistency and Predicate Locks in a Database System</a>,” <em>Communications of the ACM</em>, volume 19, number 11, pages 624–633, November 1976.</li>
<li>“<a href="http://web.archive.org/web/20150320053809/https://foundationdb.com/acid-claims">ACID Transactions Are Incredibly Helpful</a>,” FoundationDB, LLC, 2013.</li>
<li>John D. Cook: “<a href="http://www.johndcook.com/blog/2009/07/06/brewer-cap-theorem-base/">ACID Versus BASE for Database Transactions</a>,” <em>johndcook.com</em>, July 6, 2009.</li>
<li>Gavin Clarke: “<a href="http://www.theregister.co.uk/2012/11/22/foundationdb_fear_of_cap_theorem/">NoSQL&#39;s CAP Theorem Busters: We Don&#39;t Drop ACID</a>,” <em>theregister.co.uk</em>, November 22, 2012.</li>
<li>Theo Härder and Andreas Reuter: “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.2812&amp;rep=rep1&amp;type=pdf">Principles of Transaction-Oriented Database Recovery</a>,” <em>ACM Computing Surveys</em>, volume 15, number 4, pages 287–317, December 1983. <a href="http://dx.doi.org/10.1145/289.291">doi:10.1145/289.291</a></li>
<li>Peter Bailis, Alan Fekete, Ali Ghodsi, et al.: “<a href="http://www.bailis.org/papers/hat-hotos2013.pdf">HAT, not CAP: Towards Highly Available Transactions</a>,”
at <em>14th USENIX Workshop on Hot Topics in Operating Systems</em> (HotOS), May 2013.</li>
<li>Armando Fox, Steven D. Gribble, Yatin Chawathe, et al.: “<a href="http://www.cs.berkeley.edu/%7Ebrewer/cs262b/TACC.pdf">Cluster-Based Scalable Network Services</a>,” at
<em>16th ACM Symposium on Operating Systems Principles</em> (SOSP), October 1997.</li>
<li>Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman: <a href="http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx"><em>Concurrency Control and Recovery in Database Systems</em></a>. Addison-Wesley, 1987. ISBN: 978-0-201-10715-9, available online at <em>research.microsoft.com</em>.</li>
<li>Alan Fekete, Dimitrios Liarokapis, Elizabeth O&#39;Neil, et al.: “<a href="https://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/2009/Papers/p492-fekete.pdf">Making Snapshot Isolation Serializable</a>,” <em>ACM Transactions on Database Systems</em>, volume 30, number 2, pages 492–528, June 2005.
<a href="http://dx.doi.org/10.1145/1071610.1071615">doi:10.1145/1071610.1071615</a></li>
<li>Mai Zheng, Joseph Tucek, Feng Qin, and Mark Lillibridge:  “<a href="https://www.usenix.org/system/files/conference/fast13/fast13-final80.pdf">Understanding the Robustness of SSDs Under Power Fault</a>,” at <em>11th USENIX Conference on File and Storage Technologies</em> (FAST), February 2013.</li>
<li>Laurie Denness:  “<a href="https://laur.ie/blog/2015/06/ssds-a-gift-and-a-curse/">SSDs: A Gift and a Curse</a>,”  <em>laur.ie</em>, June 2, 2015.</li>
<li>Adam Surak:  “<a href="https://blog.algolia.com/when-solid-state-drives-are-not-that-solid/">When Solid State Drives Are Not That Solid</a>,” <em>blog.algolia.com</em>, June 15, 2015.</li>
<li>Thanumalayan Sankaranarayana Pillai, Vijay Chidambaram,  Ramnatthan Alagappan, et al.: “<a href="http://research.cs.wisc.edu/wind/Publications/alice-osdi14.pdf">All   File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent Applications</a>,”  at <em>11th USENIX Symposium on Operating Systems Design and Implementation</em> (OSDI),
  October 2014.</li>
<li>Chris Siebenmann:  “<a href="https://utcc.utoronto.ca/%7Ecks/space/blog/unix/FileSyncProblem">Unix&#39;s File Durability  Problem</a>,” <em>utcc.utoronto.ca</em>, April 14, 2016.</li>
<li>Lakshmi N. Bairavasundaram, Garth R.  Goodson, Bianca Schroeder, et al.:  “<a href="http://research.cs.wisc.edu/adsl/Publications/corruption-fast08.pdf">An Analysis of Data   Corruption in the Storage Stack</a>,” at <em>6th USENIX Conference on File and Storage Technologies</em> (FAST), February 2008.</li>
<li>Bianca Schroeder, Raghav Lagisetty, and Arif Merchant:  “<a href="https://www.usenix.org/conference/fast16/technical-sessions/presentation/schroeder">Flash   Reliability in Production: The Expected and the Unexpected</a>,” at <em>14th USENIX Conference on  File and Storage Technologies</em> (FAST), February 2016.</li>
<li>Don Allison:  “<a href="https://blog.korelogic.com/blog/2015/03/24">SSD Storage – Ignorance of Technology Is No Excuse</a>,” <em>blog.korelogic.com</em>, March 24, 2015.</li>
<li>Dave Scherer: “<a href="http://web.archive.org/web/20150526065247/http://blog.foundationdb.com/those-are-not-transactions-cassandra-2-0">Those Are Not Transactions (Cassandra 2.0)</a>,” <em>blog.foundationdb.com</em>, September 6, 2013.</li>
<li>Kyle Kingsbury: “<a href="http://aphyr.com/posts/294-call-me-maybe-cassandra/">Call Me Maybe: Cassandra</a>,” <em>aphyr.com</em>, September 24, 2013.</li>
<li>“<a href="http://www.aerospike.com/docs/architecture/assets/AerospikeACIDSupport.pdf">ACID Support in Aerospike</a>,” Aerospike, Inc., June 2014.</li>
<li>Martin Kleppmann: “<a href="http://martin.kleppmann.com/2014/11/25/hermitage-testing-the-i-in-acid.html">Hermitage: Testing the &#39;I&#39; in ACID</a>,” <em>martin.kleppmann.com</em>, November 25, 2014.</li>
<li>Tristan D&#39;Agosta: “<a href="https://bitcointalk.org/index.php?topic=499580">BTC Stolen from Poloniex</a>,” <em>bitcointalk.org</em>, March 4, 2014.</li>
<li>bitcointhief2: “<a href="http://www.reddit.com/r/Bitcoin/comments/1wtbiu/how_i_stole_roughly_100_btc_from_an_exchange_and/">How I Stole Roughly 100 BTC from an Exchange and How I Could Have Stolen More!</a>,” <em>reddit.com</em>, February 2, 2014.</li>
<li>Sudhir Jorwekar, Alan Fekete, Krithi Ramamritham, and S. Sudarshan: “<a href="http://www.vldb.org/conf/2007/papers/industrial/p1263-jorwekar.pdf">Automating the Detection of Snapshot Isolation Anomalies</a>,” at <em>33rd International Conference on Very Large Data Bases</em> (VLDB), September 2007.</li>
<li>Michael Melanson: “<a href="http://www.michaelmelanson.net/2014/03/20/transactions/">Transactions: The Limits of Isolation</a>,” <em>michaelmelanson.net</em>, March 20, 2014.</li>
<li>Hal Berenson, Philip A. Bernstein, Jim N. Gray, et al.: “<a href="http://research.microsoft.com/pubs/69541/tr-95-51.pdf">A Critique of ANSI SQL Isolation Levels</a>,”
at <em>ACM International Conference on Management of Data</em> (SIGMOD), May 1995.</li>
<li>Atul Adya: “<a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Weak Consistency: A Generalized Theory and Optimistic Implementations for Distributed Transactions</a>,” PhD Thesis, Massachusetts Institute of Technology, March 1999.</li>
<li>Peter Bailis, Aaron Davidson, Alan Fekete, et al.: “<a href="http://arxiv.org/pdf/1302.0309.pdf">Highly Available Transactions: Virtues and Limitations (Extended Version)</a>,” at <em>40th International Conference on Very Large Data Bases</em> (VLDB), September 2014.</li>
<li>Bruce Momjian: “<a href="http://momjian.us/main/presentations/internals.html#mvcc">MVCC Unmasked</a>,” <em>momjian.us</em>, July 2014.</li>
<li>Annamalai Gurusami: “<a href="https://blogs.oracle.com/mysqlinnodb/entry/repeatable_read_isolation_level_in">Repeatable Read Isolation Level in InnoDB – How Consistent Read View Works</a>,” <em>blogs.oracle.com</em>, January 15, 2013.</li>
<li>Nikita Prokopov: “<a href="http://tonsky.me/blog/unofficial-guide-to-datomic-internals/">Unofficial Guide to Datomic Internals</a>,” <em>tonsky.me</em>, May 6, 2014.</li>
<li>Baron Schwartz: “<a href="http://www.xaprb.com/blog/2013/12/28/immutability-mvcc-and-garbage-collection/">Immutability, MVCC, and Garbage Collection</a>,” <em>xaprb.com</em>, December 28, 2013.</li>
<li>J. Chris Anderson, Jan Lehnardt, and Noah Slater: <em>CouchDB: The Definitive Guide</em>. O&#39;Reilly Media, 2010.
ISBN: 978-0-596-15589-6 Rikdeb Mukherjee: “<a href="http://mframes.blogspot.co.uk/2013/07/isolation-in-cursor.html">Isolation in DB2 (Repeatable Read, Read Stability, Cursor Stability, Uncommitted Read) with Examples</a>,” <em>mframes.blogspot.co.uk</em>, July 4, 2013.</li>
<li>Steve Hilker: “<a href="http://www.toadworld.com/platforms/ibmdb2/w/wiki/6661.cursor-stability-cs.aspx">Cursor Stability (CS) – IBM DB2 Community</a>,” <em>toadworld.com</em>, March 14, 2013.</li>
<li>Nate Wiger: “<a href="http://www.nateware.com/an-atomic-rant.html">An Atomic Rant</a>,” <em>nateware.com</em>, February 18, 2010.</li>
<li>Joel Jacobson: “<a href="http://blog.joeljacobson.com/riak-2-0-data-types/">Riak 2.0: Data Types</a>,” <em>blog.joeljacobson.com</em>, March 23, 2014.</li>
<li>Michael J. Cahill, Uwe Röhm, and Alan Fekete: “<a href="http://www.cs.nyu.edu/courses/fall12/CSCI-GA.2434-001/p729-cahill.pdf">Serializable Isolation for Snapshot Databases</a>,” at <em>ACM International Conference on Management of Data</em> (SIGMOD), June 2008. <a href="http://dx.doi.org/10.1145/1376616.1376690">doi:10.1145/1376616.1376690</a></li>
<li>Dan R. K. Ports and Kevin Grittner: “<a href="http://drkp.net/papers/ssi-vldb12.pdf">Serializable Snapshot Isolation in PostgreSQL</a>,” at <em>38th International Conference on Very Large Databases</em> (VLDB), August 2012.</li>
<li>Tony Andrews:   “<a href="http://tonyandrews.blogspot.co.uk/2004/10/enforcing-complex-constraints-in.html">Enforcing Complex Constraints in Oracle</a>,” <em>tonyandrews.blogspot.co.uk</em>, October 15, 2004.</li>
<li>Douglas B. Terry, Marvin M. Theimer, Karin Petersen, et al.:  “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.141.7889&amp;rep=rep1&amp;type=pdf">Managing   Update Conflicts in Bayou, a Weakly Connected Replicated Storage System</a>,” at <em>15th ACM Symposium on Operating Systems Principles</em> (SOSP), December 1995.  <a href="http://dx.doi.org/10.1145/224056.224070">doi:10.1145/224056.224070</a></li>
<li>Gary Fredericks:  “<a href="https://github.com/gfredericks/pg-serializability-bug">Postgres Serializability Bug</a>,” <em>github.com</em>, September 2015.</li>
<li>Michael Stonebraker, Samuel Madden, Daniel J. Abadi, et al.: “<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.3697&amp;rep=rep1&amp;type=pdf">The End of an Architectural Era (It’s Time for a Complete Rewrite)</a>,” at <em>33rd International Conference on Very Large Data Bases</em> (VLDB), September 2007.</li>
<li>John Hugg: “<a href="https://www.youtube.com/watch?v=hD5M4a1UVz8">H-Store/VoltDB Architecture vs. CEP Systems and Newer Streaming Architectures</a>,” at <em>Data @Scale Boston</em>, November 2014.</li>
<li>Robert Kallman, Hideaki Kimura, Jonathan Natkins, et al.: “<a href="http://www.vldb.org/pvldb/1/1454211.pdf">H-Store: A High-Performance, Distributed Main Memory Transaction Processing System</a>,” <em>Proceedings of the VLDB Endowment</em>, volume 1, number 2, pages 1496–1499, August 2008.</li>
<li>Rich Hickey: “<a href="http://www.infoq.com/articles/Architecture-Datomic">The Architecture of Datomic</a>,” <em>infoq.com</em>, November 2, 2012.</li>
<li>John Hugg: “<a href="http://voltdb.com/blog/debunking-myths-about-voltdb-memory-database">Debunking Myths About the VoltDB In-Memory Database</a>,” <em>voltdb.com</em>, May 12, 2014.</li>
<li>Joseph M. Hellerstein, Michael Stonebraker, and James Hamilton: “<a href="http://db.cs.berkeley.edu/papers/fntdb07-architecture.pdf">Architecture of a Database System</a>,”
<em>Foundations and Trends in Databases</em>, volume 1, number 2, pages 141–259, November 2007.
<a href="http://dx.doi.org/10.1561/1900000002">doi:10.1561/1900000002</a></li>
<li>Michael J. Cahill: “<a href="http://cahill.net.au/wp-content/uploads/2010/02/cahill-thesis.pdf">Serializable Isolation for Snapshot Databases</a>,” PhD Thesis, University of Sydney, July 2009.</li>
<li>D. Z. Badal: “<a href="http://ieeexplore.ieee.org/abstract/document/762563/">Correctness of Concurrency Control and Implications in Distributed Databases</a>,” at <em>3rd International IEEE Computer Software and Applications Conference</em> (COMPSAC), November 1979.</li>
<li>Rakesh Agrawal, Michael J. Carey, and Miron Livny: “<a href="http://www.eecs.berkeley.edu/%7Ebrewer/cs262/ConcControl.pdf">Concurrency Control Performance Modeling: Alternatives and Implications</a>,” <em>ACM Transactions on Database Systems</em> (TODS), volume 12, number 4, pages 609–654, December 1987. <a href="http://dx.doi.org/10.1145/32204.32220">doi:10.1145/32204.32220</a></li>
<li>Dave Rosenthal: “<a href="http://web.archive.org/web/20150427041746/http://blog.foundationdb.com/databases-at-14.4mhz">Databases at 14.4MHz</a>,” <em>blog.foundationdb.com</em>, December 10, 2014.</li>
</ol>

<hr/>

<table>
<thead>
<tr>
<th>上一章</th>
<th>目录</th>
<th>下一章</th>
</tr>
</thead>

<tbody>
<tr>
<td><a href="ch6.md">第六章：分区</a></td>
<td><a href="README.md">设计数据密集型应用</a></td>
<td><a href="ch8.md">第八章：分布式系统的麻烦</a></td>
</tr>
</tbody>
</table>

<div class="footnotes">
<hr/>
<ol>

<li id="fn1">
<p>乔·海勒斯坦（Joe Hellerstein）指出，在论Härder与Reuter的论文中，“ACID中的C”是被“扔进去凑缩写单词的”【7】，而且那时候大家都不怎么在乎一致性。&nbsp;<a href="#fnref1" rev="footnote">&#8617;</a></p>
</li>

<li id="fn2">
<p>可以说邮件应用中的错误计数器并不是什么特别重要的问题。但换种方式来看，你可以把未读计数器换成客户账户余额，把邮件收发看成支付交易。&nbsp;<a href="#fnref2" rev="footnote">&#8617;</a></p>
</li>

<li id="fn3">
<p>这并不完美。如果TCP连接中断，则事务必须中止。如果中断发生在客户端请求提交之后，但在服务器确认提交发生之前，客户端并不知道事务是否已提交。为了解决这个问题，事务管理器可以通过一个唯一事务标识符来对操作进行分组，这个标识符并未绑定到特定TCP连接。后续再“<a href="ch12.md#%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E4%BA%89%E8%AE%BA">数据库端到端的争论</a>”一节将回到这个主题。&nbsp;<a href="#fnref3" rev="footnote">&#8617;</a></p>
</li>

<li id="fn4">
<p>严格地说，<strong>原子自增（atomic increment）</strong> 这个术语在多线程编程的意义上使用了原子这个词。 在ACID的情况下，它实际上应该被称为 <strong>孤立（isolated）</strong> 的或<strong>可序列化（serializable）</strong> 的增量。 但这就太吹毛求疵了。&nbsp;<a href="#fnref4" rev="footnote">&#8617;</a></p>
</li>

<li id="fn5">
<p>轶事：偶然出现的瞬时错误有时称为<strong><em>Heisenbug</em></strong>，而确定性的问题对应地称为<strong><em>Bohrbugs</em></strong>&nbsp;<a href="#fnref5" rev="footnote">&#8617;</a></p>
</li>

<li id="fn6">
<p>某些数据库支持甚至更弱的隔离级别，称为<strong>读未提交（Read uncommitted）</strong>。它可以防止脏写，但不防止脏读。&nbsp;<a href="#fnref6" rev="footnote">&#8617;</a></p>
</li>

<li id="fn7">
<p>在撰写本文时，唯一在读已提交隔离级别使用读锁的主流数据库是使用<code>read_committed_snapshot = off</code>配置的IBM DB2和Microsoft SQL Server [23,36]。&nbsp;<a href="#fnref7" rev="footnote">&#8617;</a></p>
</li>

<li id="fn8">
<p>事实上，事务ID是32位整数，所以大约会在40亿次事务之后溢出。 PostgreSQL的Vacuum过程会清理老旧的事务ID，确保事务ID溢出（回卷）不会影响到数据。&nbsp;<a href="#fnref8" rev="footnote">&#8617;</a></p>
</li>

<li id="fn9">
<p>在PostgreSQL中，<code>created_by</code> 的实际名称为<code>xmin</code>，<code>deleted_by</code> 的实际名称为<code>xmax</code>&nbsp;<a href="#fnref9" rev="footnote">&#8617;</a></p>
</li>

<li id="fn10">
<p>将文本文档的编辑表示为原子的变化流是可能的，尽管相当复杂。参阅“<a href="ch5.md#%E9%A2%98%E5%A4%96%E8%AF%9D%EF%BC%9A%E8%87%AA%E5%8A%A8%E5%86%B2%E7%AA%81%E8%A7%A3%E5%86%B3">自动冲突解决</a>”。&nbsp;<a href="#fnref10" rev="footnote">&#8617;</a></p>
</li>

<li id="fn11">
<p>在PostgreSQL中，您可以使用范围类型优雅地执行此操作，但在其他数据库中并未得到广泛支持。&nbsp;<a href="#fnref11" rev="footnote">&#8617;</a></p>
</li>

<li id="fn12">
<p>如果事务需要访问不在内存中的数据，最好的解决方案可能是中止事务，异步地将数据提取到内存中，同时继续处理其他事务，然后在数据加载完毕时重新启动事务。这种方法被称为<strong>反缓存（anti-caching）</strong>，正如前面在第88页“将所有内容保存在内存”中所述。&nbsp;<a href="#fnref12" rev="footnote">&#8617;</a></p>
</li>

<li id="fn13">
<p>有时也称为<strong>严格两阶段锁定（SS2PL, strict two-phase locking）</strong>，以便和其他2PL变体区分。&nbsp;<a href="#fnref13" rev="footnote">&#8617;</a></p>
</li>

</ol>
</div>



<h1 id="toc_0">9. 一致性与共识</h1>

<h2 id="toc_1">9.1 一致性保证</h2>

<h2 id="toc_2">9.2 线性一致性</h2>

<h3 id="toc_3">9.2.1 什么使得系统线性一致？</h3>

<h3 id="toc_4">9.2.2 依赖线性一致性</h3>

<h4 id="toc_5">锁定和领导选举</h4>

<h4 id="toc_6">约束和唯一性保证</h4>

<h4 id="toc_7">跨信道的时序依赖</h4>

<h3 id="toc_8">9.2.3 实现线性一致的系统</h3>

<h4 id="toc_9">线性一致性和法定人数</h4>

<h3 id="toc_10">9.2.4 线性一致性的代价</h3>

<h4 id="toc_11">CAP定理</h4>

<h5 id="toc_12">线性一致性和网络延迟</h5>

<h3 id="toc_13">9.2.5 顺序保证</h3>

<h4 id="toc_14">顺序与因果</h4>

<h5 id="toc_15">因果顺序不是全序的</h5>

<h5 id="toc_16">线性一致性强于因果一致性</h5>

<h5 id="toc_17">捕获因果关系</h5>

<h4 id="toc_18">序列号顺序</h4>

<h5 id="toc_19">非因果序列号生成器</h5>

<h5 id="toc_20">兰伯特时间戳</h5>

<h5 id="toc_21">光有时间戳排序还不够</h5>

<h4 id="toc_22">全序广播</h4>

<h4 id="toc_23">使用全序广播</h4>

<h4 id="toc_24">使用全序广播实现线性一致的存储</h4>

<h4 id="toc_25">使用线性一致性存储实现全序广播</h4>

<h2 id="toc_26">9.3 分布式事务与共识</h2>

<h2 id="toc_27">9.4 原子提交与二阶段提交（2PC）</h2>

<h3 id="toc_28">9.4.1 从单节点到分布式原子提交</h3>

<h3 id="toc_29">9.4.2 两阶段提交简介</h3>

<h3 id="toc_30">9.4.3 系统承诺</h3>

<h3 id="toc_31">9.4.4 协调者失效</h3>

<h3 id="toc_32">9.4.5 三阶段提交</h3>

<h2 id="toc_33">9.5 实践中的分布式事务</h2>

<h3 id="toc_34">恰好一次的消息处理</h3>

<h3 id="toc_35">XA事务</h3>

<h3 id="toc_36">怀疑时持有锁</h3>

<h3 id="toc_37">从协调者故障中恢复</h3>

<h3 id="toc_38">分布式事务的限制</h3>

<h2 id="toc_39">9.6 容错共识</h2>

<h3 id="toc_40">共识算法和全序广播</h3>

<h3 id="toc_41">单领导者复制和共识</h3>

<h3 id="toc_42">时代编号和法定人数</h3>

<h3 id="toc_43">共识的局限性</h3>

<h2 id="toc_44">9.7 成员与协调服务</h2>

<h3 id="toc_45">将工作分配给节点</h3>

<h3 id="toc_46">服务发现</h3>

<h3 id="toc_47">成员服务</h3>

<h2 id="toc_48">9.8 本章小结</h2>

<h2 id="toc_49">参考文献</h2>

<p><img src="img/ch9.png" alt=""/></p>

<blockquote>
<p>好死不如赖活着<br/>
—— Jay Kreps, 关于Kafka与 Jepsen的若干笔记 (2013)</p>
</blockquote>

<hr/>

<ul>
<li>
<a href="#toc_0">9. 一致性与共识</a>
<ul>
<li>
<a href="#toc_1">9.1 一致性保证</a>
</li>
<li>
<a href="#toc_2">9.2 线性一致性</a>
<ul>
<li>
<a href="#toc_3">9.2.1 什么使得系统线性一致？</a>
</li>
<li>
<a href="#toc_4">9.2.2 依赖线性一致性</a>
<ul>
<li>
<a href="#toc_5">锁定和领导选举</a>
</li>
<li>
<a href="#toc_6">约束和唯一性保证</a>
</li>
<li>
<a href="#toc_7">跨信道的时序依赖</a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">9.2.3 实现线性一致的系统</a>
<ul>
<li>
<a href="#toc_9">线性一致性和法定人数</a>
</li>
</ul>
</li>
<li>
<a href="#toc_10">9.2.4 线性一致性的代价</a>
<ul>
<li>
<a href="#toc_11">CAP定理</a>
<ul>
<li>
<a href="#toc_12">线性一致性和网络延迟</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_13">9.2.5 顺序保证</a>
<ul>
<li>
<a href="#toc_14">顺序与因果</a>
<ul>
<li>
<a href="#toc_15">因果顺序不是全序的</a>
</li>
<li>
<a href="#toc_16">线性一致性强于因果一致性</a>
</li>
<li>
<a href="#toc_17">捕获因果关系</a>
</li>
</ul>
</li>
<li>
<a href="#toc_18">序列号顺序</a>
<ul>
<li>
<a href="#toc_19">非因果序列号生成器</a>
</li>
<li>
<a href="#toc_20">兰伯特时间戳</a>
</li>
<li>
<a href="#toc_21">光有时间戳排序还不够</a>
</li>
</ul>
</li>
<li>
<a href="#toc_22">全序广播</a>
</li>
<li>
<a href="#toc_23">使用全序广播</a>
</li>
<li>
<a href="#toc_24">使用全序广播实现线性一致的存储</a>
</li>
<li>
<a href="#toc_25">使用线性一致性存储实现全序广播</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_26">9.3 分布式事务与共识</a>
</li>
<li>
<a href="#toc_27">9.4 原子提交与二阶段提交（2PC）</a>
<ul>
<li>
<a href="#toc_28">9.4.1 从单节点到分布式原子提交</a>
</li>
<li>
<a href="#toc_29">9.4.2 两阶段提交简介</a>
</li>
<li>
<a href="#toc_30">9.4.3 系统承诺</a>
</li>
<li>
<a href="#toc_31">9.4.4 协调者失效</a>
</li>
<li>
<a href="#toc_32">9.4.5 三阶段提交</a>
</li>
</ul>
</li>
<li>
<a href="#toc_33">9.5 实践中的分布式事务</a>
<ul>
<li>
<a href="#toc_34">恰好一次的消息处理</a>
</li>
<li>
<a href="#toc_35">XA事务</a>
</li>
<li>
<a href="#toc_36">怀疑时持有锁</a>
</li>
<li>
<a href="#toc_37">从协调者故障中恢复</a>
</li>
<li>
<a href="#toc_38">分布式事务的限制</a>
</li>
</ul>
</li>
<li>
<a href="#toc_39">9.6 容错共识</a>
<ul>
<li>
<a href="#toc_40">共识算法和全序广播</a>
</li>
<li>
<a href="#toc_41">单领导者复制和共识</a>
</li>
<li>
<a href="#toc_42">时代编号和法定人数</a>
</li>
<li>
<a href="#toc_43">共识的局限性</a>
</li>
</ul>
</li>
<li>
<a href="#toc_44">9.7 成员与协调服务</a>
<ul>
<li>
<a href="#toc_45">将工作分配给节点</a>
</li>
<li>
<a href="#toc_46">服务发现</a>
</li>
<li>
<a href="#toc_47">成员服务</a>
</li>
</ul>
</li>
<li>
<a href="#toc_48">9.8 本章小结</a>
</li>
<li>
<a href="#toc_49">参考文献</a>
</li>
<li>
<a href="#toc_50">一致性保证</a>
</li>
<li>
<a href="#toc_51">线性一致性</a>
<ul>
<li>
<a href="#toc_52">什么使得系统线性一致？</a>
</li>
<li>
<a href="#toc_54">依赖线性一致性</a>
<ul>
<li>
<a href="#toc_55">锁定和领导选举</a>
</li>
<li>
<a href="#toc_56">约束和唯一性保证</a>
</li>
<li>
<a href="#toc_57">跨信道的时序依赖</a>
</li>
</ul>
</li>
<li>
<a href="#toc_58">实现线性一致的系统</a>
<ul>
<li>
<a href="#toc_59">线性一致性和法定人数</a>
</li>
</ul>
</li>
<li>
<a href="#toc_60">线性一致性的代价</a>
<ul>
<li>
<a href="#toc_61">CAP定理</a>
<ul>
<li>
<a href="#toc_63">线性一致性和网络延迟</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_64">顺序保证</a>
<ul>
<li>
<a href="#toc_65">顺序与因果</a>
<ul>
<li>
<a href="#toc_66">因果顺序不是全序的</a>
</li>
<li>
<a href="#toc_67">线性一致性强于因果一致性</a>
</li>
<li>
<a href="#toc_68">捕获因果关系</a>
</li>
</ul>
</li>
<li>
<a href="#toc_69">序列号顺序</a>
<ul>
<li>
<a href="#toc_70">非因果序列号生成器</a>
</li>
<li>
<a href="#toc_71">兰伯特时间戳</a>
</li>
<li>
<a href="#toc_72">光有时间戳排序还不够</a>
</li>
</ul>
</li>
<li>
<a href="#toc_73">全序广播</a>
</li>
<li>
<a href="#toc_75">使用全序广播</a>
</li>
<li>
<a href="#toc_76">使用全序广播实现线性一致的存储</a>
</li>
<li>
<a href="#toc_77">使用线性一致性存储实现全序广播</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_78">分布式事务与共识</a>
</li>
<li>
<a href="#toc_80">原子提交与二阶段提交（2PC）</a>
<ul>
<li>
<a href="#toc_81">从单节点到分布式原子提交</a>
</li>
<li>
<a href="#toc_82">两阶段提交简介</a>
</li>
<li>
<a href="#toc_84">系统承诺</a>
</li>
<li>
<a href="#toc_85">协调者失效</a>
</li>
<li>
<a href="#toc_86">三阶段提交</a>
</li>
</ul>
</li>
<li>
<a href="#toc_87">实践中的分布式事务</a>
<ul>
<li>
<a href="#toc_88">恰好一次的消息处理</a>
</li>
<li>
<a href="#toc_89">XA事务</a>
</li>
<li>
<a href="#toc_90">怀疑时持有锁</a>
</li>
<li>
<a href="#toc_91">从协调者故障中恢复</a>
</li>
<li>
<a href="#toc_92">分布式事务的限制</a>
</li>
</ul>
</li>
<li>
<a href="#toc_93">容错共识</a>
<ul>
<li>
<a href="#toc_94">共识算法和全序广播</a>
</li>
<li>
<a href="#toc_95">单领导者复制和共识</a>
</li>
<li>
<a href="#toc_96">时代编号和法定人数</a>
</li>
<li>
<a href="#toc_97">共识的局限性</a>
</li>
</ul>
</li>
<li>
<a href="#toc_98">成员与协调服务</a>
<ul>
<li>
<a href="#toc_99">将工作分配给节点</a>
</li>
<li>
<a href="#toc_100">服务发现</a>
</li>
<li>
<a href="#toc_101">成员服务</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_102">本章小结</a>
</li>
<li>
<a href="#toc_103">参考文献</a>
</li>
</ul>
</li>
</ul>


<p>​   正如<a href="ch8.md">第8章</a>所讨论的，分布式系统中的许多事情可能会出错。处理这种故障的最简单方法是简单地让整个服务失效，并向用户显示错误消息。如果无法接受这个解决方案，我们就需要找到容错的方法—— 即使某些内部组件出现故障，服务也能正常运行。</p>

<p>​   在本章中，我们将讨论构建容错分布式系统的算法和协议的一些例子。我们将假设<a href="ch8.md">第8章</a>的所有问题都可能发生：网络中的数据包可能会丢失，重新排序，重复递送或任意延迟；时钟只是尽其所能地近似；且节点可以暂停（例如，由于垃圾收集）或随时崩溃。</p>

<p>​   构建容错系统的最好方法，是找到一些带有实用保证的通用抽象，实现一次，然后让应用依赖这些保证。这与<a href="ch7.md">第7章</a>中的事务处理方法相同：通过使用事务，应用可以假装没有崩溃（原子性），没有其他人同时访问数据库（隔离），存储设备是完全可靠的（持久性）。即使发生崩溃，竞态条件和磁盘故障，事务抽象隐藏了这些问题，因此应用不必担心它们。</p>

<p>​   现在我们将继续沿着同样的路线前进，寻求可以让应用忽略分布式系统部分问题的抽象概念。例如，分布式系统最重要的抽象之一就是<strong>共识（consensus）</strong>：<strong>就是让所有的节点对某件事达成一致</strong>。正如我们在本章中将会看到的那样，尽管存在网络故障和流程故障，可靠地达成共识是一个令人惊讶的棘手问题。</p>

<p>​   一旦达成共识，应用可以将其用于各种目的。例如，假设你有一个单主复制的数据库。如果领导者挂掉，并且需要故障切换到另一个节点，剩余的数据库节点可以使用共识来选举新的领导者。正如在“<a href="ch5.md#%E5%A4%84%E7%90%86%E8%8A%82%E7%82%B9%E5%AE%95%E6%9C%BA">处理节点宕机</a>”中所讨论的那样，重要的是只有一个领导者，且所有的节点都认同其领导。如果两个节点都认为自己是领导者，这种情况被称为<strong>脑裂（split brain）</strong>，且经常导致数据丢失。正确实现共识有助于避免这种问题。</p>

<p>​   在本章后面的“<a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%92%8C%E5%85%B1%E8%AF%86">分布式事务和共识</a>”中，我们将研究解决共识和相关问题的算法。但首先，我们首先需要探索可以在分布式系统中提供的保证和抽象的范围。</p>

<p>​   我们需要了解可以做什么和不可以做什么的范围：在某些情况下，系统可以容忍故障并继续工作；在其他情况下，这是不可能的。我们将深入研究什么可能而什么不可能的限制，既通过理论证明，也通过实际实现。我们将在本章中概述这些基本限制。</p>

<p>​   分布式系统领域的研究人员几十年来一直在研究这些主题，所以有很多资料—— 我们只能介绍一些皮毛。在本书中，我们没有空间去详细介绍形式模型和证明的细节，所以我们将坚持非正式的直觉。如果你有兴趣，参考文献可以提供更多的深度。</p>

<h2 id="toc_50">一致性保证</h2>

<p>​   在“<a href="ch5.md#%E5%A4%8D%E5%88%B6%E5%BB%B6%E8%BF%9F%E9%97%AE%E9%A2%98">复制延迟问题</a>”中，我们看到了数据库复制中发生的一些时序问题。如果你在同一时刻查看两个数据库节点，则可能在两个节点上看到不同的数据，因为写请求在不同的时间到达不同的节点。无论数据库使用何种复制方法（单主复制，多主复制或无主复制），都会出现这些不一致情况。</p>

<p>​   大多数复制的数据库至少提供了<strong>最终一致性</strong>，这意味着如果你停止向数据库写入数据并等待一段不确定的时间，那么最终所有的读取请求都会返回相同的值【1】。换句话说，不一致性是暂时的，最终会自行解决（假设网络中的任何故障最终都会被修复）。最终一致性的一个更好的名字可能是<strong>收敛（convergence）</strong>，因为我们预计所有的副本最终会收敛到相同的值【2】。</p>

<p>​   然而，这是一个非常弱的保证 —— 它并没有说什么时候副本会收敛。在收敛之前，读操作可能会返回任何东西或什么都没有【1】。例如，如果你写入了一个值，然后立即再次读取，这并不能保证你能看到刚跟写入的值，因为读请求可能会被路由到另外的副本上。（参阅“<a href="ch5.md#%E8%AF%BB%E5%B7%B1%E4%B9%8B%E5%86%99">读己之写</a>” ）。</p>

<p>​   对于应用开发人员而言，最终一致性是很困难的，因为它与普通单线程程序中变量的行为有很大区别。对于后者，如果将一个值赋给一个变量，然后很快地再次读取，不可能读到旧的值，或者读取失败。数据库表面上看起来像一个你可以读写的变量，但实际上它有更复杂的语义【3】。</p>

<p>​   在与只提供弱保证的数据库打交道时，你需要始终意识到它的局限性，而不是意外地作出太多假设。错误往往是微妙的，很难找到，也很难测试，因为应用可能在大多数情况下运行良好。当系统出现故障（例如网络中断）或高并发时，最终一致性的边缘情况才会显现出来。</p>

<p>​   本章将探索数据系统可能选择提供的更强一致性模型。它不是免费的：具有较强保证的系统可能会比保证较差的系统具有更差的性能或更少的容错性。尽管如此，更强的保证可以吸引人，因为它们更容易用对。只有见过不同的一致性模型后，才能更好地决定哪一个最适合自己的需求。</p>

<p>​   <strong>分布式一致性模型</strong>和我们之前讨论的事务隔离级别的层次结构有一些相似之处【4,5】（参见“<a href="ch7.md#%E5%BC%B1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB">弱隔离级别</a>”）。尽管两者有一部分内容重叠，但它们大多是无关的问题：事务隔离主要是为了，<strong>避免由于同时执行事务而导致的竞争状态</strong>，而分布式一致性主要关于，<strong>面对延迟和故障时，如何协调副本间的状态。</strong></p>

<p>本章涵盖了广泛的话题，但我们将会看到这些领域实际上是紧密联系在一起的：</p>

<ul>
<li>首先看一下常用的<strong>最强一致性模型</strong>之一，<strong>线性一致性（linearizability）</strong>，并考察其优缺点。</li>
<li>然后我们将检查分布式系统中<a href="#%E9%A1%BA%E5%BA%8F%E4%BF%9D%E8%AF%81"><strong>事件顺序</strong></a>的问题，特别是因果关系和全局顺序的问题。</li>
<li>在第三部分（“<a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%92%8C%E5%85%B1%E8%AF%86">分布式事务和共识</a>”）中将探讨如何原子地提交分布式事务，这将最终引领我们走向共识问题的解决方案。</li>
</ul>

<h2 id="toc_51">线性一致性</h2>

<p>​   在<strong>最终一致</strong>的数据库，如果你在同一时刻问两个不同副本相同的问题，可能会得到两个不同的答案。这很让人困惑。如果数据库可以提供只有一个副本的假象（即，只有一个数据副本），那么事情就简单太多了。那么每个客户端都会有相同的数据视图，且不必担心复制滞后了。</p>

<p>​   这就是<strong>线性一致性（linearizability）</strong>背后的想法【6】（也称为<strong>原子一致性（atomic consistency）</strong>【7】，<strong>强一致性（strong consistency）</strong>，<strong>立即一致性（immediate consistency）</strong>或<strong>外部一致性（external consistency ）</strong>【8】）。线性一致性的精确定义相当微妙，我们将在本节的剩余部分探讨它。但是基本的想法是让一个系统看起来好像只有一个数据副本，而且所有的操作都是原子性的。有了这个保证，即使实际中可能有多个副本，应用也不需要担心它们。</p>

<p>​   在一个线性一致的系统中，只要一个客户端成功完成写操作，所有客户端从数据库中读取数据必须能够看到刚刚写入的值。维护数据的单个副本的错觉是指，系统能保障读到的值是最近的，最新的，而不是来自陈旧的缓存或副本。换句话说，线性一致性是一个<strong>新鲜度保证（recency guarantee）</strong>。为了阐明这个想法，我们来看看一个非线性一致系统的例子。</p>

<p><img src="img/fig9-1.png" alt=""/></p>

<p><strong>图9-1 这个系统是非线性一致的，导致了球迷的困惑</strong></p>

<p>​   <a href="img/fig9-1.png">图9-1 </a>展示了一个关于体育网站的非线性一致例子【9】。Alice和Bob正坐在同一个房间里，都盯着各自的手机，关注着2014年FIFA世界杯决赛的结果。在最后得分公布后，Alice刷新页面，看到宣布了获胜者，并兴奋地告诉Bob。Bob难以置信地刷新了自己的手机，但他的请求路由到了一个落后的数据库副本上，手机显示比赛仍在进行。</p>

<p>​   如果Alice和Bob在同一时间刷新并获得了两个不同的查询结果，也许就没有那么令人惊讶了。因为他们不知道服务器处理他们请求的精确时刻。然而Bob是在听到Alice惊呼最后得分<strong>之后</strong>，点击了刷新按钮（启动了他的查询），因此他希望查询结果至少与爱丽丝一样新鲜。但他的查询返回了陈旧结果，这一事实违背了线性一致性的要求。</p>

<h3 id="toc_52">什么使得系统线性一致？</h3>

<p>​   线性一致性背后的基本思想很简单：使系统看起来好像只有一个数据副本。然而确切来讲，实际上有更多要操心的地方。为了更好地理解线性一致性，让我们再看几个例子。</p>

<p>​   <a href="img/fig9-2.png">图9-2</a> 显示了三个客户端在线性一致数据库中同时读写相同的键<code>x</code>。在分布式系统文献中，<code>x</code>被称为<strong>寄存器（register）</strong>，例如，它可以是键值存储中的一个<strong>键</strong>，关系数据库中的一<strong>行</strong>，或文档数据库中的一个<strong>文档</strong>。</p>

<p><img src="img/fig9-2.png" alt=""/></p>

<p><strong>图9-2 如果读取请求与写入请求并发，则可能会返回旧值或新值</strong></p>

<p>​   为了简单起见，<a href="img/fig9-2.png">图9-2</a>采用了用户请求的视角，而不是数据库内部的视角。每个柱都是由客户端发出的请求，其中柱头是请求发送的时刻，柱尾是客户端收到响应的时刻。因为网络延迟变化无常，客户端不知道数据库处理其请求的精确时间——只知道它发生在发送请求和接收响应的之间的某个时刻。<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup></p>

<p>在这个例子中，寄存器有两种类型的操作：</p>

<ul>
<li><p>\( read(x)⇒v\)表示客户端请求读取寄存器 <code>x</code> 的值，数据库返回值 <code>v</code>。</p></li>
<li><p>\(write(x,v)⇒r\) 表示客户端请求将寄存器 <code>x</code> 设置为值 <code>v</code> ，数据库返回响应 <code>r</code> （可能正确，可能错误）。</p></li>
</ul>

<p>在<a href="img/fig9-2.png">图9-2</a> 中，<code>x</code> 的值最初为 <code>0</code>，客户端C 执行写请求将其设置为 <code>1</code>。发生这种情况时，客户端A和B反复轮询数据库以读取最新值。 A和B的请求可能会收到怎样的响应？</p>

<ul>
<li>客户端A的第一个读操作，完成于写操作开始之前，因此必须返回旧值 <code>0</code>。</li>
<li>客户端A的最后一个读操作，开始于写操作完成之后。如果数据库是线性一致性的，它必然返回新值 <code>1</code>：因为读操作和写操作一定是在其各自的起止区间内的某个时刻被处理。如果在写入结束后开始读取，则必须在写入之后处理读取，因此它必须看到写入的新值。</li>
<li>与写操作在时间上重叠的任何读操作，可能会返回 <code>0</code> 或 <code>1</code> ，因为我们不知道读取时，写操作是否已经生效。这些操作是<strong>并发（concurrent）</strong>的。</li>
</ul>

<p>但是，这还不足以完全描述线性一致性：如果与写入同时发生的读取可以返回旧值或新值，那么读者可能会在写入期间看到数值在旧值和新值之间来回翻转。这不是我们所期望的仿真“单一数据副本”的系统。<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup></p>

<p>为了使系统线性一致，我们需要添加另一个约束，如<a href="img/fig9-3.png">图9-3</a>所示</p>

<p><img src="img/fig9-3.png" alt=""/><br/>
<strong>图9-3 任何一个读取返回新值后，所有后续读取（在相同或其他客户端上）也必须返回新值。</strong></p>

<p>​   在一个线性一致的系统中，我们可以想象，在 <code>x</code> 的值从<code>0</code> 自动翻转到 <code>1</code> 的时候（在写操作的开始和结束之间）必定有一个时间点。因此，如果一个客户端的读取返回新的值 <code>1</code>，即使写操作尚未完成，所有后续读取也必须返回新值。</p>

<p>​   <a href="img/fig9-3.png">图9-3</a>中的箭头说明了这个时序依赖关系。客户端A 是第一个读取新的值 <code>1</code> 的位置。在A 的读取返回之后，B开始新的读取。由于B的读取严格在发生于A的读取之后，因此即使C的写入仍在进行中，也必须返回 <code>1</code>。 （与<a href="img/fig9-1.png">图9-1</a>中的Alice和Bob的情况相同：在Alice读取新值之后，Bob也希望读取新的值。）</p>

<p>​   我们可以进一步细化这个时序图，展示每个操作是如何在特定时刻原子性生效的。<a href="img/fig9-4.png">图9-4</a>显示了一个更复杂的例子【10】。</p>

<p>在<a href="">图9-4</a>中，除了读写之外，还增加了第三种类型的操作：</p>

<ul>
<li>\(cas(x, v_{old}, v_{new})⇒r\) 表示客户端请求进行原子性的<a href="ch7.md#%E6%AF%94%E8%BE%83%E5%B9%B6%E8%AE%BE%E7%BD%AE%EF%BC%88CAS%EF%BC%89"><strong>比较与设置</strong></a>操作。如果寄存器 \(x\) 的当前值等于 \(v_{old}\) ，则应该原子地设置为 \(v_{new}\) 。如果 \(x≠v_{old}\) ，则操作应该保持寄存器不变并返回一个错误。 \(r\) 是数据库的响应（正确或错误）。</li>
</ul>

<p><a href="">图9-4</a>中的每个操作都在我们认为执行操作的时候用竖线标出（在每个操作的条柱之内）。这些标记按顺序连在一起，其结果必须是一个有效的寄存器读写序列（<strong>每次读取都必须返回最近一次写入设置的值</strong>）。</p>

<p>​   线性一致性的要求是，操作标记的连线总是按时间（从左到右）向前移动，而不是向后移动。这个要求确保了我们之前讨论的新鲜性保证：一旦新的值被写入或读取，所有后续的读都会看到写入的值，直到它被再次覆盖。</p>

<p><img src="img/fig9-4.png" alt=""/></p>

<p><strong>图9-4 可视化读取和写入看起来已经生效的时间点。 B的最后读取不是线性一致性的</strong></p>

<p><a href="">图9-4</a>中有一些有趣的细节需要指出：</p>

<ul>
<li><p>第一个客户端B发送一个读取 <code>x</code> 的请求，然后客户端D发送一个请求将 <code>x</code> 设置为 <code>0</code>，然后客户端A发送请求将 <code>x</code> 设置为 <code>1</code>。尽管如此，返回到B的读取值为 <code>1</code>（由A写入的值）。这是可以的：这意味着数据库首先处理D的写入，然后是A的写入，最后是B的读取。虽然这不是请求发送的顺序，但这是一个可以接受的顺序，因为这三个请求是并发的。也许B的读请求在网络上略有延迟，所以它在两次写入之后才到达数据库。</p></li>
<li><p>在客户端A从数据库收到响应之前，客户端B的读取返回 <code>1</code> ，表示写入值 <code>1</code> 已成功。这也是可以的：这并不意味着在写之前读到了值，这只是意味着从数据库到客户端A的正确响应在网络中略有延迟。</p></li>
<li><p>此模型不假设有任何事务隔离：另一个客户端可能随时更改值。例如，C首先读取 <code>1</code> ，然后读取 <code>2</code> ，因为两次读取之间的值由B更改。可以使用原子<strong>比较并设置（cas）</strong>操作来检查该值是否未被另一客户端同时更改：B和C的<strong>cas</strong>请求成功，但是D的<strong>cas</strong>请求失败（在数据库处理它时，<code>x</code> 的值不再是 <code>0</code> ）。</p></li>
<li><p>客户B的最后一次读取（阴影条柱中）不是线性一致性的。 该操作与C的<strong>cas</strong>写操作并发（它将 <code>x</code> 从 <code>2</code> 更新为 <code>4</code> ）。在没有其他请求的情况下，B的读取返回 <code>2</code> 是可以的。然而，在B的读取开始之前，客户端A已经读取了新的值 <code>4</code>  ，因此不允许B读取比A更旧的值。再次，与<a href="img/fig9-1.png">图9-1</a>中的Alice和Bob的情况相同。</p>

<p>这就是线性一致性背后的直觉。 正式的定义【6】更准确地描述了它。 通过记录所有请求和响应的时序，并检查它们是否可以排列成有效的顺序，测试一个系统的行为是否线性一致性是可能的（尽管在计算上是昂贵的）【11】。</p></li>
</ul>

<blockquote>
<h3 id="toc_53">线性一致性与可序列化</h3>

<p><strong>线性一致性</strong>容易和<a href="ch7.md#%E5%8F%AF%E5%BA%8F%E5%88%97%E5%8C%96"><strong>可序列化</strong></a>相混淆，因为两个词似乎都是类似“可以按顺序排列”的东西。但它们是两种完全不同的保证，区分两者非常重要：</p>

<p><strong><em>可序列化</em></strong></p>

<p><strong>可序列化（Serializability）</strong>是事务的隔离属性，每个事务可以读写多个对象（行，文档，记录）——参阅“<a href="ch7.md#%E5%8D%95%E5%AF%B9%E8%B1%A1%E5%92%8C%E5%A4%9A%E5%AF%B9%E8%B1%A1%E6%93%8D%E4%BD%9C">单对象和多对象操作</a>”。它确保事务的行为，与它们按照<strong>某种</strong>顺序依次执行的结果相同（每个事务在下一个事务开始之前运行完成）。这种执行顺序可以与事务实际执行的顺序不同。【12】。</p>

<p><strong><em>线性一致性</em></strong></p>

<p><strong>线性一致性（Linearizability）</strong>是读取和写入寄存器（单个对象）的<strong>新鲜度保证</strong>。它不会将操作组合为事务，因此它也不会阻止写偏差等问题（参阅“<a href="ch7.md#%E5%86%99%E5%81%8F%E5%B7%AE%E5%92%8C%E5%B9%BB%E8%AF%BB">写偏差和幻读</a>”），除非采取其他措施（例如<a href="ch7.md#%E7%89%A9%E5%8C%96%E5%86%B2%E7%AA%81">物化冲突</a>）。</p>

<p>一个数据库可以提供可序列化和线性一致性，这种组合被称为严格的可序列化或强的<strong>强单副本可序列化（strong-1SR）</strong>【4,13】。基于两阶段锁定的可序列化实现（参见“<a href="#%E4%B8%A4%E9%98%B6%E6%AE%B5%E9%94%81%E5%AE%9A%EF%BC%882PL%EF%BC%89">两阶段锁定（2PL）</a>”一节）或<strong>实际串行执行</strong>（参见第“<a href="ch7.md#%E5%AE%9E%E9%99%85%E4%B8%B2%E8%A1%8C%E6%89%A7%E8%A1%8C">实际串行执行</a>”）通常是线性一致性的。</p>

<p>但是，可序列化的快照隔离（参见“<a href="ch7.md#%E5%8F%AF%E5%BA%8F%E5%88%97%E5%8C%96%E7%9A%84%E5%BF%AB%E7%85%A7%E9%9A%94%E7%A6%BB%EF%BC%88SSI%EF%BC%89">可序列化的快照隔离（SSI）</a>”）不是线性一致性的：按照设计，它从一致的快照中进行读取，以避免读者和写者之间的锁竞争。一致性快照的要点就在于<strong>它不会包括该快照之后的写入</strong>，因此从快照读取不是线性一致性的。</p>
</blockquote>

<h3 id="toc_54">依赖线性一致性</h3>

<p>​   线性一致性在什么情况下有用？观看体育比赛的最后得分可能是一个轻率的例子：过了几秒钟的结果不可能在这种情况下造成任何真正的伤害。然而对于少数领域，线性一致性是系统正确工作的一个重要条件。</p>

<h4 id="toc_55">锁定和领导选举</h4>

<p>​   一个使用单主复制的系统，需要确保领导真的只有一个，而不是几个（脑裂）。一种选择领导者的方法是使用锁：每个节点在启动时尝试获取锁，成功者成为领导者【14】。不管这个锁是如何实现的，它必须是线性一致的：所有节点必须就哪个节点拥有锁达成一致，否则就没用了。</p>

<p>​   诸如Apache ZooKeeper 【15】和etcd 【16】之类的协调服务通常用于实现分布式锁和领导者选举。它们使用一致性算法，以容错的方式实现线性一致的操作（在本章后面的“<a href="#%E5%AE%B9%E9%94%99%E5%85%B1%E8%AF%86">容错共识</a>”中讨论此类算法）<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup>。还有许多微妙的细节来正确地实现锁和领导者选举（例如，参阅“<a href="#%E9%A2%86%E5%AF%BC%E8%80%85%E5%92%8C%E9%94%81">领导者和锁</a>”中的防护问题），而像Apache Curator 【17】这样的库则通过在ZooKeeper之上提供更高级别的配方来提供帮助。但是，线性一致性存储服务是这些协调任务的基础。</p>

<p>​   分布式锁也在一些分布式数据库（如Oracle Real Application Clusters（RAC）【18】）中更多的粒度级别上使用。RAC对每个磁盘页面使用一个锁，多个节点共享对同一个磁盘存储系统的访问权限。由于这些线性一致的锁处于事务执行的关键路径上，RAC部署通常具有用于数据库节点之间通信的专用集群互连网络。</p>

<h4 id="toc_56">约束和唯一性保证</h4>

<p>​   唯一性约束在数据库中很常见：例如，用户名或电子邮件地址必须唯一标识一个用户，而在文件存储服务中，不能有两个具有相同路径和文件名的文件。如果要在写入数据时强制执行此约束（例如，如果两个人试图同时创建一个具有相同名称的用户或文件，其中一个将返回一个错误），则需要线性一致性。</p>

<p>​   这种情况实际上类似于一个锁：当一个用户注册你的服务时，可以认为他们获得了所选用户名的“锁定”。该操作与原子性的比较与设置非常相似：将用户名赋予声明它的用户，前提是用户名尚未被使用。</p>

<p>​   如果想要确保银行账户余额永远不会为负数，或者不会出售比仓库里的库存更多的物品，或者两个人不会都预定了航班或剧院里同一时间的同一个位置。这些约束条件都要求所有节点都同意一个最新的值（账户余额，库存水平，座位占用率）。</p>

<p>​   在实际应用中，宽松地处理这些限制有时是可以接受的（例如，如果航班超额预订，你可以将客户转移到不同的航班并为其提供补偿）。在这种情况下，可能不需要线性一致性，我们将在“<a href="ch12.md#%E5%8F%8A%E6%97%B6%E6%80%A7%E4%B8%8E%E5%AE%8C%E6%95%B4%E6%80%A7">及时性与完整性</a>”中讨论这种宽松的约束。</p>

<p>​   然而，一个硬性的唯一性约束（关系型数据库中常见的那种）需要线性一致性。其他类型的约束，如外键或属性约束，可以不需要线性一致性【19】。</p>

<h4 id="toc_57">跨信道的时序依赖</h4>

<p>​   注意<a href="img/fig9-1.png">图9-1</a> 中的一个细节：如果Alice没有惊呼得分，Bob就不会知道他的查询结果是陈旧的。他会在几秒钟之后再次刷新页面，并最终看到最后的分数。由于系统中存在额外的信道（Alice的声音传到了Bob的耳朵中），线性一致性的违背才被注意到。</p>

<p>​   计算机系统也会出现类似的情况。例如，假设有一个网站，用户可以上传照片，一个后台进程会调整照片大小，降低分辨率以加快下载速度（缩略图）。该系统的架构和数据流如<a href="img/fig9-5.png">图9-5</a>所示。</p>

<p>​   图像缩放器需要明确的指令来执行尺寸缩放作业，指令是Web服务器通过消息队列发送的（参阅<a href="ch11.md">第11章</a>）。 Web服务器不会将整个照片放在队列中，因为大多数消息代理都是针对较短的消息而设计的，而一张照片的空间占用可能达到几兆字节。取而代之的是，首先将照片写入文件存储服务，写入完成后再将缩放器的指令放入消息队列。<br/>
<img src="img/fig9-5.png" alt=""/><br/>
<strong>图9-5 Web服务器和图像调整器通过文件存储和消息队列进行通信，打开竞争条件的可能性。</strong></p>

<p>​   如果文件存储服务是线性一致的，那么这个系统应该可以正常工作。如果它不是线性一致的，则存在竞争条件的风险：消息队列（<a href="img/fig9-5.png">图9-5</a>中的步骤3和4）可能比存储服务内部的复制更快。在这种情况下，当缩放器读取图像（步骤5）时，可能会看到图像的旧版本，或者什么都没有。如果它处理的是旧版本的图像，则文件存储中的全尺寸图和略缩图就产生了永久性的不一致。</p>

<p>​   出现这个问题是因为Web服务器和缩放器之间存在两个不同的信道：文件存储与消息队列。没有线性一致性的新鲜性保证，这两个信道之间的竞争条件是可能的。这种情况类似于<a href="img/fig9-1.png">图9-1</a>，数据库复制与Alice的嘴到Bob耳朵之间的真人音频信道之间也存在竞争条件。</p>

<p>​   线性一致性并不是避免这种竞争条件的唯一方法，但它是最容易理解的。如果你可以控制额外信道（例如消息队列的例子，而不是在Alice和Bob的例子），则可以使用在“<a href="ch5.md#%E8%AF%BB%E5%B7%B1%E4%B9%8B%E5%86%99">读己之写</a>”讨论过的备选方法，不过会有额外的复杂度代价。</p>

<h3 id="toc_58">实现线性一致的系统</h3>

<p>​   我们已经见到了几个线性一致性有用的例子，让我们思考一下，如何实现一个提供线性一致语义的系统。</p>

<p>​   由于线性一致性本质上意味着“表现得好像只有一个数据副本，而且所有的操作都是原子的”，所以最简单的答案就是，真的只用一个数据副本。但是这种方法无法容错：如果持有该副本的节点失效，数据将会丢失，或者至少无法访问，直到节点重新启动。</p>

<p>​   使系统容错最常用的方法是使用复制。我们再来回顾<a href="ch5.md">第5章</a>中的复制方法，并比较它们是否可以满足线性一致性：</p>

<p><strong><em>单主复制（可能线性一致）</em></strong></p>

<p>​   在具有单主复制功能的系统中（参见“<a href="ch5.md#%E9%A2%86%E5%AF%BC%E8%80%85%E4%B8%8E%E8%BF%BD%E9%9A%8F%E8%80%85">领导者与追随者</a>”），主库具有用于写入的数据的主副本，而追随者在其他节点上保留数据的备份副本。如果从主库或同步更新的从库读取数据，它们<strong>可能（protential）</strong>是线性一致性的<sup id="fnref4"><a href="#fn4" rel="footnote">4</a></sup>。然而，并不是每个单主数据库都是实际线性一致性的，无论是通过设计（例如，因为使用快照隔离）还是并发错误【10】。</p>

<p>​   从主库读取依赖一个假设，你确定领导是谁。正如在“<a href="ch8.md#%E7%9C%9F%E7%90%86%E8%A2%AB%E5%A4%9A%E6%95%B0%E4%BA%BA%E5%AE%9A%E4%B9%89">真理在多数人手中</a>”中所讨论的那样，一个节点很可能会认为它是领导者，而事实上并非如此——如果具有错觉的领导者继续为请求提供服务，可能违反线性一致性【20】。使用异步复制，故障切换时甚至可能会丢失已提交的写入（参阅“<a href="ch5.md#%E5%A4%84%E7%90%86%E8%8A%82%E7%82%B9%E5%AE%95%E6%9C%BA">处理节点宕机</a>”），这同时违反了持久性和线性一致性。</p>

<p><strong><em>共识算法（线性一致）</em></strong></p>

<p>​   一些在本章后面讨论的共识算法，与单领导者复制类似。然而，共识协议包含防止脑裂和陈旧副本的措施。正是由于这些细节，共识算法可以安全地实现线性一致性存储。例如，Zookeeper 【21】和etcd 【22】就是这样工作的。</p>

<p><strong><em>多主复制（非线性一致）</em></strong></p>

<p>​   具有多主程序复制的系统通常不是线性一致的，因为它们同时在多个节点上处理写入，并将其异步复制到其他节点。因此，它们可能会产生冲突的写入，需要解析（参阅“<a href="ch5.md#%E5%A4%84%E7%90%86%E5%86%99%E5%85%A5%E5%86%B2%E7%AA%81">处理写入冲突</a>”）。这种冲突是因为缺少单一数据副本人为产生的。</p>

<p><strong><em>无主复制（也许不是线性一致的）</em></strong></p>

<p>​   对于无领导者复制的系统（Dynamo风格；参阅“<a href="ch5.md#%E6%97%A0%E4%B8%BB%E5%A4%8D%E5%88%B6">无主复制</a>”），有时候人们会声称通过要求法定人数读写（ \(w + r&gt; n\) ）可以获得“强一致性”。这取决于法定人数的具体配置，以及强一致性如何定义（通常不完全正确）。</p>

<p>​   基于时钟（例如，在Cassandra中；参见“<a href="ch8.md#%E4%BE%9D%E8%B5%96%E5%90%8C%E6%AD%A5%E6%97%B6%E9%92%9F">依赖同步时钟</a>”）的“最后写入胜利”冲突解决方法几乎可以确定是非线性的，由于时钟偏差，不能保证时钟的时间戳与实际事件顺序一致。<a href="ch5.md#%E5%AE%BD%E6%9D%BE%E7%9A%84%E6%B3%95%E5%AE%9A%E4%BA%BA%E6%95%B0%E4%B8%8E%E6%8F%90%E7%A4%BA%E7%A7%BB%E4%BA%A4">宽松的法定人数</a>也破坏了线性一致的可能性。即使使用严格的法定人数，非线性一致的行为也是可能的，如下节所示。</p>

<h4 id="toc_59">线性一致性和法定人数</h4>

<p>​   直觉上在Dynamo风格的模型中，严格的法定人数读写应该是线性一致性的。但是当我们有可变的网络延迟时，就可能存在竞争条件，如<a href="img/fig9-6.png">图9-6</a>所示。</p>

<p><img src="img/fig9-6.png" alt=""/></p>

<p><strong>图9-6 非线性一致的执行，尽管使用了严格的法定人数</strong></p>

<p>​   在<a href="img/fig9-6.png">图9-6</a>中，\(x\) 的初始值为0，写入客户端通过向所有三个副本（ \(n = 3, w = 3\) ）发送写入将 \(x\) 更新为 <code>1</code>。客户端A并发地从两个节点组成的法定人群（ \(r = 2\) ）中读取数据，并在其中一个节点上看到新值 <code>1</code> 。客户端B也并发地从两个不同的节点组成的法定人数中读取，并从两个节点中取回了旧值 <code>0</code> 。</p>

<p>​   法定人数条件满足（ \(w + r&gt; n\) ），但是这个执行是非线性一致的：B的请求在A的请求完成后开始，但是B返回旧值，而A返回新值。 （又一次，如同Alice和Bob的例子 <a href="">图9-1</a>）</p>

<p>​   有趣的是，通过牺牲性能，可以使Dynamo风格的法定人数线性化：读取者必须在将结果返回给应用之前，同步执行读修复（参阅“<a href="ch5.md#%E8%AF%BB%E6%97%B6%E4%BF%AE%E5%A4%8D%E4%B8%8E%E5%8F%8D%E7%86%B5%E8%BF%87%E7%A8%8B">读时修复与反熵过程</a>”） ，并且写入者必须在发送写入之前，读取法定数量节点的最新状态【24,25】。然而，由于性能损失，Riak不执行同步读修复【26】。 Cassandra在进行法定人数读取时，<strong>确实</strong>在等待读修复完成【27】；但是由于使用了最后写入胜利的冲突解决方案，当同一个键有多个并发写入时，将不能保证线性一致性。</p>

<p>​   而且，这种方式只能实现线性一致的读写；不能实现线性一致的比较和设置操作，因为它需要一个共识算法【28】。</p>

<p>​   总而言之，最安全的做法是：假设采用Dynamo风格无主复制的系统不能提供线性一致性。</p>

<h3 id="toc_60">线性一致性的代价</h3>

<p>​   一些复制方法可以提供线性一致性，另一些复制方法则不能，因此深入地探讨线性一致性的优缺点是很有趣的。</p>

<p>​   我们已经在<a href="ch5.md">第五章</a>中讨论了不同复制方法的一些用例。例如对多数据中心的复制而言，多主复制通常是理想的选择（参阅“<a href="ch5.md#%E8%BF%90%E7%BB%B4%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83">运维多个数据中心</a>”）。<a href="img/fig9-7.png">图9-7</a>说明了这种部署的一个例子。</p>

<p><img src="img/fig9-7.png" alt=""/></p>

<p><strong>图9-7 网络中断迫使在线性一致性和可用性之间做出选择。</strong></p>

<p>​   考虑这样一种情况：如果两个数据中心之间发生网络中断会发生什么？我们假设每个数据中心内的网络正在工作，客户端可以访问数据中心，但数据中心之间彼此无法互相连接。</p>

<p>​   使用多主数据库，每个数据中心都可以继续正常运行：由于在一个数据中心写入的数据是异步复制到另一个数据中心的，所以在恢复网络连接时，写入操作只是简单地排队并交换。</p>

<p>​   另一方面，如果使用单主复制，则主库必须位于其中一个数据中心。任何写入和任何线性一致的读取请求都必须发送给该主库，因此对于连接到从库所在数据中心的客户端，这些读取和写入请求必须通过网络同步发送到主库所在的数据中心。</p>

<p>​   在单主配置的条件下，如果数据中心之间的网络被中断，则连接到从库数据中心的客户端无法联系到主库，因此它们无法对数据库执行任何写入，也不能执行任何线性一致的读取。它们仍能从从库读取，但结果可能是陈旧的（非线性一致）。如果应用需要线性一致的读写，却又位于与主库网络中断的数据中心，则网络中断将导致这些应用不可用。</p>

<p>如果客户端可以直接连接到主库所在的数据中心，这就不是问题了，那些应用可以继续正常工作。但只能访问从库数据中心的客户端会中断运行，直到网络链接得到修复。</p>

<h4 id="toc_61">CAP定理</h4>

<p>​   这个问题不仅仅是单主复制和多主复制的后果：任何线性一致的数据库都有这个问题，不管它是如何实现的。这个问题也不仅仅局限于多数据中心部署，而可能发生在任何不可靠的网络上，即使在同一个数据中心内也是如此。问题面临的权衡如下：<sup id="fnref5"><a href="#fn5" rel="footnote">5</a></sup></p>

<ul>
<li>如果应用需要线性一致性，且某些副本因为网络问题与其他副本断开连接，那么这些副本掉线时不能处理请求。请求必须等到网络问题解决，或直接返回错误。（无论哪种方式，服务都<strong>不可用（unavailable）</strong>）。</li>
<li>如果应用不需要线性一致性，那么某个副本即使与其他副本断开连接，也可以独立处理请求（例如多主复制）。在这种情况下，应用可以在网络问题前保持可用，但其行为不是线性一致的。</li>
</ul>

<p>因此不需要线性一致性的应用对网络问题有更强的容错能力。这种见解通常被称为CAP定理【29,30,31,32】，由Eric Brewer于2000年命名，尽管70年代的分布式数据库设计者早就知道了这种权衡【33,34,35,36】。</p>

<p>​   CAP最初是作为一个经验法则提出的，没有准确的定义，目的是开始讨论数据库的权衡。那时候许多分布式数据库侧重于在共享存储的集群上提供线性一致性的语义【18】，CAP定理鼓励数据库工程师向分布式无共享系统的设计领域深入探索，这类架构更适合实现大规模的网络服务【37】。 对于这种文化上的转变，CAP值得赞扬 —— 它见证了自00年代中期以来新数据库的技术爆炸（即NoSQL）。</p>

<blockquote>
<h3 id="toc_62">CAP定理没有帮助</h3>

<p>CAP有时以这种面目出现：一致性，可用性和分区容错性：三者只能择其二。不幸的是这种说法很有误导性【32】，因为网络分区是一种错误，所以它并不是一个选项：不管你喜不喜欢它都会发生【38】。</p>

<p>在网络正常工作的时候，系统可以提供一致性（线性一致性）和整体可用性。发生网络故障时，你必须在线性一致性和整体可用性之间做出选择。因此，CAP更好的表述成：在分区时要么选择一致，要么选择可用【39】。一个更可靠的网络需要减少这个选择，但是在某些时候选择是不可避免的。</p>

<p>在CAP的讨论中，术语可用性有几个相互矛盾的定义，形式化作为一个定理【30】并不符合其通常的含义【40】。许多所谓的“高可用”（容错）系统实际上不符合CAP对可用性的特殊定义。总而言之，围绕着CAP有很多误解和困惑，并不能帮助我们更好地理解系统，所以最好避免使用CAP。</p>
</blockquote>

<p>​   CAP定理的正式定义仅限于很狭隘的范围【30】，它只考虑了一个一致性模型（即线性一致性）和一种故障（网络分区<sup id="fnref6"><a href="#fn6" rel="footnote">6</a></sup>，或活跃但彼此断开的节点）。它没有讨论任何关于网络延迟，死亡节点或其他权衡的事。 因此，尽管CAP在历史上有一些影响力，但对于设计系统而言并没有实际价值【9,40】。</p>

<p>​   在分布式系统中有更多有趣的“不可能”的结果【41】，且CAP定理现在已经被更精确的结果取代【2,42】，所以它现在基本上成了历史古迹了。</p>

<h4 id="toc_63">线性一致性和网络延迟</h4>

<p>​   虽然线性一致是一个很有用的保证，但实际上，线性一致的系统惊人的少。例如，现代多核CPU上的内存甚至都不是线性一致的【43】：如果一个CPU核上运行的线程写入某个内存地址，而另一个CPU核上运行的线程不久之后读取相同的地址，并没有保证一定能一定读到第一个线程写入的值（除非使用了<strong>内存屏障（memory barrier）</strong>或<strong>围栏（fence）</strong>【44】）。</p>

<p>​   这种行为的原因是每个CPU核都有自己的内存缓存和存储缓冲区。默认情况下，内存访问首先走缓存，任何变更会异步写入主存。因为缓存访问比主存要快得多【45】，所以这个特性对于现代CPU的良好性能表现至关重要。但是现在就有几个数据副本（一个在主存中，也许还有几个在不同缓存中的其他副本），而且这些副本是异步更新的，所以就失去了线性一致性。</p>

<p>​   为什么要做这个权衡？对多核内存一致性模型而言，CAP定理是没有意义的：在同一台计算机中，我们通常假定通 信都是可靠的。并且我们并不指望一个CPU核能在脱离计算机其他部分的条件下继续正常工作。牺牲线性一致性的原因是<strong>性能（performance）</strong>，而不是容错。</p>

<p>​   许多分布式数据库也是如此：它们是<strong>为了提高性能</strong>而选择了牺牲线性一致性，而不是为了容错【46】。线性一致的速度很慢——这始终是事实，而不仅仅是网络故障期间。</p>

<p>​   能找到一个更高效的线性一致存储实现吗？看起来答案是否定的：Attiya和Welch 【47】证明，如果你想要线性一致性，读写请求的响应时间至少与网络延迟的不确定性成正比。在像大多数计算机网络一样具有高度可变延迟的网络中（参见“<a href="ch8.md#%E8%B6%85%E6%97%B6%E4%B8%8E%E6%97%A0%E7%A9%B7%E7%9A%84%E5%BB%B6%E8%BF%9F">超时与无穷的延迟</a>”），线性读写的响应时间不可避免地会很高。更快地线性一致算法不存在，但更弱的一致性模型可以快得多，所以对延迟敏感的系统而言，这类权衡非常重要。在<a href="ch12.md">第12章</a>中将讨论一些在不牺牲正确性的前提下，绕开线性一致性的方法。</p>

<h2 id="toc_64">顺序保证</h2>

<p>​   之前说过，线性一致寄存器的行为就好像只有单个数据副本一样，且每个操作似乎都是在某个时间点以原子性的方式生效的。这个定义意味着操作是按照某种良好定义的顺序执行的。我们通过操作（似乎）执行完毕的顺序来连接操作，以此说明<a href="img/fig9-4.png">图9-4</a>中的顺序。</p>

<p><strong>顺序（ordering）</strong>这一主题在本书中反复出现，这表明它可能是一个重要的基础性概念。让我们简要回顾一下其它<strong>顺序</strong>曾经出现过的上下文：</p>

<ul>
<li>在<a href="ch5.md">第5章</a>中我们看到，领导者在单主复制中的主要目的就是，在复制日志中确定<strong>写入顺序（order of write）</strong>——也就是从库应用这些写入的顺序。如果不存在一个领导者，则并发操作可能导致冲突（参阅“<a href="ch5.md#%E5%A4%84%E7%90%86%E5%86%99%E5%85%A5%E5%86%B2%E7%AA%81">处理写入冲突</a>”）。</li>
<li>在<a href="ch7.md">第7章</a>中讨论的<strong>可序列化</strong>，是关于事务表现的像按<strong>某种序列顺序（some sequential order）</strong>执行的保证。它可以通过字面意义上地<strong>序列顺序（serial order）</strong>执行事务来实现，或者通过允许并行执行，同时防止序列化冲突来实现（通过锁或中止事务）。</li>
<li>在<a href="ch8.md">第8章</a>讨论过的在分布式系统中使用时间戳和时钟（参阅“<a href="ch8.md#%E4%BE%9D%E8%B5%96%E4%BA%8E%E5%90%8C%E6%AD%A5%E6%97%B6%E9%92%9F">依赖于同步时钟</a>”）是另一种将顺序引入无序世界的尝试，例如，确定两个写入操作哪一个更晚发生。</li>
</ul>

<p>事实证明，顺序，线性一致性和共识之间有着深刻的联系。尽管这个概念比本书其他部分更加理论化和抽象，但对于明确系统的能力范围（可以做什么和不可以做什么）而言是非常有帮助的。我们将在接下来的几节中探讨这个话题。</p>

<h3 id="toc_65">顺序与因果</h3>

<p><strong>顺序</strong>反复出现有几个原因，其中一个原因是，它有助于保持<strong>因果关系（causality）</strong>。在本书中我们已经看到了几个例子，其中因果关系是很重要的：</p>

<ul>
<li>在“<a href="ch5.md#%E4%B8%80%E8%87%B4%E5%89%8D%E7%BC%80%E8%AF%BB">一致前缀读</a>”（<a href="img/fig5-5.png">图5-5</a>）中，我们看到一个例子：一个对话的观察者首先看到问题的答案，然后才看到被回答的问题。这是令人困惑的，因为它违背了我们对<strong>因（cause）</strong>与<strong>果（effect）</strong>的直觉：如果一个问题被回答，显然问题本身得先在那里，因为给出答案的人必须看到这个问题（假如他们并没有预见未来的超能力）。我们认为在问题和答案之间存在<strong>因果依赖（causal dependency）</strong>。</li>
<li><a href="img/fig5-9.png">图5-9</a>中出现了类似的模式，我们看到三位领导者之间的复制，并注意到由于网络延迟，一些写入可能会“压倒”其他写入。从其中一个副本的角度来看，好像有一个对尚不存在的记录的更新操作。这里的因果意味着，一条记录必须先被创建，然后才能被更新。</li>
<li>在“<a href="ch5.md#%E6%A3%80%E6%B5%8B%E5%B9%B6%E5%8F%91%E5%86%99%E5%85%A5">检测并发写入</a>”中我们观察到，如果有两个操作A和B，则存在三种可能性：A发生在B之前，或B发生在A之前，或者A和B<strong>并发</strong>。这种<strong>此前发生（happened before）</strong>关系是因果关系的另一种表述：如果A在B前发生，那么意味着B可能已经知道了A，或者建立在A的基础上，或者依赖于A。如果A和B是<strong>并发</strong>的，那么它们之间并没有因果联系；换句话说，我们确信A和B不知道彼此。</li>
<li>在事务快照隔离的上下文中（“<a href="ch7.md#%E5%BF%AB%E7%85%A7%E9%9A%94%E7%A6%BB%E5%92%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB">快照隔离和可重复读</a>”），我们说事务是从一致性快照中读取的。但此语境中“一致”到底又是什么意思？这意味着<strong>与因果关系保持一致（consistent with causality）</strong>：如果快照包含答案，它也必须包含被回答的问题【48】。在某个时间点观察整个数据库，与因果关系保持一致意味着：因果上在该时间点之前发生的所有操作，其影响都是可见的，但因果上在该时间点之后发生的操作，其影响对观察者不可见。<strong>读偏差（read skew）</strong>意味着读取的数据处于违反因果关系的状态（不可重复读，如<a href="img/fig7-6">图7-6</a>所示）。</li>
<li>事务之间<strong>写偏差（write skew）</strong>的例子（参见“<a href="ch7.md#%E5%86%99%E5%81%8F%E5%B7%AE%E5%92%8C%E5%B9%BB%E8%B1%A1">写偏差和幻象</a>”）也说明了因果依赖：在<a href="img/fig7-8.png">图7-8</a>中，爱丽丝被允许离班，因为事务认为鲍勃仍在值班，反之亦然。在这种情况下，离班的动作因果依赖于对当前值班情况的观察。<a href="ch7.md#%E5%8F%AF%E5%BA%8F%E5%88%97%E5%8C%96%E7%9A%84%E5%BF%AB%E7%85%A7%E9%9A%94%E7%A6%BB%EF%BC%88SSI%EF%BC%89">可序列化的快照隔离</a>通过跟踪事务之间的因果依赖来检测写偏差。</li>
<li>在爱丽丝和鲍勃看球的例子中（<a href="img/fig9-1.png">图9-1</a>），在听到爱丽丝惊呼比赛结果后，鲍勃从服务器得到陈旧结果的事实违背了因果关系：爱丽丝的惊呼因果依赖于得分宣告，所以鲍勃应该也能在听到爱丽斯惊呼后查询到比分。相同的模式在“<a href="#%E8%B7%A8%E4%BF%A1%E9%81%93%E7%9A%84%E6%97%B6%E5%BA%8F%E4%BE%9D%E8%B5%96">跨信道的时序依赖</a>”一节中，以“图像大小调整服务”的伪装再次出现。</li>
</ul>

<p>因果关系对事件施加了一种<strong>顺序</strong>：因在果之前；消息发送在消息收取之前。而且就像现实生活中一样，一件事会导致另一件事：某个节点读取了一些数据然后写入一些结果，另一个节点读取其写入的内容，并依次写入一些其他内容，等等。这些因果依赖的操作链定义了系统中的因果顺序，即，什么在什么之前发生。</p>

<p>如果一个系统服从因果关系所规定的顺序，我们说它是<strong>因果一致（causally consistent）</strong>的。例如，快照隔离提供了因果一致性：当你从数据库中读取到一些数据时，你一定还能够看到其因果前驱（假设在此期间这些数据还没有被删除）。</p>

<h4 id="toc_66">因果顺序不是全序的</h4>

<p>​   <strong>全序（total order）</strong>允许任意两个元素进行比较，所以如果有两个元素，你总是可以说出哪个更大，哪个更小。例如，自然数集是全序的：给定两个自然数，比如说5和13，那么你可以告诉我，13大于5。</p>

<p>​   然而数学集合并不完全是全序的：<code>{a, b}</code> 比 <code>{b, c}</code> 更大吗？好吧，你没法真正比较它们，因为二者都不是对方的子集。我们说它们是<strong>无法比较（incomparable）</strong>的，因此数学集合是<strong>偏序（partially order）</strong>的：在某些情况下，可以说一个集合大于另一个（如果一个集合包含另一个集合的所有元素），但在其他情况下它们是无法比较的<sup id="fnref7"><a href="#fn7" rel="footnote">7</a></sup>。</p>

<p>​   全序和偏序之间的差异反映在不同的数据库一致性模型中：</p>

<p><strong><em>线性一致性</em></strong></p>

<p>​   在线性一致的系统中，操作是全序的：如果系统表现的就好像只有一个数据副本，并且所有操作都是原子性的，这意味着对任何两个操作，我们总是能判定哪个操作先发生。这个全序<a href="img/fig9-4.png">图9-4</a>中以时间线表示。</p>

<p><strong><em>因果性</em></strong></p>

<p>​   我们说过，如果两个操作都没有在彼此<strong>之前发生</strong>，那么这两个操作是并发的（参阅<a href="ch5.md#%E2%80%9C%E6%AD%A4%E5%89%8D%E5%8F%91%E7%94%9F%E2%80%9D%E7%9A%84%E5%85%B3%E7%B3%BB%E5%92%8C%E5%B9%B6%E5%8F%91">“此前发生”的关系和并发</a>）。换句话说，如果两个事件是因果相关的（一个发生在另一个事件之前），则它们之间是有序的，但如果它们是并发的，则它们之间的顺序是无法比较的。这意味着因果关系定义了一个偏序，而不是一个全序：一些操作相互之间是有顺序的，但有些则是无法比较的。</p>

<p>​   因此，根据这个定义，在线性一致的数据存储中是不存在并发操作的：必须有且仅有一条时间线，所有的操作都在这条时间线上，构成一个全序关系。可能有几个请求在等待处理，但是数据存储确保了每个请求都是在唯一时间线上的某个时间点自动处理的，不存在任何并发。</p>

<p>​   并发意味着时间线会分岔然后合并 —— 在这种情况下，不同分支上的操作是无法比较的（即并发操作）。在<a href="ch5.md">第五章</a>中我们看到了这种现象：例如，<a href="img/fig5-14.md">图5-14</a> 并不是一条直线的全序关系，而是一堆不同的操作并发进行。图中的箭头指明了因果依赖 —— 操作的偏序。</p>

<p>​   如果你熟悉像Git这样的分布式版本控制系统，那么其版本历史与因果关系图极其相似。通常，一个<strong>提交（Commit）</strong>发生在另一个提交之后，在一条直线上。但是有时你会遇到分支（当多个人同时在一个项目上工作时），<strong>合并（Merge）</strong>会在这些并发创建的提交相融合时创建。</p>

<h4 id="toc_67">线性一致性强于因果一致性</h4>

<p>​   那么因果顺序和线性一致性之间的关系是什么？答案是线性一致性<strong>隐含着（implies）</strong>因果关系：任何线性一致的系统都能正确保持因果性【7】。特别是，如果系统中有多个通信通道（如<a href="img/fig9-5.png">图9-5</a> 中的消息队列和文件存储服务），线性一致性可以自动保证因果性，系统无需任何特殊操作（如在不同组件间传递时间戳）。</p>

<p>​   线性一致性确保因果性的事实使线性一致系统变得简单易懂，更有吸引力。然而，正如“<a href="#%E7%BA%BF%E6%80%A7%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E4%BB%A3%E4%BB%B7">线性一致性的代价</a>”中所讨论的，使系统线性一致可能会损害其性能和可用性，尤其是在系统具有严重的网络延迟的情况下（例如，如果系统在地理上散布）。出于这个原因，一些分布式数据系统已经放弃了线性一致性，从而获得更好的性能，但它们用起来也更为困难。</p>

<p>​   好消息是存在折衷的可能性。线性一致性并不是保持因果性的唯一途径 —— 还有其他方法。一个系统可以是因果一致的，而无需承担线性一致带来的性能折损（尤其对于CAP定理不适用的情况）。实际上在所有的不会被网络延迟拖慢的一致性模型中，因果一致性是可行的最强的一致性模型。而且在网络故障时仍能保持可用【2,42】。</p>

<p>​   在许多情况下，看上去需要线性一致性的系统，实际上需要的只是因果一致性，因果一致性可以更高效地实现。基于这种观察结果，研究人员正在探索新型的数据库，既能保证因果一致性，且性能与可用性与最终一致的系统类似【49,50,51】。</p>

<p>​   这方面的研究相当新鲜，其中很多尚未应用到生产系统，仍然有不少挑战需要克服【52,53】。但对于未来的系统而言，这是一个有前景的方向。</p>

<h4 id="toc_68">捕获因果关系</h4>

<p>​   我们不会在这里讨论非线性一致的系统如何保证因果性的细节，而只是简要地探讨一些关键的思想。</p>

<p>​   为了维持因果性，你需要知道哪个操作发生在哪个其他操作之前（<strong>happened before</strong>）。这是一个偏序：并发操作可以以任意顺序进行，但如果一个操作发生在另一个操作之前，那它们必须在所有副本上以那个顺序被处理。因此，当一个副本处理一个操作时，它必须确保所有因果前驱的操作（之前发生的所有操作）已经被处理；如果前面的某个操作丢失了，后面的操作必须等待，直到前面的操作被处理完毕。</p>

<p>​   为了确定因果依赖，我们需要一些方法来描述系统中节点的“知识”。如果节点在发出写入Y 的请求时已经看到了 X的值，则 X 和 Y 可能存在因果关系。这个分析使用了那些在欺诈指控刑事调查中常见的问题：CEO在做出决定 Y 时是否<strong>知道</strong> X ？</p>

<p>​   用于确定<em>哪些操作发生在其他操作之前</em> 的技术，与我们在“<a href="ch5.md#%E6%A3%80%E6%B5%8B%E5%B9%B6%E5%8F%91%E5%86%99%E5%85%A5">检测并发写入</a>”中所讨论的内容类似。那一节讨论了无领导者数据存储中的因果性：为了防止丢失更新，我们需要检测到对同一个键的并发写入。因果一致性则更进一步：它需要跟踪整个数据库中的因果依赖，而不仅仅是一个键。可以推广版本向量以解决此类问题【54】。</p>

<p>​   为了确定因果顺序，数据库需要知道应用读取了哪个版本的数据。这就是为什么在 <a href="img/fig5-13.png">图5-13 </a>中，来自先前操作的版本号在写入时被传回到数据库的原因。在SSI 的冲突检测中会出现类似的想法，如“<a href="">可序列化的快照隔离（SSI）</a>”中所述：当事务要提交时，数据库将检查它所读取的数据版本是否仍然是最新的。为此，数据库跟踪哪些数据被哪些事务所读取。</p>

<h3 id="toc_69">序列号顺序</h3>

<p>​   虽然因果是一个重要的理论概念，但实际上跟踪所有的因果关系是不切实际的。在许多应用中，客户端在写入内容之前会先读取大量数据，我们无法弄清写入因果依赖于先前全部的读取内容，还是仅包括其中一部分。显式跟踪所有已读数据意味着巨大的额外开销。</p>

<p>​   但还有一个更好的方法：我们可以使用<strong>序列号（sequence nunber）</strong>或<strong>时间戳（timestamp）</strong>来排序事件。时间戳不一定来自时钟（或物理时钟，存在许多问题，如 “<a href="ch8.md#%E4%B8%8D%E5%8F%AF%E9%9D%A0%E7%9A%84%E6%97%B6%E9%92%9F">不可靠时钟</a>” 中所述）。它可以来自一个<strong>逻辑时钟（logical clock）</strong>，这是一个用来生成标识操作的数字序列的算法，典型实现是使用一个每次操作自增的计数器。</p>

<p>​   这样的序列号或时间戳是紧凑的（只有几个字节大小），它提供了一个全序关系：也就是说每操作都有一个唯一的序列号，而且总是可以比较两个序列号，确定哪一个更大（即哪些操作后发生）。</p>

<p>​   特别是，我们可以使用<strong>与因果一致（consistent with causality）</strong>的全序来生成序列号<sup id="fnref8"><a href="#fn8" rel="footnote">8</a></sup>：我们保证，如果操作 A 因果后继于操作 B，那么在这个全序中 A 在 B 前（ A 具有比 B 更小的序列号）。并行操作之间可以任意排序。这样一个全序关系捕获了所有关于因果的信息，但也施加了一个比因果性要求更为严格的顺序。</p>

<p>​   在单主复制的数据库中（参见“<a href="ch5.md#%E9%A2%86%E5%AF%BC%E8%80%85%E4%B8%8E%E8%BF%BD%E9%9A%8F%E8%80%85">领导者与追随者</a>”），复制日志定义了与因果一致的写操作。主库可以简单地为每个操作自增一个计数器，从而为复制日志中的每个操作分配一个单调递增的序列号。如果一个从库按照它们在复制日志中出现的顺序来应用写操作，那么从库的状态始终是因果一致的（即使它落后于领导者）。</p>

<h4 id="toc_70">非因果序列号生成器</h4>

<p>​   如果主库不存在（可能因为使用了多主数据库或无主数据库，或者因为使用了分区的数据库），如何为操作生成序列号就没有那么明显了。在实践中有各种各样的方法：</p>

<ul>
<li>每个节点都可以生成自己独立的一组序列号。例如有两个节点，一个节点只能生成奇数，而另一个节点只能生成偶数。通常，可以在序列号的二进制表示中预留一些位，用于唯一的节点标识符，这样可以确保两个不同的节点永远不会生成相同的序列号。</li>
<li>可以将时钟（物理时钟）时间戳附加到每个操作上【55】。这种时间戳并不连续，但是如果它具有足够高的分辨率，那也许足以提供一个操作的全序关系。这一事实应用于 <em>最后写入胜利</em> 的冲突解决方法中（参阅“<a href="ch8.md#%E6%9C%89%E5%BA%8F%E4%BA%8B%E4%BB%B6%E7%9A%84%E6%97%B6%E9%97%B4%E6%88%B3">有序事件的时间戳</a>”）。</li>
<li>可以预先分配序列号区块。例如，节点 A 可能要求从序列号1到1,000区块的所有权，而节点 B 可能要求序列号1,001到2,000区块的所有权。然后每个节点可以独立分配所属区块中的序列号，并在序列号告急时请求分配一个新的区块。</li>
</ul>

<p>这三个选项都比单一主库的自增计数器表现要好，并且更具可扩展性。它们为每个操作生成一个唯一的，近似自增的序列号。然而它们都有同一个问题：生成的序列号与因果不一致。</p>

<p>因为这些序列号生成器不能正确地捕获跨节点的操作顺序，所以会出现因果关系的问题：</p>

<ul>
<li><p>每个节点每秒可以处理不同数量的操作。因此，如果一个节点产生偶数序列号而另一个产生奇数序列号，则偶数计数器可能落后于奇数计数器，反之亦然。如果你有一个奇数编号的操作和一个偶数编号的操作，你无法准确地说出哪一个操作在因果上先发生。</p></li>
<li><p>来自物理时钟的时间戳会受到时钟偏移的影响，这可能会使其与因果不一致。例如<a href="img/fig8-3.png">图8-3</a> 展示了一个例子，其中因果上晚发生的操作，却被分配了一个更早的时间戳。<sup id="fnref8"><a href="#fn8" rel="footnote">8</a></sup></p></li>
<li><p>在分配区块的情况下，某个操作可能会被赋予一个范围在1,001到2,000内的序列号，然而一个因果上更晚的操作可能被赋予一个范围在1到1,000之间的数字。这里序列号与因果关系也是不一致的。</p></li>
</ul>

<h4 id="toc_71">兰伯特时间戳</h4>

<p>​   尽管刚才描述的三个序列号生成器与因果不一致，但实际上有一个简单的方法来产生与因果关系一致的序列号。它被称为兰伯特时间戳，莱斯利·兰伯特（Leslie Lamport）于1978年提出【56】，现在是分布式系统领域中被引用最多的论文之一。</p>

<p>​   <a href="img/fig9-8.png">图9-8</a> 说明了兰伯特时间戳的应用。每个节点都有一个唯一标识符，和一个保存自己执行操作数量的计数器。 兰伯特时间戳就是两者的简单组合：（计数器，节点ID）\((counter, node ID)\)。两个节点有时可能具有相同的计数器值，但通过在时间戳中包含节点ID，每个时间戳都是唯一的。</p>

<p><img src="img/fig9-8.png" alt=""/></p>

<p><strong>图9-8  Lamport时间戳提供了与因果关系一致的总排序。</strong></p>

<p>​   兰伯特时间戳与物理时间时钟没有任何关系，但是它提供了一个全序：如果你有两个时间戳，则<strong>计数器</strong>值大者是更大的时间戳。如果计数器值相同，则节点ID越大的，时间戳越大。</p>

<p>​   迄今，这个描述与上节所述的奇偶计数器基本类似。使兰伯特时间戳因果一致的关键思想如下所示：每个节点和每个客户端跟踪迄今为止所见到的最大<strong>计数器</strong>值，并在每个请求中包含这个最大计数器值。当一个节点收到最大计数器值大于自身计数器值的请求或响应时，它立即将自己的计数器设置为这个最大值。</p>

<p>​   这如 <a href="img/fig9-8.png">图9-8</a> 所示，其中客户端 A 从节点2 接收计数器值 <code>5</code> ，然后将最大值 <code>5</code> 发送到节点1 。此时，节点1 的计数器仅为 <code>1</code> ，但是它立即前移至 <code>5</code> ，所以下一个操作的计数器的值为 <code>6</code> 。</p>

<p>​   只要每一个操作都携带着最大计数器值，这个方案确保兰伯特时间戳的排序与因果一致，因为每个因果依赖都会导致时间戳增长。</p>

<p>​   兰伯特时间戳有时会与我们在 “<a href="ch5.md#%E6%A3%80%E6%B5%8B%E5%B9%B6%E5%8F%91%E5%86%99%E5%85%A5">检测并发写入</a>” 中看到的版本向量相混淆。虽然两者有一些相似之处，但它们有着不同的目的：版本向量可以区分两个操作是并发的，还是一个因果依赖另一个；而兰伯特时间戳总是施行一个全序。从兰伯特时间戳的全序中，你无法分辨两个操作是并发的还是因果依赖的。 兰伯特时间戳优于版本向量的地方是，它更加紧凑。</p>

<h4 id="toc_72">光有时间戳排序还不够</h4>

<p>​   虽然兰伯特时间戳定义了一个与因果一致的全序，但它还不足以解决分布式系统中的许多常见问题。</p>

<p>​   例如，考虑一个需要确保用户名能唯一标识用户帐户的系统。如果两个用户同时尝试使用相同的用户名创建帐户，则其中一个应该成功，另一个应该失败。 （我们之前在“<a href="ch8.md#%E9%A2%86%E5%AF%BC%E8%80%85%E4%B8%8E%E9%94%81%E5%AE%9A">领导者与锁定</a>”中提到过这个问题。）</p>

<p>​   乍看之下，似乎操作的全序关系足以解决这一问题（例如使用兰伯特时间戳）：如果创建了两个具有相同用户名的帐户，选择时间戳较小的那个作为胜者（第一个抓到用户名的人），并让带有更大时间戳者失败。由于时间戳上有全序关系，所以这个比较总是可行的。</p>

<p>​   这种方法适用于事后确定胜利者：一旦你收集了系统中的所有用户名创建操作，就可以比较它们的时间戳。然而当某个节点需要实时处理用户创建用户名的请求时，这样的方法就无法满足了。节点需要<strong>马上（right now）</strong>决定这个请求是成功还是失败。在那个时刻，节点并不知道是否存其他节点正在并发执行创建同样用户名的操作，罔论其它节点可能分配给那个操作的时间戳。</p>

<p>​   为了确保没有其他节点正在使用相同的用户名和较小的时间戳并发创建同名账户，你必须检查其它每个节点，看看它在做什么【56】。如果其中一个节点由于网络问题出现故障或不可达，则整个系统可能被拖至停机。这不是我们需要的那种容错系统。</p>

<p>​   这里的问题是，只有在所有的操作都被收集之后，操作的全序才会出现。如果另一个节点已经产生了一些操作，但你还不知道那些操作是什么，那就无法构造所有操作最终的全序关系：来自另一个节点的未知操作可能需要被插入到全序中的不同位置。</p>

<p>​   总之：为了实诸如如用户名上的唯一约束这种东西，仅有操作的全序是不够的，你还需要知道这个全序何时会尘埃落定。如果你有一个创建用户名的操作，并且确定在全序中，没有任何其他节点可以在你的操作之前插入对同一用户名的声称，那么你就可以安全地宣告操作执行成功。</p>

<p>​   如何确定全序关系已经尘埃落定，这将在<a href="#%E5%85%A8%E5%BA%8F%E5%B9%BF%E6%92%AD">全序广播</a>一节中详细说明。</p>

<h3 id="toc_73">全序广播</h3>

<p>​   如果你的程序只运行在单个CPU核上，那么定义一个操作全序是很容易的：可以简单地就是CPU执行这些操作的顺序。但是在分布式系统中，让所有节点对同一个全局操作顺序达成一致可能相当棘手。在上一节中，我们讨论了按时间戳或序列号进行排序，但发现它还不如单主复制给力（如果你使用时间戳排序来实现唯一性约束，而且不能容忍任何错误）。</p>

<p>​   如前所述，单主复制通过选择一个节点作为主库来确定操作的全序，并在主库的单个CPU核上对所有操作进行排序。接下来的挑战是，如果吞吐量超出单个主库的处理能力，这种情况下如何扩展系统；以及，如果主库失效（“<a href="#%E5%A4%84%E7%90%86%E8%8A%82%E7%82%B9%E5%AE%95%E6%9C%BA">处理节点宕机</a>”），如何处理故障切换。在分布式系统文献中，这个问题被称为<strong>全序广播（total order broadcast）</strong>或<strong>原子广播（atomic broadcast）</strong><sup id="fnref9"><a href="#fn9" rel="footnote">9</a></sup>【25,57,58】。</p>

<blockquote>
<h4 id="toc_74">顺序保证的范围</h4>

<p>每个分区各有一个主库的分区数据库，通常只在每个分区内维持顺序，这意味着它们不能提供跨分区的一致性保证（例如，一致性快照，外键引用）。 跨所有分区的全序是可能的，但需要额外的协调【59】。</p>
</blockquote>

<p>全序广播通常被描述为在节点间交换消息的协议。 非正式地讲，它要满足两个安全属性：</p>

<p><strong><em>可靠交付（reliable delivery）</em></strong></p>

<p>​   没有消息丢失：如果消息被传递到一个节点，它将被传递到所有节点。</p>

<p><strong><em>全序交付（totally ordered delivery）</em></strong></p>

<p>​   消息以相同的顺序传递给每个节点。</p>

<p>正确的全序广播算法必须始终保证可靠性和有序性，即使节点或网络出现故障。当然在网络中断的时候，消息是传不出去的，但是算法可以不断重试，以便在网络最终修复时，消息能及时通过并送达（当然它们必须仍然按照正确的顺序传递）。</p>

<h4 id="toc_75">使用全序广播</h4>

<p>​   像ZooKeeper和etcd这样的共识服务实际上实现了全序广播。这一事实暗示了全序广播与共识之间有着紧密联系，我们将在本章稍后进行探讨。</p>

<p>​   全序广播正是数据库复制所需的：如果每个消息都代表一次数据库的写入，且每个副本都按相同的顺序处理相同的写入，那么副本间将相互保持一致（除了临时的复制延迟）。这个原理被称为<strong>状态机复制（state machine replication）</strong>【60】，我们将在<a href="ch11.md">第11章</a>中重新回到这个概念。</p>

<p>​   与之类似，可以使用全序广播来实现可序列化的事务：如“<a href="ch7.md#%E7%9C%9F%E7%9A%84%E4%B8%B2%E8%A1%8C%E6%89%A7%E8%A1%8C">真的串行执行</a>”中所述，如果每个消息都表示一个确定性事务，以存储过程的形式来执行，且每个节点都以相同的顺序处理这些消息，那么数据库的分区和副本就可以相互保持一致【61】。</p>

<p>​   全序广播的一个重要表现是，顺序在消息送达时被固化：如果后续的消息已经送达，节点就不允许追溯地将（先前）消息插入顺序中的较早位置。这个事实使得全序广播比时间戳命令更强。</p>

<p>​   考量全序广播的另一种方式是，这是一种创建日志的方式（如在复制日志，事务日志或预写式日志中）：传递消息就像附加写入日志。由于所有节点必须以相同的顺序传递相同的消息，因此所有节点都可以读取日志，并看到相同的消息序列。</p>

<p>​   全序广播对于实现提供防护令牌的锁服务也很有用（参见“<a href="ch8.md#%E9%98%B2%E6%8A%A4%E4%BB%A4%E7%89%8C">防护令牌</a>”）。每个获取锁的请求都作为一条消息追加到日志末尾，并且所有的消息都按它们在日志中出现的顺序依次编号。序列号可以当成防护令牌用，因为它是单调递增的。在ZooKeeper中，这个序列号被称为<code>zxid</code> 【15】。</p>

<h4 id="toc_76">使用全序广播实现线性一致的存储</h4>

<p>​   如 <a href="img/fig9-4.png">图9-4</a> 所示，在线性一致的系统中，存在操作的全序。这是否意味着线性一致与全序广播一样？不尽然，但两者之间有者密切的联系<sup id="fnref10"><a href="#fn10" rel="footnote">10</a></sup>。</p>

<p>​   全序广播是异步的：消息被保证以固定的顺序可靠地传送，但是不能保证消息<strong>何时</strong>被送达（所以一个接收者可能落后于其他接收者）。相比之下，线性一致性是新鲜性的保证：读取一定能看见最新的写入值。</p>

<p>​   但如果有了全序广播，你就可以在此基础上构建线性一致的存储。例如，你可以确保用户名能唯一标识用户帐户。</p>

<p>​   设想对于每一个可能的用户名，你都可以有一个带有CAS原子操作的线性一致寄存器。每个寄存器最初的值为空值（表示不使用用户名）。当用户想要创建一个用户名时，对该用户名的寄存器执行CAS操作，在先前寄存器值为空的条件，将其值设置为用户的账号ID。如果多个用户试图同时获取相同的用户名，则只有一个CAS操作会成功，因为其他用户会看到非空的值（由于线性一致性）。</p>

<p>你可以通过将全序广播当成仅追加日志【62,63】的方式来实现这种线性一致的CAS操作：</p>

<ol>
<li>在日志中追加一条消息，试探性地指明你要声明的用户名。</li>
<li>读日志，并等待你所附加的信息被回送。<sup id="fnref11"><a href="#fn11" rel="footnote">11</a></sup></li>
<li>检查是否有任何消息声称目标用户名的所有权。如果这些消息中的第一条就你自己的消息，那么你就成功了：你可以提交声称的用户名（也许是通过向日志追加另一条消息）并向客户端确认。如果所需用户名的第一条消息来自其他用户，则中止操作。</li>
</ol>

<p>​   由于日志项是以相同顺序送达至所有节点，因此如果有多个并发写入，则所有节点会对最先到达者达成一致。选择冲突写入中的第一个作为胜利者，并中止后来者，以此确定所有节点对某个写入是提交还是中止达成一致。类似的方法可以在一个日志的基础上实现可序列化的多对象事务【62】。</p>

<p>​   尽管这一过程保证写入是线性一致的，但它并不保证读取也是线性一致的 —— 如果你从与日志异步更新的存储中读取数据，结果可能是陈旧的。 （精确地说，这里描述的过程提供了<strong>顺序一致性（sequential consistency）</strong>【47,64】，有时也称为<strong>时间线一致性（timeline consistency）</strong>【65,66】，比线性一致性稍微弱一些的保证）。为了使读取也线性一致，有几个选项：</p>

<ul>
<li>你可以通过追加一条消息，当消息回送时读取日志，执行实际的读取。消息在日志中的位置因此定义了读取发生的时间点。 （etcd的法定人数读取有些类似这种情况【16】。）</li>
<li>如果日志允许以线性一致的方式获取最新日志消息的位置，则可以查询该位置，等待直到该位置前的所有消息都传达到你，然后执行读取。 （这是Zookeeper <code>sync()</code> 操作背后的思想【15】）。</li>
<li>你可以从同步更新的副本中进行读取，因此可以确保结果是最新的。 （这种技术用于链式复制【63】；参阅“<a href="ch5.md#%E8%AE%BE%E7%BD%AE%E6%96%B0%E4%BB%8E%E5%BA%93">复制研究</a>”。）</li>
</ul>

<h4 id="toc_77">使用线性一致性存储实现全序广播</h4>

<p>​   上一节介绍了如何从全序广播构建一个线性一致的CAS操作。我们也可以把它反过来，假设我们有线性一致的存储，接下来会展示如何在此基础上构建全序广播。</p>

<p>​   最简单的方法是假设你有一个线性一致的寄存器来存储一个整数，并且有一个原子<strong>自增并返回</strong>操作【28】。或者原子CAS操作也可以完成这项工作。</p>

<p>​   该算法很简单：每个要通过全序广播发送的消息首先对线性一致寄存器执行<strong>自增并返回</strong>操作。然后将从寄存器获得的值作为序列号附加到消息中。然后你可以将消息发送到所有节点（重新发送任何丢失的消息），而收件人将按序列号连续发送消息。</p>

<p>​   请注意，与兰伯特时间戳不同，通过自增线性一致性寄存器获得的数字形式上是一个没有间隙的序列。因此，如果一个节点已经发送了消息 4 并且接收到序列号为 6 的传入消息，则它知道它在传递消息 6 之前必须等待消息 5 。兰伯特时间戳则与之不同 —— 事实上，这是全序广播和时间戳排序间的关键区别。</p>

<p>​   实现一个带有原子性<strong>自增并返回</strong>操作的线性一致寄存器有多困难？像往常一样，如果事情从来不出差错，那很容易：你可以简单地把它保存在单个节点内的变量中。问题在于处理当该节点的网络连接中断时的情况，并在该节点失效时能恢复这个值【59】。一般来说，如果你对线性一致性的序列号生成器进行深入过足够深入的思考，你不可避免地会得出一个共识算法。</p>

<p>​   这并非巧合：可以证明，线性一致的CAS（或自增并返回）寄存器与全序广播都都等价于<strong>共识</strong>问题【28,67】。也就是说，如果你能解决其中的一个问题，你可以把它转化成为其他问题的解决方案。这是相当深刻和令人惊讶的洞察！</p>

<p>现在是时候正面处理共识问题了，我们将在本章的其余部分进行讨论。</p>

<h2 id="toc_78">分布式事务与共识</h2>

<p>​   <strong>共识</strong>是分布式计算中最重要也是最基本的问题之一。从表面上看似乎很简单：非正式地讲，目标只是<strong>让几个节点达成一致（get serveral nodes to agree on something）</strong>。你也许会认为这不会太难。不幸的是，许多出故障的系统都是因为错误地轻信这个问题很容易解决。</p>

<p>​   尽管共识非常重要，但关于它的内容出现在本书的后半部分，因为这个主题非常微妙，欣赏细微之处需要一些必要的知识。即使在学术界，对共识的理解也是在几十年的过程中逐渐沉淀而来，一路上也有着许多误解。现在我们已经讨论了复制（<a href="ch5.md">第5章</a>），事务（<a href="ch7.md">第7章</a>），系统模型（<a href="ch8.md">第8章</a>），线性一致以及全序（<a href="ch9.md">本章</a>），我们终于准备好解决共识问题了。</p>

<p>节点能达成一致，在很多场景下都非常重要，例如：</p>

<p><strong><em>领导选举</em></strong></p>

<p>​   在单主复制的数据库中，所有节点需要就哪个节点是领导者达成一致。如果一些节点由于网络故障而无法与其他节点通信，则可能会对领导权的归属引起争议。在这种情况下，共识对于避免错误的故障切换非常重要。错误的故障切换会导致两个节点都认为自己是领导者（<strong>脑裂</strong>，参阅“<a href="ch5.md#%E5%A4%84%E7%90%86%E8%8A%82%E7%82%B9%E5%AE%95%E6%9C%BA">处理节点宕机</a>”）。如果有两个领导者，它们都会接受写入，它们的数据会发生分歧，从而导致不一致和数据丢失。</p>

<p><strong><em>原子提交</em></strong></p>

<p>​   在支持跨多节点或跨多分区事务的数据库中，一个事务可能在某些节点上失败，但在其他节点上成功。如果我们想要维护事务的原子性（就ACID而言，请参“<a href="ch7.md#%E5%8E%9F%E5%AD%90%E6%80%A7">原子性</a>”），我们必须让所有节点对事务的结果达成一致：要么全部中止/回滚（如果出现任何错误），要么它们全部提交（如果没有出错）。这个共识的例子被称为<strong>原子提交（atomic commit）</strong>问题<sup id="fnref12"><a href="#fn12" rel="footnote">12</a></sup>。</p>

<blockquote>
<h3 id="toc_79">共识的不可能性</h3>

<p>你可能已经听说过作者Fischer，Lynch和Paterson之后的FLP结果【68】，它证明，如果存在节点可能崩溃的风险，则不存在<strong>总是</strong>能够达成共识的算法。在分布式系统中，我们必须假设节点可能会崩溃，所以可靠的共识是不可能的。然而这里我们正在讨论达成共识的算法，到底是怎么回事？</p>

<p>答案是FLP结果在<strong>异步系统模型</strong>中得到了证明（参阅“<a href="ch8.md#%E7%B3%BB%E7%BB%9F%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%8E%B0%E5%AE%9E">系统模型与现实</a>”），这是一种限制性很强的模型，它假定确定性算法不能使用任何时钟或超时。如果允许算法使用<strong>超时</strong>或其他方法来识别可疑的崩溃节点（即使怀疑有时是错误的），则共识变为一个可解的问题【67】。即使仅仅允许算法使用随机数，也足以绕过这个不可能的结果【69】。</p>

<p>因此，FLP是关于共识不可能性的重要理论结果，但现实中的分布式系统通常是可以达成共识的。</p>
</blockquote>

<p>​   在本节中，我们将首先更详细地研究<strong>原子提交</strong>问题。具体来说，我们将讨论<strong>两阶段提交（2PC, two-phase commit）</strong>算法，这是解决原子提交问题最常见的办法，并在各种数据库、消息队列和应用服务器中实现。事实证明2PC是一种共识算法，但不是一个非常好的算法【70,71】。</p>

<p>​   通过对2PC的学习，我们将继续努力实现更好的一致性算法，比如ZooKeeper（Zab）和etcd（Raft）中使用的算法。</p>

<h3 id="toc_80">原子提交与二阶段提交（2PC）</h3>

<p>​   在<a href="ch7.md">第7章</a>中我们了解到，事务原子性的目的是在多次写操作中途出错的情况下，提供一种简单的语义。事务的结果要么是成功提交，在这种情况下，事务的所有写入都是持久化的；要么是中止，在这种情况下，事务的所有写入都被回滚（即撤消或丢弃）。</p>

<p>​   原子性可以防止失败的事务搅乱数据库，避免数据库陷入半成品结果和半更新状态。这对于多对象事务（参阅“<a href="ch7.md#%E5%8D%95%E5%AF%B9%E8%B1%A1%E5%92%8C%E5%A4%9A%E5%AF%B9%E8%B1%A1%E6%93%8D%E4%BD%9C">单对象和多对象操作</a>”）和维护次级索引的数据库尤其重要。每个辅助索引都是与主数据相分离的数据结构—— 因此，如果你修改了一些数据，则还需要在辅助索引中进行相应的更改。原子性确保二级索引与主数据保持一致（如果索引与主数据不一致，就没什么用了）。</p>

<h4 id="toc_81">从单节点到分布式原子提交</h4>

<p>​   对于在单个数据库节点执行的事务，原子性通常由存储引擎实现。当客户端请求数据库节点提交事务时，数据库将使事务的写入持久化（通常在预写式日志中：参阅“<a href="ch3.md#%E4%BD%BFB%E6%A0%91%E5%8F%AF%E9%9D%A0">使B树可靠</a>”），然后将提交记录追加到磁盘中的日志里。如果数据库在这个过程中间崩溃，当节点重启时，事务会从日志中恢复：如果提交记录在崩溃之前成功地写入磁盘，则认为事务被提交；否则来自该事务的任何写入都被回滚。</p>

<p>​   因此，在单个节点上，事务的提交主要取决于数据持久化落盘的<strong>顺序</strong>：首先是数据，然后是提交记录【72】。事务提交或终止的关键决定时刻是磁盘完成写入提交记录的时刻：在此之前，仍有可能中止（由于崩溃），但在此之后，事务已经提交（即使数据库崩溃）。因此，是单一的设备（连接到单个磁盘驱动的控制器，且挂载在单台机器上）使得提交具有原子性。</p>

<p>​   但是，如果一个事务中涉及多个节点呢？例如，你也许在分区数据库中会有一个多对象事务，或者是一个按关键词分区的二级索引（其中索引条目可能位于与主数据不同的节点上；参阅“<a href="ch6.md#%E5%88%86%E5%8C%BA%E5%92%8C%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95">分区和二级索引</a>”）。大多数“NoSQL”分布式数据存储不支持这种分布式事务，但是很多关系型数据库集群支持（参见“<a href="#%E5%AE%9E%E8%B7%B5%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1">实践中的分布式事务</a>”）。</p>

<p>​   在这些情况下，仅向所有节点发送提交请求并独立提交每个节点的事务是不够的。这样很容易发生违反原子性的情况：提交在某些节点上成功，而在其他节点上失败：</p>

<ul>
<li>某些节点可能会检测到约束冲突或冲突，因此需要中止，而其他节点则可以成功进行提交。</li>
<li>某些提交请求可能在网络中丢失，最终由于超时而中止，而其他提交请求则通过。</li>
<li>在提交记录完全写入之前，某些节点可能会崩溃，并在恢复时回滚，而其他节点则成功提交。</li>
</ul>

<p>如果某些节点提交了事务，但其他节点却放弃了这些事务，那么这些节点就会彼此不一致（如 <a href="img/fig7-3.png">图7-3</a> 所示）。而且一旦在某个节点上提交了一个事务，如果事后发现它在其它节点上被中止了，它是无法撤回的。出于这个原因，一旦确定事务中的所有其他节点也将提交，节点就必须进行提交。</p>

<p>​   事务提交必须是不可撤销的 —— 事务提交之后，你不能改变主意，并追溯性地中止事务。这个规则的原因是，一旦数据被提交，其结果就对其他事务可见，因此其他客户端可能会开始依赖这些数据。这个原则构成了<strong>读已提交</strong>隔离等级的基础，在“<a href="ch7.md#%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4">读已提交</a>”一节中讨论了这个问题。如果一个事务在提交后被允许中止，所有那些读取了<strong>已提交却又被追溯声明不存在数据</strong>的事务也必须回滚。</p>

<p>​   （提交事务的结果有可能通过事后执行另一个补偿事务来取消【73,74】，但从数据库的角度来看，这是一个单独的事务，因此任何关于跨事务正确性的保证都是应用自己的问题。）</p>

<h4 id="toc_82">两阶段提交简介</h4>

<p>​   <strong>两阶段提交（two-phase commit）</strong>是一种用于实现跨多个节点的原子事务提交的算法，即确保所有节点提交或所有节点中止。 它是分布式数据库中的经典算法【13,35,75】。 2PC在某些数据库内部使用，也以<strong>XA事务</strong>的形式对应用可用【76,77】（例如Java Transaction API支持）或以SOAP Web服务的<code>WS-AtomicTransaction</code> 形式提供给应用【78,79】。</p>

<p><a href="img/fig9-9">   图9-9</a>说明了2PC的基本流程。2PC中的提交/中止过程分为两个阶段（因此而得名），而不是单节点事务中的单个提交请求。</p>

<p><img src="img/fig9-9.png" alt=""/></p>

<p><strong>图9-9 两阶段提交（2PC）的成功执行</strong></p>

<blockquote>
<h4 id="toc_83">不要把2PC和2PL搞混了</h4>

<p>两阶段提交（2PC）和两阶段锁定（参阅“<a href="ch7.md#%E4%B8%A4%E9%98%B6%E6%AE%B5%E9%94%81%E5%AE%9A%EF%BC%882PL%EF%BC%89">两阶段锁定（2PL）</a>”）是两个完全不同的东西。 2PC在分布式数据库中提供原子提交，而2PL提供可序列化的隔离等级。为了避免混淆，最好把它们看作完全独立的概念，并忽略名称中不幸的相似性。</p>
</blockquote>

<p>​   2PC使用一个通常不会出现在单节点事务中的新组件：<strong>协调者（coordinator）</strong>（也称为<strong>事务管理器（transaction manager）</strong>）。协调者通常在请求事务的相同应用进程中以库的形式实现（例如，嵌入在Java EE容器中），但也可以是单独的进程或服务。这种协调者的例子包括Narayana，JOTM，BTM或MSDTC。</p>

<p>​   正常情况下，2PC事务以应用在多个数据库节点上读写数据开始。我们称这些数据库节点为<strong>参与者（participants）</strong>。当应用准备提交时，协调者开始阶段 1 ：它发送一个<strong>准备（prepare）</strong>请求到每个节点，询问它们是否能够提交。然后协调者会跟踪参与者的响应：</p>

<ul>
<li>如果所有参与者都回答“是”，表示它们已经准备好提交，那么协调者在阶段 2 发出<strong>提交（commit）</strong>请求，然后提交真正发生。</li>
<li>如果任意一个参与者回复了“否”，则协调者在阶段2 中向所有节点发送<strong>中止（abort）</strong>请求。</li>
</ul>

<p>这个过程有点像西方传统婚姻仪式：司仪分别询问新娘和新郎是否要结婚，通常是从两方都收到“我愿意”的答复。收到两者的回复后，司仪宣布这对情侣成为夫妻：事务就提交了，这一幸福事实会广播至所有的参与者中。如果新娘与新郎之一没有回复”我愿意“，婚礼就会中止【73】。</p>

<h4 id="toc_84">系统承诺</h4>

<p>​   这个简短的描述可能并没有说清楚为什么两阶段提交保证了原子性，而跨多个节点的一阶段提交却没有。在两阶段提交的情况下，准备请求和提交请求当然也可以轻易丢失。 2PC又有什么不同呢？</p>

<p>为了理解它的工作原理，我们必须更详细地分解这个过程：</p>

<ol>
<li>当应用想要启动一个分布式事务时，它向协调者请求一个事务ID。此事务ID是全局唯一的。</li>
<li>应用在每个参与者上启动单节点事务，并在单节点事务上捎带上这个全局事务ID。所有的读写都是在这些单节点事务中各自完成的。如果在这个阶段出现任何问题（例如，节点崩溃或请求超时），则协调者或任何参与者都可以中止。</li>
<li>当应用准备提交时，协调者向所有参与者发送一个<strong>准备</strong>请求，并打上全局事务ID的标记。如果任意一个请求失败或超时，则协调者向所有参与者发送针对该事务ID的中止请求。</li>
<li>参与者收到准备请求时，需要确保在任意情况下都的确可以提交事务。这包括将所有事务数据写入磁盘（出现故障，电源故障，或硬盘空间不足都不能是稍后拒绝提交的理由）以及检查是否存在任何冲突或违反约束。通过向协调者回答“是”，节点承诺，只要请求，这个事务一定可以不出差错地提交。换句话说，参与者放弃了中止事务的权利，但没有实际提交。</li>
<li>当协调者收到所有准备请求的答复时，会就提交或中止事务作出明确的决定（只有在所有参与者投赞成票的情况下才会提交）。协调者必须把这个决定写到磁盘上的事务日志中，如果它随后就崩溃，恢复后也能知道自己所做的决定。这被称为<strong>提交点（commit point）</strong>。</li>
<li>一旦协调者的决定落盘，提交或放弃请求会发送给所有参与者。如果这个请求失败或超时，协调者必须永远保持重试，直到成功为止。没有回头路：如果已经做出决定，不管需要多少次重试它都必须被执行。如果参与者在此期间崩溃，事务将在其恢复后提交——由于参与者投了赞成，因此恢复后它不能拒绝提交。</li>
</ol>

<p>因此，该协议包含两个关键的“不归路”点：当参与者投票“是”时，它承诺它稍后肯定能够提交（尽管协调者可能仍然选择放弃）。一旦协调者做出决定，这一决定是不可撤销的。这些承诺保证了2PC的原子性。 （单节点原子提交将这两个事件混为一谈：将提交记录写入事务日志。）</p>

<p>​   回到婚姻的比喻，在说“我是”之前，你和你的新娘/新郎有中止这个事务的自由，通过回复 “没门！”（或者有类似效果的话）。然而在说了“我愿意”之后，你就不能撤回那个声明了。如果你说“我愿意”后晕倒了，没有听到司仪说“你们现在是夫妻了”，那也并不会改变事务已经提交的现实。当你稍后恢复意识时，可以通过查询司仪的全局事务ID状态来确定你是否已经成婚，或者你可以等待司仪重试下一次提交请求（因为重试将在你无意识期间一直持续）。</p>

<h4 id="toc_85">协调者失效</h4>

<p>​   我们已经讨论了在2PC期间，如果参与者之一或网络发生故障时会发生什么情况：如果任何一个<strong>准备</strong>请求失败或者超时，协调者就会中止事务。如果任何提交或中止请求失败，协调者将无条件重试。但是如果协调者崩溃，会发生什么情况就不太清楚了。</p>

<p>​   如果协调者在发送<strong>准备</strong>请求之前失败，参与者可以安全地中止事务。但是，一旦参与者收到了准备请求并投了“是”，就不能再单方面放弃 —— 必须等待协调者回答事务是否已经提交或中止。如果此时协调者崩溃或网络出现故障，参与者什么也做不了只能等待。参与者的这种事务状态称为<strong>存疑（in doubt）</strong>的或<strong>不确定（uncertain）</strong>的。</p>

<p>​   情况如<a href="img/fig9-10">图9-10</a> 所示。在这个特定的例子中，协调者实际上决定提交，数据库2 收到提交请求。但是，协调者在将提交请求发送到数据库1 之前发生崩溃，因此数据库1 不知道是否提交或中止。即使<strong>超时</strong>在这里也没有帮助：如果数据库1 在超时后单方面中止，它将最终与执行提交的数据库2 不一致。同样，单方面提交也是不安全的，因为另一个参与者可能已经中止了。</p>

<p><img src="img/fig9-10.png" alt=""/><br/>
 <strong>图9-10 参与者投赞成票后，协调者崩溃。数据库1不知道是否提交或中止</strong></p>

<p>​   没有协调者的消息，参与者无法知道是提交还是放弃。原则上参与者可以相互沟通，找出每个参与者是如何投票的，并达成一致，但这不是2PC协议的一部分。</p>

<p>​   可以完成2PC的唯一方法是等待协调者恢复。这就是为什么协调者必须在向参与者发送提交或中止请求之前，将其提交或中止决定写入磁盘上的事务日志：协调者恢复后，通过读取其事务日志来确定所有存疑事务的状态。任何在协调者日志中没有提交记录的事务都会中止。因此，2PC的<strong>提交点</strong>归结为协调者上的常规单节点原子提交。</p>

<h4 id="toc_86">三阶段提交</h4>

<p>​   两阶段提交被称为<strong>阻塞（blocking）</strong>原子提交协议，因为存在2PC可能卡住并等待协调者恢复的情况。理论上，可以使一个原子提交协议变为<strong>非阻塞（nonblocking）</strong>的，以便在节点失败时不会卡住。但是让这个协议能在实践中工作并没有那么简单。</p>

<p>​   作为2PC的替代方案，已经提出了一种称为<strong>三阶段提交（3PC）</strong>的算法【13,80】。然而，3PC假定网络延迟有界，节点响应时间有限；在大多数具有无限网络延迟和进程暂停的实际系统中（见<a href="ch8.md">第8章</a>），它并不能保证原子性。</p>

<p>​   通常，非阻塞原子提交需要一个<strong>完美的故障检测器（perfect failure detector）</strong>【67,71】—— 即一个可靠的机制来判断一个节点是否已经崩溃。在具有无限延迟的网络中，超时并不是一种可靠的故障检测机制，因为即使没有节点崩溃，请求也可能由于网络问题而超时。出于这个原因，2PC仍然被使用，尽管大家都清楚可能存在协调者故障的问题。</p>

<h3 id="toc_87">实践中的分布式事务</h3>

<p>​   分布式事务的名声毁誉参半，尤其是那些通过两阶段提交实现的。一方面，它被视作提供了一个难以实现的重要的安全性保证；另一方面，它们因为导致运维问题，造成性能下降，做出超过能力范围的承诺而饱受批评【81,82,83,84】。许多云服务由于其导致的运维问题，而选择不实现分布式事务【85,86】。</p>

<p>​   分布式事务的某些实现会带来严重的性能损失 —— 例如据报告称，MySQL中的分布式事务比单节点事务慢10倍以上【87】，所以当人们建议不要使用它们时就不足为奇了。两阶段提交所固有的性能成本，大部分是由于崩溃恢复所需的额外强制刷盘（<code>fsync</code>）【88】以及额外的网络往返。</p>

<p>​   但我们不应该直接忽视分布式事务，而应当更加仔细地审视这些事务，因为从中可以汲取重要的经验教训。首先，我们应该精确地说明“<strong>分布式事务</strong>”的含义。两种截然不同的分布式事务类型经常被混淆：</p>

<p><strong><em>数据库内部的分布式事务</em></strong></p>

<p>​   一些分布式数据库（即在其标准配置中使用复制和分区的数据库）支持数据库节点之间的内部事务。例如，VoltDB和MySQL Cluster的NDB存储引擎就有这样的内部事务支持。在这种情况下，所有参与事务的节点都运行相同的数据库软件。</p>

<p><strong><em>异构分布式事务</em></strong></p>

<p>​   在<strong>异构（heterogeneous）</strong>事务中，参与者是两种或以上不同技术：例如来自不同供应商的两个数据库，甚至是非数据库系统（如消息代理）。跨系统的分布式事务必须确保原子提交，尽管系统可能完全不同。</p>

<p>​   数据库内部事务不必与任何其他系统兼容，因此它们可以使用任何协议，并能针对特定技术进行特定的优化。因此数据库内部的分布式事务通常工作地很好。另一方面，跨异构技术的事务则更有挑战性。</p>

<h4 id="toc_88">恰好一次的消息处理</h4>

<p>​   异构的分布式事务处理能够以强大的方式集成不同的系统。例如：消息队列中的一条消息可以被确认为已处理，当且仅当用于处理消息的数据库事务成功提交。这是通过在同一个事务中原子提交<strong>消息确认</strong>和<strong>数据库写入</strong>两个操作来实现的。藉由分布式事务的支持，即使消息代理和数据库是在不同机器上运行的两种不相关的技术，这种操作也是可能的。</p>

<p>​   如果消息传递或数据库事务任意一者失败，两者都会中止，因此消息代理可能会在稍后安全地重传消息。因此，通过原子提交<strong>消息处理及其副作用</strong>，即使在成功之前需要几次重试，也可以确保消息被<strong>有效地（effectively）</strong>恰好处理一次。中止会抛弃部分完成事务所导致的任何副作用。</p>

<p>​   然而，只有当所有受事务影响的系统都使用同样的<strong>原子提交协议（atomic commit protocl）</strong>时，这样的分布式事务才是可能的。例如，假设处理消息的副作用是发送一封邮件，而邮件服务器并不支持两阶段提交：如果消息处理失败并重试，则可能会发送两次或更多次的邮件。但如果处理消息的所有副作用都可以在事务中止时回滚，那么这样的处理流程就可以安全地重试，就好像什么都没有发生过一样。</p>

<p>​   在<a href="ch11.md">第11章</a>中将再次回到”恰好一次“消息处理的主题。让我们先来看看允许这种异构分布式事务的原子提交协议。</p>

<h4 id="toc_89">XA事务</h4>

<p>​   <em>X/Open XA</em>（<strong>扩展架构（eXtended Architecture）</strong>的缩写）是跨异构技术实现两阶段提交的标准【76,77】。它于1991年推出并得到了广泛的实现：许多传统关系数据库（包括PostgreSQL，MySQL，DB2，SQL Server和Oracle）和消息代理（包括ActiveMQ，HornetQ，MSMQ和IBM MQ） 都支持XA。</p>

<p>​   XA不是一个网络协议——它只是一个用来与事务协调者连接的C API。其他语言也有这种API的绑定；例如在Java EE应用的世界中，XA事务是使用<strong>Java事务API（JTA, Java Transaction API）</strong>实现的，而许多使用<strong>Java数据库连接（JDBC, Java Database Connectivity）</strong>的数据库驱动，以及许多使用<strong>Java消息服务（JMS）</strong>API的消息代理都支持<strong>Java事务API（JTA）</strong>。</p>

<p>​   XA假定你的应用使用网络驱动或客户端库来与<strong>参与者</strong>进行通信（数据库或消息服务）。如果驱动支持XA，则意味着它会调用XA API 以查明操作是否为分布式事务的一部分 —— 如果是，则将必要的信息发往数据库服务器。驱动还会向协调者暴露回调接口，协调者可以通过回调来要求参与者准备，提交或中止。</p>

<p>​   事务协调者需要实现XA API。标准没有指明应该如何实现，但实际上协调者通常只是一个库，被加载到发起事务的应用的同一个进程中（而不是单独的服务）。它在事务中个跟踪所有的参与者，并在要求它们<strong>准备</strong>之后收集参与者的响应（通过驱动回调），并使用本地磁盘上的日志记录每次事务的决定（提交/中止）。</p>

<p>​   如果应用进程崩溃，或者运行应用的机器报销了，协调者也随之往生极乐。然后任何带有<strong>准备了</strong>但未提交事务的参与者都会在疑虑中卡死。由于协调程序的日志位于应用服务器的本地磁盘上，因此必须重启该服务器，且协调程序库必须读取日志以恢复每个事务的提交/中止结果。只有这样，协调者才能使用数据库驱动的XA回调来要求参与者提交或中止。数据库服务器不能直接联系协调者，因为所有通信都必须通过客户端库。</p>

<h4 id="toc_90">怀疑时持有锁</h4>

<p>​   为什么我们这么关心存疑事务？系统的其他部分就不能继续正常工作，无视那些终将被清理的存疑事务吗？</p>

<p>​   问题在于<strong>锁（locking）</strong>。正如在“<a href="ch7.md#%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4">读已提交</a>”中所讨论的那样，数据库事务通常获取待修改的行上的<strong>行级排他锁</strong>，以防止脏写。此外，如果要使用可序列化的隔离等级，则使用两阶段锁定的数据库也必须为事务所读取的行加上共享锁（参见“<a href="ch7.md#%E4%B8%A4%E9%98%B6%E6%AE%B5%E9%94%81%E5%AE%9A%EF%BC%882PL%EF%BC%89">两阶段锁定（2PL）</a>”）。</p>

<p>​   在事务提交或中止之前，数据库不能释放这些锁（如<a href="img/fig9-9.png">图9-9</a>中的阴影区域所示）。因此，在使用两阶段提交时，事务必须在整个存疑期间持有这些锁。如果协调者已经崩溃，需要20分钟才能重启，那么这些锁将会被持有20分钟。如果协调者的日志由于某种原因彻底丢失，这些锁将被永久持有 —— 或至少在管理员手动解决该情况之前。</p>

<p>​   当这些锁被持有时，其他事务不能修改这些行。根据数据库的不同，其他事务甚至可能因为读取这些行而被阻塞。因此，其他事务没法儿简单地继续它们的业务了 —— 如果它们要访问同样的数据，就会被阻塞。这可能会导致应用大面积进入不可用状态，直到存疑事务被解决。</p>

<h4 id="toc_91">从协调者故障中恢复</h4>

<p>​   理论上，如果协调者崩溃并重新启动，它应该干净地从日志中恢复其状态，并解决任何存疑事务。然而在实践中，<strong>孤立（orphaned）</strong>的存疑事务确实会出现【89,90】，即无论出于何种理由，协调者无法确定事务的结果（例如事务日志已经由于软件错误丢失或损坏）。这些事务无法自动解决，所以它们永远待在数据库中，持有锁并阻塞其他事务。</p>

<p>​   即使重启数据库服务器也无法解决这个问题，因为在2PC的正确实现中，即使重启也必须保留存疑事务的锁（否则就会冒有违反原子性保证的风险）。这是一种棘手的情况。</p>

<p>​   唯一的出路是让管理员手动决定提交还是回滚事务。管理员必须检查每个存疑事务的参与者，确定是否有任何参与者已经提交或中止，然后将相同的结果应用于其他参与者。解决这个问题潜在地需要大量的人力，并且可能发生在严重的生产中断期间（不然为什么协调者处于这种糟糕的状态），并很可能要在巨大精神压力和时间压力下完成。</p>

<p>​   许多XA的实现都有一个叫做<strong>启发式决策（heuristic decistions）</strong>的紧急逃生舱口：允许参与者单方面决定放弃或提交一个存疑事务，而无需协调者做出最终决定【76,77,91】。要清楚的是，这里<strong>启发式</strong>是<strong>可能破坏原子性（probably breaking atomicity）</strong>的委婉说法，因为它违背了两阶段提交的系统承诺。因此，启发式决策只是为了逃出灾难性的情况而准备的，而不是为了日常使用的。</p>

<h4 id="toc_92">分布式事务的限制</h4>

<p>​   XA事务解决了保持多个参与者（数据系统）相互一致的现实的重要问题，但正如我们所看到的那样，它也引入了严重的运维问题。特别来讲，这里的核心认识是：事务协调者本身就是一种数据库（存储了事务的结果），因此需要像其他重要数据库一样小心地打交道：</p>

<ul>
<li>如果协调者没有复制，而是只在单台机器上运行，那么它是整个系统的失效单点（因为它的失效会导致其他应用服务器阻塞在存疑事务持有的锁上）。令人惊讶的是，许多协调者实现默认情况下并不是高可用的，或者只有基本的复制支持。</li>
<li>许多服务器端应用都是使用无状态模式开发的（受HTTP的青睐），所有持久状态都存储在数据库中，因此具有应用服务器可随意按需添加删除的优点。但是，当协调者成为应用服务器的一部分时，它会改变部署的性质。突然间，协调者的日志成为持久系统状态的关键部分—— 与数据库本身一样重要，因为协调者日志是为了在崩溃后恢复存疑事务所必需的。这样的应用服务器不再是无状态的了。</li>
<li>由于XA需要兼容各种数据系统，因此它必须是所有系统的最小公分母。例如，它不能检测不同系统间的死锁（因为这将需要一个标准协议来让系统交换每个事务正在等待的锁的信息），而且它无法与<a href="ch7.md#%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%BF%AB%E7%85%A7%E9%9A%94%E7%A6%BB%EF%BC%88SSI%EF%BC%89">SSI </a>协同工作，因为这需要一个跨系统定位冲突的协议。</li>
<li>对于数据库内部的分布式事务（不是XA），限制没有这么大，例如，分布式版本的SSI 是可能的。然而仍然存在问题：2PC成功提交一个事务需要所有参与者的响应。因此，如果系统的<strong>任何</strong>部分损坏，事务也会失败。因此，分布式事务又有<strong>扩大失效（amplifying failures）</strong>的趋势，这又与我们构建容错系统的目标背道而驰。</li>
</ul>

<p>这些事实是否意味着我们应该放弃保持几个系统相互一致的所有希望？不完全是 —— 还有其他的办法，可以让我们在没有异构分布式事务的痛苦的情况下实现同样的事情。我们将在<a href="ch11.md">第11章</a> 和<a href="ch12.md">第12章</a> 回到这些章节。但首先，我们应该概括一下关于<strong>共识</strong>的话题。</p>

<h3 id="toc_93">容错共识</h3>

<p>​   非正式地，共识意味着让几个节点就某事达成一致。例如，如果有几个人<strong>同时（concurrently）</strong>尝试预订飞机上的最后一个座位，或剧院中的同一个座位，或者尝试使用相同的用户名注册一个帐户。共识算法可以用来确定这些<strong>互不相容（mutually incompatible）</strong>的操作中，哪一个才是赢家。</p>

<p>​   共识问题通常形式化如下：一个或多个节点可以<strong>提议（propose）</strong>某些值，而共识算法<strong>决定（decides）</strong>采用其中的某个值。在座位预订的例子中，当几个顾客同时试图订购最后一个座位时，处理顾客请求的每个节点可以<strong>提议</strong>正在服务的顾客的ID，而<strong>决定</strong>指明了哪个顾客获得了座位。</p>

<p>在这种形式下，共识算法必须满足以下性质【25】：<sup id="fnref13"><a href="#fn13" rel="footnote">13</a></sup></p>

<p><strong><em>一致同意（Uniform agreement）</em></strong></p>

<p>​   没有两个节点的决定不同。</p>

<p><strong><em>完整性（Integrity）</em></strong></p>

<p>​   没有节点决定两次。</p>

<p><strong><em>有效性（Validity）</em></strong></p>

<p>​   如果一个节点决定了值 <code>v</code> ，则 <code>v</code> 由某个节点所提议。</p>

<p><strong><em>终止（Termination）</em></strong><br/>
    由所有未崩溃的节点来最终决定值。</p>

<p><strong>一致同意</strong>和<strong>完整性</strong>属性定义了共识的核心思想：所有人都决定了相同的结果，一旦决定了，你就不能改变主意。<strong>有效性</strong>属性主要是为了排除平凡的解决方案：例如，无论提议了什么值，你都可以有一个始终决定值为<code>null</code>的算法。；该算法满足<strong>一致同意</strong>和<strong>完整性</strong>属性，但不满足<strong>有效性</strong>属性。</p>

<p>​   如果你不关心容错，那么满足前三个属性很容易：你可以将一个节点硬编码为“独裁者”，并让该节点做出所有的决定。但如果该节点失效，那么系统就无法再做出任何决定。事实上，这就是我们在两阶段提交的情况中所看到的：如果协调者失效，那么存疑的参与者就无法决定提交还是中止。</p>

<p>​   <strong>终止</strong>属性正式形成了容错的思想。它实质上说的是，一个共识算法不能简单地永远闲坐着等死 —— 换句话说，它必须取得进展。即使部分节点出现故障，其他节点也必须达成一项决定。 （<strong>终止</strong>是一种<strong>活性属性</strong>，而另外三种是安全属性 —— 参见“<a href="ch8.md#%E5%AE%89%E5%85%A8%E6%80%A7%E5%92%8C%E6%B4%BB%E6%80%A7">安全性和活性</a>”。）</p>

<p>​   共识的系统模型假设，当一个节点“崩溃”时，它会突然消失而且永远不会回来。（不像软件崩溃，想象一下地震，包含你的节点的数据中心被山体滑坡所摧毁，你必须假设节点被埋在30英尺以下的泥土中，并且永远不会重新上线）在这个系统模型中，任何需要等待节点恢复的算法都不能满足<strong>终止</strong>属性。特别是，2PC不符合终止属性的要求。</p>

<p>​   当然如果<strong>所有</strong>的节点都崩溃了，没有一个在运行，那么所有算法都不可能决定任何事情。算法可以容忍的失效数量是有限的：事实上可以证明，任何共识算法都需要至少占总体<strong>多数（majority）</strong>的节点正确工作，以确保终止属性【67】。多数可以安全地组成法定人数（参阅“<a href="ch5.md#%E8%AF%BB%E5%86%99%E7%9A%84%E6%B3%95%E5%AE%9A%E4%BA%BA%E6%95%B0">读写的法定人数</a>”）。</p>

<p>​   因此<strong>终止</strong>属性取决于一个假设，<strong>不超过一半的节点崩溃或不可达</strong>。然而即使多数节点出现故障或存在严重的网络问题，绝大多数共识的实现都能始终确保安全属性得到满足—— 一致同意，完整性和有效性【92】。因此，大规模的中断可能会阻止系统处理请求，但是它不能通过使系统做出无效的决定来破坏共识系统。</p>

<p>​   大多数共识算法假设不存在<strong>拜占庭式错误</strong>，正如在“<a href="#%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%BC%8F%E9%94%99%E8%AF%AF">拜占庭式错误</a>”一节中所讨论的那样。也就是说，如果一个节点没有正确地遵循协议（例如，如果它向不同节点发送矛盾的消息），它就可能会破坏协议的安全属性。克服拜占庭故障，稳健地达成共识是可能的，只要少于三分之一的节点存在拜占庭故障【25,93】。但我们没有地方在本书中详细讨论这些算法了。</p>

<h4 id="toc_94">共识算法和全序广播</h4>

<p>​   最著名的容错共识算法是<strong>视图戳复制（VSR, viewstamped replication）</strong>【94,95】，Paxos 【96,97,98,99】，Raft 【22,100,101】以及 Zab 【15,21,102】 。这些算法之间有不少相似之处，但它们并不相同【103】。在本书中我们不会介绍各种算法的详细细节：了解一些它们共通的高级思想通常已经足够了，除非你准备自己实现一个共识系统。（可能并不明智，相当难【98,104】）</p>

<p>​   大多数这些算法实际上并不直接使用这里描述的形式化模型（提议与决定单个值，一致同意，完整性，有效性和终止属性）。取而代之的是，它们决定了值的<strong>顺序（sequence）</strong>，这使它们成为全序广播算法，正如本章前面所讨论的那样（参阅“<a href="#%E5%85%A8%E5%BA%8F%E5%B9%BF%E6%92%AD">全序广播</a>”）。</p>

<p>​   请记住，全序广播要求将消息按照相同的顺序，恰好传递一次，准确传送到所有节点。如果仔细思考，这相当于进行了几轮共识：在每一轮中，节点提议下一条要发送的消息，然后决定在全序中下一条要发送的消息【67】。</p>

<p>所以，全序广播相当于重复进行多轮共识（每次共识决定与一次消息传递相对应）：</p>

<ul>
<li><p>由于<strong>一致同意</strong>属性，所有节点决定以相同的顺序传递相同的消息。</p></li>
<li><p>由于<strong>完整性</strong>属性，消息不会重复。</p></li>
<li><p>由于<strong>有效性</strong>属性，消息不会被损坏，也不能凭空编造。</p></li>
<li><p>由于<strong>终止</strong>属性，消息不会丢失。</p></li>
</ul>

<p>视图戳复制，Raft和Zab直接实现了全序广播，因为这样做比重复<strong>一次一值（one value a time）</strong>的共识更高效。在Paxos的情况下，这种优化被称为Multi-Paxos。</p>

<h4 id="toc_95">单领导者复制和共识</h4>

<p>​   在<a href="ch5.md">第5章</a>中，我们讨论了单领导者复制（参见“<a href="ch5.md#%E9%A2%86%E5%AF%BC%E8%80%85%E5%92%8C%E8%BF%BD%E9%9A%8F%E8%80%85">领导者和追随者</a>”），它将所有的写入操作都交给主库，并以相同的顺序将它们应用到从库，从而使副本保持在最新状态。这实际上不就是一个全序广播吗？为什么我们在<a href="ch5.md">第五章</a>里一点都没担心过共识问题呢？</p>

<p>​   答案取决于如何选择领导者。如果主库是由运维人员手动选择和配置的，那么你实际上拥有一种<strong>独裁类型</strong>的“共识算法”：只有一个节点被允许接受写入（即决定写入复制日志的顺序），如果该节点发生故障，则系统将无法写入，直到运维手动配置其他节点作为主库。这样的系统在实践中可以表现良好，但它无法满足共识的<strong>终止</strong>属性，因为它需要人为干预才能取得<strong>进展</strong>。</p>

<p>​   一些数据库会自动执行领导者选举和故障切换，如果旧主库失效，会提拔一个从库为新主库（参见“<a href="ch5.md#%E5%A4%84%E7%90%86%E8%8A%82%E7%82%B9%E5%AE%95%E6%9C%BA">处理节点宕机</a>”）。这使我们向容错的全序广播更进一步，从而达成共识。</p>

<p>​   但是还有一个问题。我们之前曾经讨论过脑裂的问题，并且说过所有的节点都需要同意是谁领导，否则两个不同的节点都会认为自己是领导者，从而导致数据库进入不一致的状态。因此，选出一位领导者需要共识。但如果这里描述的共识算法实际上是全序广播算法，并且全序广播就像单主复制，而单主复制需要一个领导者，那么...</p>

<p>​   这样看来，要选出一个领导者，我们首先需要一个领导者。要解决共识问题，我们首先需要解决共识问题。我们如何跳出这个先有鸡还是先有蛋的问题？</p>

<h4 id="toc_96">时代编号和法定人数</h4>

<p>​   迄今为止所讨论的所有共识协议，在内部都以某种形式使用一个领导者，但它们并不能保证领导者是独一无二的。相反，它们可以做出更弱的保证：协议定义了一个<strong>时代编号（epoch number）</strong>（在Paxos中称为<strong>投票编号（ballot number）</strong>，视图戳复制中的<strong>视图编号（view number）</strong>，以及Raft中的<strong>任期号码（term number）</strong>），并确保在每个时代中，领导者都是唯一的。</p>

<p>​   每次当现任领导被认为挂掉的时候，节点间就会开始一场投票，以选出一个新领导。这次选举被赋予一个递增的时代编号，因此时代编号是全序且单调递增的。如果两个不同的时代的领导者之间出现冲突（也许是因为前任领导者实际上并未死亡），那么带有更高时代编号的领导说了算。</p>

<p>​   在任何领导者被允许决定任何事情之前，必须先检查是否存在其他带有更高时代编号的领导者，它们可能会做出相互冲突的决定。领导者如何知道自己没有被另一个节点赶下台？回想一下在“<a href="ch8.md#%E7%9C%9F%E7%90%86%E5%9C%A8%E5%A4%9A%E6%95%B0%E4%BA%BA%E6%89%8B%E4%B8%AD">真理在多数人手中</a>”中提到的：一个节点不一定能相信自己的判断—— 因为只有节点自己认为自己是领导者，并不一定意味着其他节点接受它作为它们的领导者。</p>

<p>​   相反，它必须从<strong>法定人数（quorum）</strong>的节点中获取选票（参阅“<a href="ch5.md#%E8%AF%BB%E5%86%99%E7%9A%84%E6%B3%95%E5%AE%9A%E4%BA%BA%E6%95%B0">读写的法定人数</a>”）。对领导者想要做出的每一个决定，都必须将提议值发送给其他节点，并等待法定人数的节点响应并赞成提案。法定人数通常（但不总是）由多数节点组成【105】。只有在没有意识到任何带有更高时代编号的领导者的情况下，一个节点才会投票赞成提议。</p>

<p>​   因此，我们有两轮投票：第一次是为了选出一位领导者，第二次是对领导者的提议进行表决。关键的洞察在于，这两次投票的<strong>法定人群</strong>必须相互<strong>重叠（overlap）</strong>：如果一个提案的表决通过，则至少得有一个参与投票的节点也必须参加过最近的领导者选举【105】。因此，如果在一个提案的表决过程中没有出现更高的时代编号。那么现任领导者就可以得出这样的结论：没有发生过更高时代的领导选举，因此可以确定自己仍然在领导。然后它就可以安全地对提议值做出决定。</p>

<p>​   这一投票过程表面上看起来很像两阶段提交。最大的区别在于，2PC中协调者不是由选举产生的，而且2PC则要求<strong>所有</strong>参与者都投赞成票，而容错共识算法只需要多数节点的投票。而且，共识算法还定义了一个恢复过程，节点可以在选举出新的领导者之后进入一个一致的状态，确保始终能满足安全属性。这些区别正是共识算法正确性和容错性的关键。</p>

<h4 id="toc_97">共识的局限性</h4>

<p>​   共识算法对于分布式系统来说是一个巨大的突破：它为其他充满不确定性的系统带来了基础的安全属性（一致同意，完整性和有效性），然而它们还能保持容错（只要多数节点正常工作且可达，就能取得进展）。它们提供了全序广播，因此它们也可以以一种容错的方式实现线性一致的原子操作（参见“<a href="#%E4%BD%BF%E7%94%A8%E5%85%A8%E5%BA%8F%E5%B9%BF%E6%92%AD%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E4%B8%80%E8%87%B4%E6%80%A7%E5%AD%98%E5%82%A8">使用全序广播实现线性一致性存储</a>”）。</p>

<p>​   尽管如此，它们并不是在所有地方都用上了，因为好处总是有代价的。</p>

<p>​   节点在做出决定之前对提议进行投票的过程是一种同步复制。如“<a href="ch5.md#%E5%90%8C%E6%AD%A5%E4%B8%8E%E5%BC%82%E6%AD%A5%E5%A4%8D%E5%88%B6">同步与异步复制</a>”中所述，通常数据库会配置为异步复制模式。在这种配置中发生故障切换时，一些已经提交的数据可能会丢失 —— 但是为了获得更好的性能，许多人选择接受这种风险。</p>

<p>​   共识系统总是需要严格多数来运转。这意味着你至少需要三个节点才能容忍单节点故障（其余两个构成多数），或者至少有五个节点来容忍两个节点发生故障（其余三个构成多数）。如果网络故障切断了某些节点同其他节点的连接，则只有多数节点所在的网络可以继续工作，其余部分将被阻塞（参阅“<a href="#%E7%BA%BF%E6%80%A7%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E4%BB%A3%E4%BB%B7">线性一致性的代价</a>”）。</p>

<p>​   大多数共识算法假定参与投票的节点是固定的集合，这意味着你不能简单的在集群中添加或删除节点。共识算法的<strong>动态成员扩展（dynamic membership extension）</strong>允许集群中的节点集随时间推移而变化，但是它们比静态成员算法要难理解得多。</p>

<p>​   共识系统通常依靠超时来检测失效的节点。在网络延迟高度变化的环境中，特别是在地理上散布的系统中，经常发生一个节点由于暂时的网络问题，错误地认为领导者已经失效。虽然这种错误不会损害安全属性，但频繁的领导者选举会导致糟糕的性能表现，因系统最后可能花在权力倾扎上的时间要比花在建设性工作的多得多。</p>

<p>​   有时共识算法对网络问题特别敏感。例如Raft已被证明存在让人不悦的极端情况【106】：如果整个网络工作正常，但只有一条特定的网络连接一直不可靠，Raft可能会进入领导频繁二人转的局面，或者当前领导者不断被迫辞职以致系统实质上毫无进展。其他一致性算法也存在类似的问题，而设计能健壮应对不可靠网络的算法仍然是一个开放的研究问题。</p>

<h3 id="toc_98">成员与协调服务</h3>

<p>​   像ZooKeeper或etcd这样的项目通常被描述为“分布式键值存储”或“协调与配置服务”。这种服务的API看起来非常像数据库：你可以读写给定键的值，并遍历键。所以如果它们基本上算是数据库的话，为什么它们要把工夫全花在实现一个共识算法上呢？是什么使它们区别于其他任意类型的数据库？</p>

<p>​   为了理解这一点，简单了解如何使用ZooKeeper这类服务是很有帮助的。作为应用开发人员，你很少需要直接使用ZooKeeper，因为它实际上不适合当成通用数据库来用。更有可能的是，你会通过其他项目间接依赖它，例如HBase，Hadoop YARN，OpenStack Nova和Kafka都依赖ZooKeeper在后台运行。这些项目从它那里得到了什么？</p>

<p>​   ZooKeeper和etcd被设计为容纳少量完全可以放在内存中的数据（虽然它们仍然会写入磁盘以保证持久性），所以你不会想着把所有应用数据放到这里。这些少量数据会通过容错的全序广播算法复制到所有节点上。正如前面所讨论的那样，数据库复制需要的就是全序广播：如果每条消息代表对数据库的写入，则以相同的顺序应用相同的写入操作可以使副本之间保持一致。</p>

<p>​   ZooKeeper模仿了Google的Chubby锁服务【14,98】，不仅实现了全序广播（因此也实现了共识），而且还构建了一组有趣的其他特性，这些特性在构建分布式系统时变得特别有用：</p>

<p><strong><em>线性一致性的原子操作</em></strong></p>

<p>​   使用原子CAS操作可以实现锁：如果多个节点同时尝试执行相同的操作，只有一个节点会成功。共识协议保证了操作的原子性和线性一致性，即使节点发生故障或网络在任意时刻中断。分布式锁通常以<strong>租约（lease）</strong>的形式实现，租约有一个到期时间，以便在客户端失效的情况下最终能被释放（参阅“<a href="ch8.md#%E8%BF%9B%E7%A8%8B%E6%9A%82%E5%81%9C">进程暂停</a>”）。</p>

<p><strong><em>操作的全序排序</em></strong></p>

<p>如“<a href="ch8.md#%E9%A2%86%E5%AF%BC%E8%80%85%E4%B8%8E%E9%94%81%E5%AE%9A">领导者与锁定</a>”中所述，当某个资源受到锁或租约的保护时，你需要一个防护令牌来防止客户端在进程暂停的情况下彼此冲突。防护令牌是每次锁被获取时单调增加的数字。 ZooKeeper通过全局排序操作来提供这个功能，它为每个操作提供一个单调递增的事务ID（<code>zxid</code>）和版本号（<code>cversion</code>）【15】。</p>

<p><strong><em>失效检测</em></strong></p>

<p>​   客户端在ZooKeeper服务器上维护一个长期会话，客户端和服务器周期性地交换心跳包来检查节点是否还活着。即使连接暂时中断，或者ZooKeeper节点失效，会话仍保持在活跃状态。但如果心跳停止的持续时间超出会话超时，ZooKeeper会宣告该会话已死亡。当会话超时（ZooKeeper调用这些临时节点）时，会话持有的任何锁都可以配置为自动释放（ZooKeeper称之为<strong>临时节点（ephemeral nodes）</strong>）。</p>

<p><strong><em>变更通知</em></strong></p>

<p>​   客户端不仅可以读取其他客户端创建的锁和值，还可以监听它们的变更。因此，客户端可以知道另一个客户端何时加入集群（基于新客户端写入ZooKeeper的值），或发生故障（因其会话超时，而其临时节点消失）。通过订阅通知，客户端不用再通过频繁轮询的方式来找出变更。</p>

<p>​   在这些功能中，只有线性一致的原子操作才真的需要共识。但正是这些功能的组合，使得像ZooKeeper这样的系统在分布式协调中非常有用。</p>

<h4 id="toc_99">将工作分配给节点</h4>

<p>​   ZooKeeper/Chubby模型运行良好的一个例子是，如果你有几个进程实例或服务，需要选择其中一个实例作为主库或首选服务。如果领导者失败，其他节点之一应该接管。这对单主数据库当然非常实用，但对作业调度程序和类似的有状态系统也很好用。</p>

<p>​   另一个例子是，当你有一些分区资源（数据库，消息流，文件存储，分布式Actor系统等），并需要决定将哪个分区分配给哪个节点时。当新节点加入集群时，需要将某些分区从现有节点移动到新节点，以便重新平衡负载（参阅“<a href="ch6.md#%E9%87%8D%E6%96%B0%E5%B9%B3%E8%A1%A1%E5%88%86%E5%8C%BA">重新平衡分区</a>”）。当节点被移除或失效时，其他节点需要接管失效节点的工作。</p>

<p>​   这类任务可以通过在ZooKeeper中明智地使用原子操作，临时节点与通知来实现。如果设计得当，这种方法允许应用自动从故障中恢复而无需人工干预。不过这并不容易，尽管已经有不少在ZooKeeper客户端API基础之上提供更高层工具的库，例如Apache Curator 【17】。但它仍然要比尝试从头实现必要的共识算法要好得多，这样的尝试鲜有成功记录【107】。</p>

<p>​   应用最初只能在单个节点上运行，但最终可能会增长到数千个节点。试图在如此之多的节点上进行多数投票将是非常低效的。相反，ZooKeeper在固定数量的节点（通常是三到五个）上运行，并在这些节点之间执行其多数票，同时支持潜在的大量客户端。因此，ZooKeeper提供了一种将协调节点（共识，操作排序和故障检测）的一些工作“外包”到外部服务的方式。</p>

<p>​   通常，由ZooKeeper管理的数据的类型变化十分缓慢：代表“分区 7 中的节点运行在 <code>10.1.1.23</code> 上”的信息可能会在几分钟或几小时的时间内发生变化。它不是用来存储应用的运行时状态的，每秒可能会改变数千甚至数百万次。如果应用状态需要从一个节点复制到另一个节点，则可以使用其他工具（如Apache BookKeeper 【108】）。</p>

<h4 id="toc_100">服务发现</h4>

<p>​   ZooKeeper，etcd和Consul也经常用于服务发现——也就是找出你需要连接到哪个IP地址才能到达特定的服务。在云数据中心环境中，虚拟机连续来去常见，你通常不会事先知道服务的IP地址。相反，你可以配置你的服务，使其在启动时注册服务注册表中的网络端点，然后可以由其他服务找到它们。</p>

<p>​   但是，服务发现是否需要达成共识还不太清楚。 DNS是查找服务名称的IP地址的传统方式，它使用多层缓存来实现良好的性能和可用性。从DNS读取是绝对不线性一致性的，如果DNS查询的结果有点陈旧，通常不会有问题【109】。 DNS的可用性和对网络中断的鲁棒性更重要。</p>

<p>​   尽管服务发现并不需要共识，但领导者选举却是如此。因此，如果你的共识系统已经知道领导是谁，那么也可以使用这些信息来帮助其他服务发现领导是谁。为此，一些共识系统支持只读缓存副本。这些副本异步接收共识算法所有决策的日志，但不主动参与投票。因此，它们能够提供不需要线性一致性的读取请求。</p>

<h4 id="toc_101">成员服务</h4>

<p>​   ZooKeeper和它的小伙伴们可以看作是成员服务研究的悠久历史的一部分，这个历史可以追溯到20世纪80年代，并且对建立高度可靠的系统（例如空中交通管制）非常重要【110】。</p>

<p>​   成员资格服务确定哪些节点当前处于活动状态并且是群集的活动成员。正如我们在<a href="ch8.md">第8章</a>中看到的那样，由于无限的网络延迟，无法可靠地检测到另一个节点是否发生故障。但是，如果你通过一致的方式进行故障检测，那么节点可以就哪些节点应该被认为是存在或不存在达成一致。</p>

<p>​   即使它确实存在，仍然可能发生一个节点被共识错误地宣告死亡。但是对于一个系统来说，在哪些节点构成当前的成员关系方面是非常有用的。例如，选择领导者可能意味着简单地选择当前成员中编号最小的成员，但如果不同的节点对现有成员的成员有不同意见，则这种方法将不起作用。</p>

<h2 id="toc_102">本章小结</h2>

<p>​   在本章中，我们从几个不同的角度审视了关于一致性与共识的话题。我们深入研究了线性一致性（一种流行的一致性模型）：其目标是使多副本数据看起来好像只有一个副本一样，并使其上所有操作都原子性地生效。虽然线性一致性因为简单易懂而很吸引人 —— 它使数据库表现的好像单线程程序中的一个变量一样，但它有着速度缓慢的缺点，特别是在网络延迟很大的环境中。</p>

<p>​   我们还探讨了因果性，因果性对系统中的事件施加了顺序（什么发生在什么之前，基于因与果）。与线性一致不同，线性一致性将所有操作放在单一的全序时间线中，因果一致性为我们提供了一个较弱的一致性模型：某些事件可以是<strong>并发</strong>的，所以版本历史就像是一条不断分叉与合并的时间线。因果一致性没有线性一致性的协调开销，而且对网络问题的敏感性要低得多。</p>

<p>​   但即使捕获到因果顺序（例如使用兰伯特时间戳），我们发现有些事情也不能通过这种方式实现：在“<a href="#%E5%85%89%E6%9C%89%E6%97%B6%E9%97%B4%E6%88%B3%E6%8E%92%E5%BA%8F%E8%BF%98%E4%B8%8D%E5%A4%9F">光有时间戳排序还不够</a>”一节的例子中，我们需要确保用户名是唯一的，并拒绝同一用户名的其他并发注册。如果一个节点要通过注册，则需要知道其他的节点没有在并发抢注同一用户名的过程中。这个问题引领我们走向<strong>共识</strong>。</p>

<p>​   我们看到，达成共识意味着以这样一种方式决定某件事：所有节点一致同意所做决定，且这一决定不可撤销。通过深入挖掘，结果我们发现很广泛的一系列问题实际上都可以归结为共识问题，并且彼此等价（从这个意义上来讲，如果你有其中之一的解决方案，就可以轻易将它转换为其他问题的解决方案）。这些等价的问题包括：</p>

<p><strong><em>线性一致性的CAS寄存器</em></strong></p>

<p>​   寄存器需要基于当前值是否等于操作给出的参数，原子地<strong>决定</strong>是否设置新值。</p>

<p><strong><em>原子事务提交</em></strong></p>

<p>​   数据库必须<strong>决定</strong>是否提交或中止分布式事务。</p>

<p><strong><em>全序广播</em></strong></p>

<p>​   消息系统必须<strong>决定</strong>传递消息的顺序。</p>

<p><strong><em>锁和租约</em></strong></p>

<p>​   当几个客户端争抢锁或租约时，由锁来<strong>决定</strong>哪个客户端成功获得锁。</p>

<p><strong><em>成员/协调服务</em></strong></p>

<p>​   给定某种故障检测器（例如超时），系统必须<strong>决定</strong>哪些节点活着，哪些节点因为会话超时需要被宣告死亡。</p>

<p><strong><em>唯一性约束</em></strong></p>

<p>​   当多个事务同时尝试使用相同的键创建冲突记录时，约束必须<strong>决定</strong>哪一个被允许，哪些因为违反约束而失败。</p>

<p>如果你只有一个节点，或者你愿意将决策的权能分配给单个节点，所有这些事都很简单。这就是在单领导者数据库中发生的事情：所有决策权归属于领导者，这就是为什么这样的数据库能够提供线性一致的操作，唯一性约束，完全有序的复制日志，以及更多。</p>

<p>​   但如果该领导者失效，或者如果网络中断导致领导者不可达，这样的系统就无法取得任何进展。应对这种情况可以有三种方法：</p>

<ol>
<li>等待领导者恢复，接受系统将在这段时间阻塞的事实。许多XA/JTA事务协调者选择这个选项。这种方法并不能完全达成共识，因为它不能满足<strong>终止</strong>属性的要求：如果领导者续命失败，系统可能会永久阻塞。</li>
<li>人工故障切换，让人类选择一个新的领导者节点，并重新配置系统使之生效，许多关系型数据库都采用这种方方式。这是一种来自“天意”的共识 —— 由计算机系统之外的运维人员做出决定。故障切换的速度受到人类行动速度的限制，通常要比计算机慢（得多）。</li>
<li>使用算法自动选择一个新的领导者。这种方法需要一种共识算法，使用成熟的算法来正确处理恶劣的网络条件是明智之举【107】。</li>
</ol>

<p>尽管单领导者数据库可以提供线性一致性，且无需对每个写操作都执行共识算法，但共识对于保持及变更领导权仍然是必须的。因此从某种意义上说，使用单个领导者不过是“缓兵之计”：共识仍然是需要的，只是在另一个地方，而且没那么频繁。好消息是，容错的共识算法与容错的共识系统是存在的，我们在本章中简要地讨论了它们。</p>

<p>​   像ZooKeeper这样的工具为应用提供了“外包”的共识、故障检测和成员服务。它们扮演了重要的角色，虽说使用不易，但总比自己去开发一个能经受<a href="ch8.md">第8章</a>中所有问题考验的算法要好得多。如果你发现自己想要解决的问题可以归结为共识，并且希望它能容错，使用一个类似ZooKeeper的东西是明智之举。</p>

<p>​   尽管如此，并不是所有系统都需要共识：例如，无领导者复制和多领导者复制系统通常不会使用全局的共识。这些系统中出现的冲突（参见“<a href="ch5.md#%E5%A4%84%E7%90%86%E5%86%B2%E7%AA%81">处理冲突</a>”）正是不同领导者之间没有达成共识的结果，但也这并没有关系：也许我们只是需要接受没有线性一致性的事实，并学会更好地与具有分支与合并版本历史的数据打交道。</p>

<p>​   本章引用了大量关于分布式系统理论的研究。虽然理论论文和证明并不总是容易理解，有时也会做出不切实际的假设，但它们对于指导这一领域的实践有着极其重要的价值：它们帮助我们推理什么可以做，什么不可以做，帮助我们找到反直觉的分布式系统缺陷。如果你有时间，这些参考资料值得探索。</p>

<p>​   这里已经到了本书<a href="part-ii.md">第二部分</a>的末尾，第二部介绍了复制（<a href="ch5.md">第5章</a>），分区（<a href="ch6.md">第6章</a>），事务（<a href="ch7.md">第7章</a>），分布式系统的故障模型（<a href="ch8.md">第8章</a>）以及最后的一致性与共识（<a href="ch9.md">第9章</a>）。现在我们已经奠定了扎实的理论基础，我们将在<a href="part-iii.md">第三部分</a>再次转向更实际的系统，并讨论如何使用异构的组件积木块构建强大的应用。</p>

<h2 id="toc_103">参考文献</h2>

<ol>
<li><p>Peter Bailis and Ali Ghodsi: “<a href="http://queue.acm.org/detail.cfm?id=2462076">Eventual Consistency Today: Limitations, Extensions, and Beyond</a>,” <em>ACM Queue</em>, volume 11, number 3, pages 55-63, March 2013. <a href="http://dx.doi.org/10.1145/2460276.2462076">doi:10.1145/2460276.2462076</a></p></li>
<li><p>Prince Mahajan, Lorenzo Alvisi, and Mike Dahlin: “<a href="http://apps.cs.utexas.edu/tech_reports/reports/tr/TR-2036.pdf">Consistency, Availability, and Convergence</a>,” University of Texas at Austin, Department of Computer Science, Tech Report UTCS TR-11-22, May 2011.</p></li>
<li><p>Alex Scotti: “<a href="http://www.slideshare.net/AlexScotti1/allyourbase-55212398">Adventures in Building Your Own Database</a>,” at <em>All Your Base</em>, November 2015.</p></li>
<li><p>Peter Bailis, Aaron Davidson, Alan Fekete, et al.: “<a href="http://arxiv.org/pdf/1302.0309.pdf">Highly Available Transactions: Virtues and Limitations</a>,” at <em>40th International Conference on Very Large Data Bases</em> (VLDB), September 2014. Extended version published as pre-print arXiv:1302.0309 &#91;cs.DB&#93;.</p></li>
<li><p>Paolo Viotti and Marko Vukolić: “<a href="http://arxiv.org/abs/1512.00168">Consistency in Non-Transactional Distributed Storage Systems</a>,” arXiv:1512.00168, 12 April 2016.</p></li>
<li><p>Maurice P. Herlihy and Jeannette M. Wing: “<a href="http://cs.brown.edu/%7Emph/HerlihyW90/p463-herlihy.pdf">Linearizability: A Correctness Condition for Concurrent Objects</a>,” <em>ACM Transactions on Programming Languages and Systems</em> (TOPLAS), volume 12, number 3, pages 463–492, July 1990. <a href="http://dx.doi.org/10.1145/78969.78972">doi:10.1145/78969.78972</a></p></li>
<li><p>Leslie Lamport: “<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/interprocess.pdf">On interprocess communication</a>,” <em>Distributed Computing</em>, volume 1, number 2, pages 77–101, June 1986. <a href="http://dx.doi.org/10.1007/BF01786228">doi:10.1007/BF01786228</a></p></li>
<li><p>David K. Gifford: “<a href="http://www.mirrorservice.org/sites/www.bitsavers.org/pdf/xerox/parc/techReports/CSL-81-8_Information_Storage_in_a_Decentralized_Computer_System.pdf">Information Storage in a Decentralized Computer System</a>,” Xerox Palo Alto Research Centers, CSL-81-8, June 1981.</p></li>
<li><p>Martin Kleppmann: “<a href="http://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html">Please Stop Calling Databases CP or AP</a>,” <em>martin.kleppmann.com</em>, May 11, 2015.</p></li>
<li><p>Kyle Kingsbury: “<a href="https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads">Call Me Maybe: MongoDB Stale Reads</a>,” <em>aphyr.com</em>, April 20, 2015.</p></li>
<li><p>Kyle Kingsbury: “<a href="https://aphyr.com/posts/314-computational-techniques-in-knossos">Computational Techniques in Knossos</a>,” <em>aphyr.com</em>, May 17, 2014.</p></li>
<li><p>Peter Bailis:  “<a href="http://www.bailis.org/blog/linearizability-versus-serializability/">Linearizability   Versus Serializability</a>,” <em>bailis.org</em>, September 24, 2014.</p></li>
<li><p>Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman: <a href="http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx"><em>Concurrency Control and Recovery in Database Systems</em></a>. Addison-Wesley, 1987. ISBN: 978-0-201-10715-9, available online at <em>research.microsoft.com</em>.</p></li>
<li><p>Mike Burrows: “<a href="http://research.google.com/archive/chubby.html">The Chubby Lock Service for Loosely-Coupled Distributed Systems</a>,” at <em>7th USENIX Symposium on Operating System Design and Implementation</em> (OSDI), November 2006.</p></li>
<li><p>Flavio P. Junqueira and Benjamin Reed: <em>ZooKeeper: Distributed Process Coordination</em>. O&#39;Reilly Media, 2013. ISBN: 978-1-449-36130-3</p></li>
<li><p>“<a href="https://coreos.com/etcd/docs/2.0.12/">etcd 2.0.12 Documentation</a>,” CoreOS, Inc., 2015.</p></li>
<li><p>“<a href="http://curator.apache.org/">Apache Curator</a>,” Apache Software Foundation, <em>curator.apache.org</em>, 2015.</p></li>
<li><p>Morali Vallath: <em>Oracle 10g RAC Grid, Services &amp; Clustering</em>. Elsevier Digital Press, 2006. ISBN: 978-1-555-58321-7</p></li>
<li><p>Peter Bailis, Alan Fekete, Michael J Franklin, et al.: “<a href="http://arxiv.org/pdf/1402.2237.pdf">Coordination-Avoiding Database Systems</a>,” <em>Proceedings of the VLDB Endowment</em>, volume 8, number 3, pages 185–196, November 2014.</p></li>
<li><p>Kyle Kingsbury: “<a href="https://aphyr.com/posts/316-call-me-maybe-etcd-and-consul">Call Me Maybe: etcd and Consul</a>,” <em>aphyr.com</em>, June 9, 2014.</p></li>
<li><p>Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini:  “<a href="https://pdfs.semanticscholar.org/b02c/6b00bd5dbdbd951fddb00b906c82fa80f0b3.pdf">Zab: High-Performance Broadcast for   Primary-Backup Systems</a>,” at <em>41st IEEE International Conference on Dependable Systems and Networks</em> (DSN), June 2011.  <a href="http://dx.doi.org/10.1109/DSN.2011.5958223">doi:10.1109/DSN.2011.5958223</a></p></li>
<li><p>Diego Ongaro and John K. Ousterhout:  “<a href="http://ramcloud.stanford.edu/raft.pdf">In Search of an Understandable Consensus   Algorithm (Extended Version)</a>,” at <em>USENIX Annual Technical Conference</em>  (ATC), June 2014.</p></li>
<li><p>Hagit Attiya, Amotz Bar-Noy, and Danny Dolev: “<a href="http://www.cse.huji.ac.il/course/2004/dist/p124-attiya.pdf">Sharing Memory Robustly in Message-Passing Systems</a>,” <em>Journal of the ACM</em>, volume 42, number 1, pages 124–142, January 1995. <a href="http://dx.doi.org/10.1145/200836.200869">doi:10.1145/200836.200869</a></p></li>
<li><p>Nancy Lynch and Alex Shvartsman: “<a href="http://groups.csail.mit.edu/tds/papers/Lynch/FTCS97.pdf">Robust Emulation of Shared Memory Using Dynamic Quorum-Acknowledged Broadcasts</a>,” at <em>27th Annual International Symposium on Fault-Tolerant Computing</em> (FTCS), June 1997. <a href="http://dx.doi.org/10.1109/FTCS.1997.614100">doi:10.1109/FTCS.1997.614100</a></p></li>
<li><p>Christian Cachin, Rachid Guerraoui, and Luís Rodrigues: <a href="http://www.distributedprogramming.net/"><em>Introduction to Reliable and Secure Distributed Programming</em></a>, 2nd edition. Springer, 2011. ISBN: 978-3-642-15259-7, <a href="http://dx.doi.org/10.1007/978-3-642-15260-3">doi:10.1007/978-3-642-15260-3</a></p></li>
<li><p>Sam Elliott, Mark Allen, and Martin Kleppmann: <a href="https://twitter.com/lenary/status/654761711933648896">personal communication</a>, thread on <em>twitter.com</em>, October 15, 2015.</p></li>
<li><p>Niklas Ekström, Mikhail Panchenko, and Jonathan Ellis: “<a href="http://mail-archives.apache.org/mod_mbox/cassandra-dev/201210.mbox/%3CFA480D1DC3964E2C8B0A14E0880094C9%40Robotech%3E">Possible Issue with Read Repair?</a>,” email thread on <em>cassandra-dev</em> mailing list, October 2012.</p></li>
<li><p>Maurice P. Herlihy: “<a href="https://cs.brown.edu/%7Emph/Herlihy91/p124-herlihy.pdf">Wait-Free Synchronization</a>,” <em>ACM Transactions on Programming Languages and Systems</em> (TOPLAS), volume 13, number 1, pages 124–149, January 1991. <a href="http://dx.doi.org/10.1145/114005.102808">doi:10.1145/114005.102808</a></p></li>
<li><p>Armando Fox and Eric A. Brewer: “<a href="http://radlab.cs.berkeley.edu/people/fox/static/pubs/pdf/c18.pdf">Harvest, Yield, and Scalable Tolerant Systems</a>,” at <em>7th Workshop on Hot Topics in Operating Systems</em> (HotOS), March 1999. <a href="http://dx.doi.org/10.1109/HOTOS.1999.798396">doi:10.1109/HOTOS.1999.798396</a></p></li>
<li><p>Seth Gilbert and Nancy Lynch: “<a href="http://www.comp.nus.edu.sg/%7Egilbert/pubs/BrewersConjecture-SigAct.pdf">Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services</a>,” <em>ACM SIGACT News</em>, volume 33, number 2, pages 51–59, June 2002. <a href="http://dx.doi.org/10.1145/564585.564601">doi:10.1145/564585.564601</a></p></li>
<li><p>Seth Gilbert and Nancy Lynch: “<a href="http://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf">Perspectives on the CAP Theorem</a>,” <em>IEEE Computer Magazine</em>, volume 45, number 2, pages 30–36, February 2012. <a href="http://dx.doi.org/10.1109/MC.2011.389">doi:10.1109/MC.2011.389</a></p></li>
<li><p>Eric A. Brewer: “<a href="http://cs609.cs.ua.edu/CAP12.pdf">CAP Twelve Years Later: How the &#39;Rules&#39; Have Changed</a>,” <em>IEEE Computer Magazine</em>, volume 45, number 2, pages 23–29, February 2012. <a href="http://dx.doi.org/10.1109/MC.2012.37">doi:10.1109/MC.2012.37</a></p></li>
<li><p>Susan B. Davidson, Hector Garcia-Molina, and Dale Skeen: “<a href="http://delab.csd.auth.gr/%7Edimitris/courses/mpc_fall05/papers/invalidation/acm_csur85_partitioned_network_consistency.pdf">Consistency in Partitioned Networks</a>,” <em>ACM Computing Surveys</em>, volume 17, number 3, pages 341–370, September 1985. <a href="http://dx.doi.org/10.1145/5505.5508">doi:10.1145/5505.5508</a></p></li>
<li><p>Paul R. Johnson and Robert H. Thomas: “<a href="https://tools.ietf.org/html/rfc677">RFC 677: The Maintenance of Duplicate Databases</a>,” Network Working Group, January 27, 1975.</p></li>
<li><p>Bruce G. Lindsay, Patricia Griffiths Selinger, C. Galtieri, et al.: “<a href="http://domino.research.ibm.com/library/cyberdig.nsf/papers/A776EC17FC2FCE73852579F100578964/$File/RJ2571.pdf">Notes on Distributed Databases</a>,” IBM Research, Research Report RJ2571(33471), July 1979.</p></li>
<li><p>Michael J. Fischer and Alan Michael: “<a href="http://www.cs.ucsb.edu/%7Eagrawal/spring2011/ugrad/p70-fischer.pdf">Sacrificing Serializability to Attain High Availability of Data in an Unreliable Network</a>,” at <em>1st ACM Symposium on Principles of Database Systems</em> (PODS), March 1982.<br/>
<a href="http://dx.doi.org/10.1145/588111.588124">doi:10.1145/588111.588124</a></p></li>
<li><p>Eric A. Brewer: “<a href="http://www.infoq.com/presentations/NoSQL-History">NoSQL: Past, Present, Future</a>,” at <em>QCon San Francisco</em>, November 2012.</p></li>
<li><p>Henry Robinson: “<a href="http://blog.cloudera.com/blog/2010/04/cap-confusion-problems-with-partition-tolerance/">CAP Confusion: Problems with &#39;Partition Tolerance,&#39;</a>” <em>blog.cloudera.com</em>, April 26, 2010.</p></li>
<li><p>Adrian Cockcroft: “<a href="http://www.infoq.com/presentations/migration-cloud-native">Migrating to Microservices</a>,” at <em>QCon London</em>, March 2014.</p></li>
<li><p>Martin Kleppmann: “<a href="http://arxiv.org/abs/1509.05393">A Critique of the CAP Theorem</a>,” arXiv:1509.05393, September 17, 2015.</p></li>
<li><p>Nancy A. Lynch: “<a href="http://groups.csail.mit.edu/tds/papers/Lynch/podc89.pdf">A Hundred Impossibility Proofs for Distributed Computing</a>,” at <em>8th ACM Symposium on Principles of Distributed Computing</em> (PODC), August 1989. <a href="http://dx.doi.org/10.1145/72981.72982">doi:10.1145/72981.72982</a></p></li>
<li><p>Hagit Attiya, Faith Ellen, and Adam Morrison: “<a href="http://www.cs.technion.ac.il/people/mad/online-publications/podc2015-replds.pdf">Limitations of Highly-Available Eventually-Consistent Data Stores</a>,” at <em>ACM Symposium on Principles of Distributed Computing</em> (PODC), July 2015.  doi:10.1145/2767386.2767419](<a href="http://dx.doi.org/10.1145/2767386.2767419">http://dx.doi.org/10.1145/2767386.2767419</a>)</p></li>
<li><p>Peter Sewell, Susmit Sarkar, Scott Owens, et al.: “<a href="http://www.cl.cam.ac.uk/%7Epes20/weakmemory/cacm.pdf">x86-TSO: A Rigorous and Usable Programmer&#39;s Model for x86 Multiprocessors</a>,” <em>Communications of the ACM</em>, volume 53, number 7, pages 89–97, July 2010.<br/>
<a href="http://dx.doi.org/10.1145/1785414.1785443">doi:10.1145/1785414.1785443</a></p></li>
<li><p>Martin Thompson: “<a href="http://mechanical-sympathy.blogspot.co.uk/2011/07/memory-barriersfences.html">Memory Barriers/Fences</a>,” <em>mechanical-sympathy.blogspot.co.uk</em>, July 24, 2011.</p></li>
<li><p>Ulrich Drepper: “<a href="http://www.akkadia.org/drepper/cpumemory.pdf">What Every Programmer Should Know About Memory</a>,” <em>akkadia.org</em>, November 21, 2007.</p></li>
<li><p>Daniel J. Abadi: “<a href="http://cs-www.cs.yale.edu/homes/dna/papers/abadi-pacelc.pdf">Consistency Tradeoffs in Modern Distributed Database System Design</a>,” <em>IEEE Computer Magazine</em>, volume 45, number 2, pages 37–42, February 2012. <a href="http://dx.doi.org/10.1109/MC.2012.33">doi:10.1109/MC.2012.33</a></p></li>
<li><p>Hagit Attiya and Jennifer L. Welch: “<a href="http://courses.csail.mit.edu/6.852/01/papers/p91-attiya.pdf">Sequential Consistency Versus Linearizability</a>,” <em>ACM Transactions on Computer Systems</em> (TOCS), volume 12, number 2, pages 91–122, May 1994. <a href="http://dx.doi.org/10.1145/176575.176576">doi:10.1145/176575.176576</a></p></li>
<li><p>Mustaque Ahamad, Gil Neiger, James E. Burns, et al.:  “<a href="http://www-i2.informatik.rwth-aachen.de/i2/fileadmin/user_upload/documents/Seminar_MCMM11/Causal_memory_1996.pdf">Causal   Memory: Definitions, Implementation, and Programming</a>,” <em>Distributed Computing</em>, volume 9, number 1, pages 37–49, March 1995.  <a href="http://dx.doi.org/10.1007/BF01784241">doi:10.1007/BF01784241</a></p></li>
<li><p>Wyatt Lloyd, Michael J. Freedman, Michael Kaminsky, and David G. Andersen: “<a href="https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final149.pdf">Stronger Semantics for Low-Latency Geo-Replicated Storage</a>,” at <em>10th USENIX Symposium on Networked Systems Design and Implementation</em> (NSDI), April 2013.</p></li>
<li><p>Marek Zawirski, Annette Bieniusa, Valter Balegas, et al.: “<a href="http://arxiv.org/abs/1310.3107">SwiftCloud: Fault-Tolerant Geo-Replication Integrated All the Way to the Client Machine</a>,” INRIA Research Report 8347, August 2013.</p></li>
<li><p>Peter Bailis, Ali Ghodsi, Joseph M Hellerstein, and Ion Stoica: “<a href="http://db.cs.berkeley.edu/papers/sigmod13-bolton.pdf">Bolt-on Causal Consistency</a>,” at<br/>
<em>ACM International Conference on Management of Data</em> (SIGMOD), June 2013.</p></li>
<li><p>Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.: “<a href="https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-ajoux.pdf">Challenges to Adopting Stronger Consistency at Scale</a>,” at <em>15th USENIX Workshop on Hot Topics in Operating Systems</em> (HotOS), May 2015.</p></li>
<li><p>Peter Bailis: “<a href="http://www.bailis.org/blog/causality-is-expensive-and-what-to-do-about-it/">Causality Is Expensive (and What to Do About It)</a>,” <em>bailis.org</em>, February 5, 2014.</p></li>
<li><p>Ricardo Gonçalves, Paulo Sérgio Almeida, Carlos Baquero, and Victor Fonte: “<a href="http://haslab.uminho.pt/tome/files/global_logical_clocks.pdf">Concise Server-Wide Causality Management for Eventually Consistent Data Stores</a>,” at <em>15th IFIP International Conference on Distributed Applications and Interoperable Systems</em> (DAIS), June 2015. <a href="http://dx.doi.org/10.1007/978-3-319-19129-4_6">doi:10.1007/978-3-319-19129-4_6</a></p></li>
<li><p>Rob Conery:  “<a href="http://rob.conery.io/2014/05/29/a-better-id-generator-for-postgresql/">A Better ID   Generator for PostgreSQL</a>,” <em>rob.conery.io</em>, May 29, 2014.</p></li>
<li><p>Leslie Lamport: “<a href="http://research.microsoft.com/en-US/um/people/Lamport/pubs/time-clocks.pdf">Time, Clocks, and the Ordering of Events in a Distributed System</a>,” <em>Communications of the ACM</em>, volume 21, number 7, pages 558–565, July 1978. <a href="http://dx.doi.org/10.1145/359545.359563">doi:10.1145/359545.359563</a></p></li>
<li><p>Xavier Défago, André Schiper, and Péter Urbán: “<a href="https://dspace.jaist.ac.jp/dspace/bitstream/10119/4883/1/defago_et_al.pdf">Total Order Broadcast and Multicast Algorithms: Taxonomy and Survey</a>,” <em>ACM Computing Surveys</em>, volume 36, number 4, pages 372–421, December 2004.<br/>
<a href="http://dx.doi.org/10.1145/1041680.1041682">doi:10.1145/1041680.1041682</a></p></li>
<li><p>Hagit Attiya and Jennifer Welch: <em>Distributed Computing: Fundamentals, Simulations and Advanced Topics</em>, 2nd edition. John Wiley &amp; Sons, 2004. ISBN: 978-0-471-45324-6, <a href="http://dx.doi.org/10.1002/0471478210">doi:10.1002/0471478210</a></p></li>
<li><p>Mahesh Balakrishnan, Dahlia Malkhi, Vijayan Prabhakaran, et al.: “<a href="https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final30.pdf">CORFU: A Shared Log Design for Flash Clusters</a>,” at <em>9th USENIX Symposium on Networked Systems Design and Implementation</em> (NSDI), April 2012.</p></li>
<li><p>Fred B. Schneider: “<a href="http://www.cs.cornell.edu/fbs/publications/smsurvey.pdf">Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial</a>,” <em>ACM Computing Surveys</em>, volume 22, number 4, pages 299–319, December 1990.</p></li>
<li><p>Alexander Thomson, Thaddeus Diamond, Shu-Chun Weng, et al.: “<a href="http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf">Calvin: Fast Distributed Transactions for Partitioned Database Systems</a>,” at <em>ACM International Conference on Management of Data</em> (SIGMOD), May 2012.</p></li>
<li><p>Mahesh Balakrishnan, Dahlia Malkhi, Ted Wobber, et al.: “<a href="http://research.microsoft.com/pubs/199947/Tango.pdf">Tango: Distributed Data Structures over a Shared Log</a>,” at <em>24th ACM Symposium on Operating Systems Principles</em> (SOSP), November 2013.<br/>
<a href="http://dx.doi.org/10.1145/2517349.2522732">doi:10.1145/2517349.2522732</a></p></li>
<li><p>Robbert van Renesse and Fred B. Schneider: “<a href="http://static.usenix.org/legacy/events/osdi04/tech/full_papers/renesse/renesse.pdf">Chain Replication for Supporting High Throughput and Availability</a>,” at <em>6th USENIX Symposium on Operating System Design and Implementation</em> (OSDI), December 2004.</p></li>
<li><p>Leslie Lamport: “<a href="http://research-srv.microsoft.com/en-us/um/people/lamport/pubs/multi.pdf">How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs</a>,” <em>IEEE Transactions on Computers</em>, volume 28, number 9, pages 690–691, September 1979.<br/>
<a href="http://dx.doi.org/10.1109/TC.1979.1675439">doi:10.1109/TC.1979.1675439</a></p></li>
<li><p>Enis Söztutar, Devaraj Das, and Carter Shanklin: “<a href="http://hortonworks.com/blog/apache-hbase-high-availability-next-level/">Apache HBase High Availability at the Next Level</a>,” <em>hortonworks.com</em>, January 22, 2015.</p></li>
<li><p>Brian F Cooper, Raghu Ramakrishnan, Utkarsh Srivastava, et al.: “<a href="http://www.mpi-sws.org/%7Edruschel/courses/ds/papers/cooper-pnuts.pdf">PNUTS: Yahoo!’s Hosted Data Serving Platform</a>,” at <em>34th International Conference on Very Large Data Bases</em> (VLDB), August 2008.<br/>
<a href="http://dx.doi.org/10.14778/1454159.1454167">doi:10.14778/1454159.1454167</a></p></li>
<li><p>Tushar Deepak Chandra and Sam Toueg: “<a href="http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf">Unreliable Failure Detectors for Reliable Distributed Systems</a>,” <em>Journal of the ACM</em>, volume 43, number 2, pages 225–267, March 1996. <a href="http://dx.doi.org/10.1145/226643.226647">doi:10.1145/226643.226647</a></p></li>
<li><p>Michael J. Fischer, Nancy Lynch, and Michael S. Paterson: “<a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf">Impossibility of Distributed Consensus with One Faulty Process</a>,” <em>Journal of the ACM</em>, volume 32, number 2, pages 374–382, April 1985. <a href="http://dx.doi.org/10.1145/3149.214121">doi:10.1145/3149.214121</a></p></li>
<li><p>Michael Ben-Or: “Another Advantage of Free Choice: Completely Asynchronous Agreement Protocols,” at <em>2nd ACM Symposium on Principles of Distributed Computing</em> (PODC), August 1983. <a href="http://dl.acm.org/citation.cfm?id=806707">doi:10.1145/800221.806707</a></p></li>
<li><p>Jim N. Gray and Leslie Lamport: “<a href="http://db.cs.berkeley.edu/cs286/papers/paxoscommit-tods2006.pdf">Consensus on Transaction Commit</a>,” <em>ACM Transactions on Database Systems</em> (TODS), volume 31, number 1, pages 133–160, March 2006. <a href="http://dx.doi.org/10.1145/1132863.1132867">doi:10.1145/1132863.1132867</a></p></li>
<li><p>Rachid Guerraoui: “<a href="https://pdfs.semanticscholar.org/5d06/489503b6f791aa56d2d7942359c2592e44b0.pdf">Revisiting the Relationship Between Non-Blocking Atomic Commitment and Consensus</a>,” at <em>9th International Workshop on Distributed Algorithms</em> (WDAG), September 1995. <a href="http://dx.doi.org/10.1007/BFb0022140">doi:10.1007/BFb0022140</a></p></li>
<li><p>Thanumalayan Sankaranarayana Pillai, Vijay Chidambaram, Ramnatthan Alagappan, et al.: “<a href="http://research.cs.wisc.edu/wind/Publications/alice-osdi14.pdf">All File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent Applications</a>,”<br/>
at <em>11th USENIX Symposium on Operating Systems Design and Implementation</em> (OSDI),<br/>
October 2014.</p></li>
<li><p>Jim Gray: “<a href="http://research.microsoft.com/en-us/um/people/gray/papers/theTransactionConcept.pdf">The Transaction Concept: Virtues and Limitations</a>,” at <em>7th International Conference on Very Large Data Bases</em> (VLDB), September 1981.</p></li>
<li><p>Hector Garcia-Molina and Kenneth Salem: “<a href="http://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf">Sagas</a>,” at <em>ACM International Conference on Management of Data</em> (SIGMOD), May 1987. <a href="http://dx.doi.org/10.1145/38713.38742">doi:10.1145/38713.38742</a></p></li>
<li><p>C. Mohan, Bruce G. Lindsay, and Ron Obermarck: “<a href="https://cs.brown.edu/courses/csci2270/archives/2012/papers/dtxn/p378-mohan.pdf">Transaction Management in the R* Distributed Database Management System</a>,” <em>ACM Transactions on Database Systems</em>, volume 11, number 4, pages 378–396, December 1986. <a href="http://dx.doi.org/10.1145/7239.7266">doi:10.1145/7239.7266</a></p></li>
<li><p>“<a href="http://pubs.opengroup.org/onlinepubs/009680699/toc.pdf">Distributed Transaction Processing: The XA Specification</a>,” X/Open Company Ltd., Technical Standard<br/>
XO/CAE/91/300, December 1991. ISBN: 978-1-872-63024-3</p></li>
<li><p>Mike Spille: “<a href="http://www.jroller.com/pyrasun/entry/xa_exposed_part_ii_schwartz">XA Exposed, Part II</a>,” <em>jroller.com</em>, April 3, 2004.</p></li>
<li><p>Ivan Silva Neto and Francisco Reverbel: “<a href="http://www.ime.usp.br/%7Ereverbel/papers/icis2008.pdf">Lessons Learned from Implementing WS-Coordination and WS-AtomicTransaction</a>,” at <em>7th IEEE/ACIS International Conference on Computer and Information Science</em> (ICIS), May 2008. <a href="http://dx.doi.org/10.1109/ICIS.2008.75">doi:10.1109/ICIS.2008.75</a></p></li>
<li><p>James E. Johnson, David E. Langworthy, Leslie Lamport, and Friedrich H. Vogt: “<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/wsfm-web.pdf">Formal Specification of a Web Services Protocol</a>,” at <em>1st International Workshop on Web Services and Formal Methods</em> (WS-FM), February 2004. <a href="http://dx.doi.org/10.1016/j.entcs.2004.02.022">doi:10.1016/j.entcs.2004.02.022</a></p></li>
<li><p>Dale Skeen: “<a href="http://www.cs.utexas.edu/%7Elorenzo/corsi/cs380d/papers/Ske81.pdf">Nonblocking Commit Protocols</a>,” at <em>ACM International Conference on Management of Data</em> (SIGMOD), April 1981. <a href="http://dx.doi.org/10.1145/582318.582339">doi:10.1145/582318.582339</a></p></li>
<li><p>Gregor Hohpe: “<a href="http://www.martinfowler.com/ieeeSoftware/coffeeShop.pdf">Your Coffee Shop Doesn’t Use Two-Phase Commit</a>,” <em>IEEE Software</em>, volume 22, number 2, pages 64–66, March 2005. <a href="http://dx.doi.org/10.1109/MS.2005.52">doi:10.1109/MS.2005.52</a></p></li>
<li><p>Pat Helland: “<a href="http://www-db.cs.wisc.edu/cidr/cidr2007/papers/cidr07p15.pdf">Life Beyond Distributed Transactions: An Apostate’s Opinion</a>,” at <em>3rd Biennial Conference on Innovative Data Systems Research</em> (CIDR), January 2007.</p></li>
<li><p>Jonathan Oliver: “<a href="http://blog.jonathanoliver.com/my-beef-with-msdtc-and-two-phase-commits/">My Beef with MSDTC and Two-Phase Commits</a>,” <em>blog.jonathanoliver.com</em>, April 4, 2011.</p></li>
<li><p>Oren Eini (Ahende Rahien): “<a href="http://ayende.com/blog/167362/the-fallacy-of-distributed-transactions">The Fallacy of Distributed Transactions</a>,” <em>ayende.com</em>, July 17, 2014.</p></li>
<li><p>Clemens Vasters: “<a href="https://blogs.msdn.microsoft.com/clemensv/2012/07/30/transactions-in-windows-azure-with-service-bus-an-email-discussion/">Transactions in Windows Azure (with Service Bus) – An Email Discussion</a>,” <em>vasters.com</em>, July 30, 2012.</p></li>
<li><p>“<a href="https://docs.particular.net/nservicebus/azure/understanding-transactionality-in-azure">Understanding Transactionality in Azure</a>,” NServiceBus Documentation, Particular Software, 2015.</p></li>
<li><p>Randy Wigginton, Ryan Lowe, Marcos Albe, and Fernando Ipar: “<a href="https://www.percona.com/live/mysql-conference-2013/sites/default/files/slides/XA_final.pdf">Distributed Transactions in MySQL</a>,” at <em>MySQL Conference and Expo</em>, April 2013.</p></li>
<li><p>Mike Spille: “<a href="http://www.jroller.com/pyrasun/entry/xa_exposed">XA Exposed, Part I</a>,” <em>jroller.com</em>, April 3, 2004.</p></li>
<li><p>Ajmer Dhariwal: “<a href="http://www.eraofdata.com/orphaned-msdtc-transactions-2-spids/">Orphaned MSDTC Transactions (-2 spids)</a>,” <em>eraofdata.com</em>, December 12, 2008.</p></li>
<li><p>Paul Randal: “<a href="http://www.sqlskills.com/blogs/paul/real-world-story-of-dbcc-page-saving-the-day/">Real World Story of DBCC PAGE Saving the Day</a>,” <em>sqlskills.com</em>, June 19, 2013.</p></li>
<li><p>“<a href="https://msdn.microsoft.com/en-us/library/ms179586.aspx">in-doubt xact resolution Server Configuration Option</a>,” SQL Server 2016 documentation, Microsoft, Inc.,<br/>
2016.</p></li>
<li><p>Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer: “<a href="http://www.net.t-labs.tu-berlin.de/%7Epetr/ADC-07/papers/DLS88.pdf">Consensus in the Presence of Partial Synchrony</a>,” <em>Journal of the ACM</em>, volume 35, number 2, pages 288–323, April 1988. <a href="http://dx.doi.org/10.1145/42282.42283">doi:10.1145/42282.42283</a></p></li>
<li><p>Miguel Castro and Barbara H. Liskov: “<a href="http://zoo.cs.yale.edu/classes/cs426/2012/bib/castro02practical.pdf">Practical Byzantine Fault Tolerance and Proactive Recovery</a>,” <em>ACM Transactions on Computer Systems</em>, volume 20, number 4, pages 396–461, November 2002. <a href="http://dx.doi.org/10.1145/571637.571640">doi:10.1145/571637.571640</a></p></li>
<li><p>Brian M. Oki and Barbara H. Liskov: “<a href="http://www.cs.princeton.edu/courses/archive/fall11/cos518/papers/viewstamped.pdf">Viewstamped Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems</a>,” at <em>7th ACM Symposium on Principles of Distributed Computing</em> (PODC), August 1988. <a href="http://dx.doi.org/10.1145/62546.62549">doi:10.1145/62546.62549</a></p></li>
<li><p>Barbara H. Liskov and James Cowling: “<a href="http://pmg.csail.mit.edu/papers/vr-revisited.pdf">Viewstamped Replication Revisited</a>,” Massachusetts Institute of Technology, Tech Report MIT-CSAIL-TR-2012-021, July 2012.</p></li>
<li><p>Leslie Lamport: “<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/lamport-paxos.pdf">The Part-Time Parliament</a>,” <em>ACM Transactions on Computer Systems</em>, volume 16, number 2, pages 133–169, May 1998. <a href="http://dx.doi.org/10.1145/279227.279229">doi:10.1145/279227.279229</a></p></li>
<li><p>Leslie Lamport: “<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf">Paxos Made Simple</a>,” <em>ACM SIGACT News</em>, volume 32, number 4, pages 51–58, December 2001.</p></li>
<li><p>Tushar Deepak Chandra, Robert Griesemer, and Joshua Redstone: “<a href="http://www.read.seas.harvard.edu/%7Ekohler/class/08w-dsi/chandra07paxos.pdf">Paxos Made Live – An Engineering Perspective</a>,” at <em>26th ACM Symposium on Principles of Distributed Computing</em> (PODC), June 2007.</p></li>
<li><p>Robbert van Renesse: “<a href="http://www.cs.cornell.edu/home/rvr/Paxos/paxos.pdf">Paxos Made Moderately Complex</a>,” <em>cs.cornell.edu</em>, March 2011.</p></li>
<li><p>Diego Ongaro: “<a href="https://github.com/ongardie/dissertation">Consensus: Bridging Theory and Practice</a>,” PhD Thesis, Stanford University, August 2014.</p></li>
<li><p>Heidi Howard, Malte Schwarzkopf, Anil Madhavapeddy, and Jon Crowcroft: “<a href="http://www.cl.cam.ac.uk/%7Ems705/pub/papers/2015-osr-raft.pdf">Raft Refloated: Do We Have Consensus?</a>,” <em>ACM SIGOPS Operating Systems Review</em>, volume 49, number 1, pages 12–21, January 2015.<br/>
<a href="http://dx.doi.org/10.1145/2723872.2723876">doi:10.1145/2723872.2723876</a></p></li>
<li><p>André Medeiros: “<a href="http://www.tcs.hut.fi/Studies/T-79.5001/reports/2012-deSouzaMedeiros.pdf">ZooKeeper’s Atomic Broadcast Protocol: Theory and Practice</a>,” Aalto University School of Science, March 20, 2012.</p></li>
<li><p>Robbert van Renesse, Nicolas Schiper, and Fred B. Schneider: “<a href="http://arxiv.org/abs/1309.5671">Vive La Différence: Paxos vs. Viewstamped Replication vs. Zab</a>,” <em>IEEE Transactions on Dependable and Secure Computing</em>,<br/>
volume 12, number 4, pages 472–484, September 2014. <a href="http://dx.doi.org/10.1109/TDSC.2014.2355848">doi:10.1109/TDSC.2014.2355848</a></p></li>
<li><p>Will Portnoy: “<a href="http://blog.willportnoy.com/2012/06/lessons-learned-from-paxos.html">Lessons Learned from Implementing Paxos</a>,” <em>blog.willportnoy.com</em>, June 14, 2012.</p></li>
<li><p>Heidi Howard, Dahlia Malkhi, and Alexander Spiegelman: “<a href="https://arxiv.org/abs/1608.06696">Flexible Paxos: Quorum Intersection Revisited</a>,” <em>arXiv:1608.06696</em>, August 24, 2016.</p></li>
<li><p>Heidi Howard and Jon Crowcroft: “<a href="http://www.sigcomm.org/sites/default/files/ccr/papers/2015/August/2829988-2790010.pdf">Coracle: Evaluating Consensus at the Internet Edge</a>,” at <em>Annual Conference of the ACM Special Interest Group on Data Communication</em> (SIGCOMM), August 2015.<br/>
<a href="http://dx.doi.org/10.1145/2829988.2790010">doi:10.1145/2829988.2790010</a></p></li>
<li><p>Kyle Kingsbury: “<a href="https://aphyr.com/posts/323-call-me-maybe-elasticsearch-1-5-0">Call Me Maybe: Elasticsearch 1.5.0</a>,” <em>aphyr.com</em>, April 27, 2015.</p></li>
<li><p>Ivan Kelly: “<a href="https://github.com/ivankelly/bookkeeper-tutorial">BookKeeper Tutorial</a>,” <em>github.com</em>, October 2014.</p></li>
<li><p>Camille Fournier: “<a href="http://www.ustream.tv/recorded/61483409">Consensus Systems for the Skeptical Architect</a>,” at <em>Craft Conference</em>, Budapest, Hungary, April 2015.</p></li>
<li><p>Kenneth P. Birman: “<a href="https://www.truststc.org/pubs/713.html">A History of the Virtual Synchrony Replication Model</a>,” in <em>Replication: Theory and Practice</em>, Springer LNCS volume 5959, chapter 6, pages 91–120, 2010. ISBN: 978-3-642-11293-5, <a href="http://dx.doi.org/10.1007/978-3-642-11294-2_6">doi:10.1007/978-3-642-11294-2_6</a></p></li>
</ol>

<hr/>

<table>
<thead>
<tr>
<th>上一章</th>
<th>目录</th>
<th>下一章</th>
</tr>
</thead>

<tbody>
<tr>
<td><a href="ch8.md">第八章：分布式系统的麻烦</a></td>
<td><a href="README.md">设计数据密集型应用</a></td>
<td><a href="part-iii.md">第三部分：衍生数据</a></td>
</tr>
</tbody>
</table>

<div class="footnotes">
<hr/>
<ol>

<li id="fn1">
<p>这个图的一个微妙的细节是它假定存在一个全局时钟，由水平轴表示。即使真实的系统通常没有准确的时钟（参阅“<a href="ch8.md#%E4%B8%8D%E5%8F%AF%E9%9D%A0%E7%9A%84%E6%97%B6%E9%92%9F">不可靠的时钟</a>”），但这种假设是允许的：为了分析分布式算法，我们可以假设一个精确的全局时钟存在，不过算法无法访问它【47】。算法只能看到由石英振荡器和NTP产生的实时逼近。&nbsp;<a href="#fnref1" rev="footnote">&#8617;</a></p>
</li>

<li id="fn2">
<p>如果读取（与写入同时发生时）可能返回旧值或新值，则称该寄存器为<strong>常规寄存器（regular register）</strong>【7,25】&nbsp;<a href="#fnref2" rev="footnote">&#8617;</a></p>
</li>

<li id="fn3">
<p>严格地说，ZooKeeper和etcd提供线性一致性的写操作，但读取可能是陈旧的，因为默认情况下，它们可以由任何一个副本服务。你可以选择请求线性一致性读取：etcd调用这个法定读取【16】，而在ZooKeeper中，你需要在读取【15】之前调用<code>sync()</code>。参阅“<a href="#%E4%BD%BF%E7%94%A8%E5%85%A8%E5%B1%80%E9%A1%BA%E5%BA%8F%E5%B9%BF%E6%92%AD%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%AD%98%E5%82%A8">使用全局顺序广播实现线性存储</a>”。&nbsp;<a href="#fnref3" rev="footnote">&#8617;</a></p>
</li>

<li id="fn4">
<p>对单领域数据库进行分区（分片），以便每个分区有一个单独的领导者，不会影响线性一致性，因为线性一致性只是对单一对象的保证。 交叉分区事务是一个不同的问题（参阅“<a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%92%8C%E5%85%B1%E8%AF%86">分布式事务和共识</a>”）。&nbsp;<a href="#fnref4" rev="footnote">&#8617;</a></p>
</li>

<li id="fn5">
<p>这两种选择有时分别称为CP（在网络分区下一致但不可用）和AP（在网络分区下可用但不一致）。 但是，这种分类方案存在一些缺陷【9】，所以最好不要这样用。&nbsp;<a href="#fnref5" rev="footnote">&#8617;</a></p>
</li>

<li id="fn6">
<p>正如“<a href="ch8.md#%E7%9C%9F%E5%AE%9E%E4%B8%96%E7%95%8C%E7%9A%84%E7%BD%91%E7%BB%9C%E6%95%85%E9%9A%9C">真实世界的网络故障</a>”中所讨论的，本书使用<strong>分区（partition）</strong>指代将大数据集细分为小数据集的操作（分片；参见<a href="ch6.md">第6章</a>）。与之对应的是，<strong>网络分区（network partition）</strong>是一种特定类型的网络故障，我们通常不会将其与其他类型的故障分开考虑。但是，由于它是CAP的P，所以这种情况下不能将其混为一谈。&nbsp;<a href="#fnref6" rev="footnote">&#8617;</a></p>
</li>

<li id="fn7">
<p>设R为非空集合A上的关系，如果R是自反的、反对称的和可传递的，则称R为A上的偏序关系。简称偏序，通常记作≦。一个集合A与A上的偏序关系R一起叫作偏序集，记作\((A,R)\)或\((A, ≦)\)。全序、偏序、关系、集合，这些概念的精确定义可以参考任意一本离散数学教材。&nbsp;<a href="#fnref7" rev="footnote">&#8617;</a></p>
</li>

<li id="fn8">
<p>与因果关系不一致的全序很容易创建，但没啥用。例如你可以为每个操作生成随机的UUID，并按照字典序比较UUID，以定义操作的全序。这是一个有效的全序，但是随机的UUID并不能告诉你哪个操作先发生，或者操作是否为并发的。&nbsp;<a href="#fnref8" rev="footnote">&#8617;</a></p>
</li>

<li id="fn9">
<p>“原子广播”是一个传统的术语，非常混乱，而且与“原子”一词的其他用法不一致：它与ACID事务中的原子性没有任何关系，只是与原子操作（在多线程编程的意义上 ）或原子寄存器（线性一致存储）有间接的联系。全序广播是另一个同义词。&nbsp;<a href="#fnref9" rev="footnote">&#8617;</a></p>
</li>

<li id="fn10">
<p>从形式上讲，线性一致读写寄存器是一个“更容易”的问题。 全序广播等价于共识【67】，而共识问题在异步的崩溃-停止模型【68】中没有确定性的解决方案，而线性一致的读写寄存器<strong>可以</strong>在这种模型中实现【23,24,25】。 然而，支持诸如<strong>比较并设置（CAS, compare-and-set）</strong>，或<strong>自增并返回（increment-and-get）</strong>的原子操作使它等价于共识问题【28】。 因此，共识问题与线性一致寄存器问题密切相关。&nbsp;<a href="#fnref10" rev="footnote">&#8617;</a></p>
</li>

<li id="fn11">
<p>如果你不等待，而是在消息入队之后立即确认写入，则会得到类似于多核x86处理器内存的一致性模型【43】。 该模型既不是线性一致的也不是顺序一致的。&nbsp;<a href="#fnref11" rev="footnote">&#8617;</a></p>
</li>

<li id="fn12">
<p>原子提交的形式化与共识稍有不同：原子事务只有在<strong>所有</strong>参与者投票提交的情况下才能提交，如果有任何参与者需要中止，则必须中止。 共识则允许就<strong>任意一个</strong>被参与者提出的候选值达成一致。 然而，原子提交和共识可以相互简化为对方【70,71】。 <strong>非阻塞</strong>原子提交则要比共识更为困难 —— 参阅“<a href="#%E4%B8%89%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4">三阶段提交</a>”。&nbsp;<a href="#fnref12" rev="footnote">&#8617;</a></p>
</li>

<li id="fn13">
<p>这种共识的特殊形式被称为<strong>统一共识（uniform consensus）</strong>，相当于在具有不可靠故障检测器的异步系统中的<strong>常规共识（regular consensus）</strong>【71】。学术文献通常指的是<strong>进程（process）</strong>而不是节点，但我们在这里使用<strong>节点（node）</strong>来与本书的其余部分保持一致。&nbsp;<a href="#fnref13" rev="footnote">&#8617;</a></p>
</li>

</ol>
</div>
